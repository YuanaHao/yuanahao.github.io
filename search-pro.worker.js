const V=Object.entries,et=Object.fromEntries,st="ENTRIES",L="KEYS",T="VALUES",_="";class D{set;_type;_path;constructor(t,s){const n=t._tree,o=Array.from(n.keys());this.set=t,this._type=s,this._path=o.length>0?[{node:n,keys:o}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:s}=E(this._path);if(E(s)===_)return{done:!1,value:this.result()};const n=t.get(E(s));return this._path.push({node:n,keys:Array.from(n.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=E(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>E(t)).filter(t=>t!==_).join("")}value(){return E(this._path).node.get(_)}result(){switch(this._type){case T:return this.value();case L:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const E=e=>e[e.length-1],nt=(e,t,s)=>{const n=new Map;if(t===void 0)return n;const o=t.length+1,u=o+s,i=new Uint8Array(u*o).fill(s+1);for(let r=0;r<o;++r)i[r]=r;for(let r=1;r<u;++r)i[r*o]=r;return R(e,t,s,n,i,1,o,""),n},R=(e,t,s,n,o,u,i,r)=>{const d=u*i;t:for(const c of e.keys())if(c===_){const a=o[d-1];a<=s&&n.set(r,[e.get(c),a])}else{let a=u;for(let h=0;h<c.length;++h,++a){const g=c[h],m=i*a,p=m-i;let l=o[m];const f=Math.max(0,a-s-1),y=Math.min(i-1,a+s);for(let F=f;F<y;++F){const v=g!==t[F],z=o[p+F]+ +v,A=o[p+F+1]+1,w=o[m+F]+1,j=o[m+F+1]=Math.min(z,A,w);j<l&&(l=j)}if(l>s)continue t}R(e.get(c),t,s,n,o,a,i,r+c)}};class C{_tree;_prefix;_size=void 0;constructor(t=new Map,s=""){this._tree=t,this._prefix=s}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[s,n]=x(this._tree,t.slice(this._prefix.length));if(s===void 0){const[o,u]=O(n);for(const i of o.keys())if(i!==_&&i.startsWith(u)){const r=new Map;return r.set(i.slice(u.length),o.get(i)),new C(r,t)}}return new C(s,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,ot(this._tree,t)}entries(){return new D(this,st)}forEach(t){for(const[s,n]of this)t(s,n,this)}fuzzyGet(t,s){return nt(this._tree,t,s)}get(t){const s=k(this._tree,t);return s!==void 0?s.get(_):void 0}has(t){const s=k(this._tree,t);return s!==void 0&&s.has(_)}keys(){return new D(this,L)}set(t,s){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,I(this._tree,t).set(_,s),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=I(this._tree,t);return n.set(_,s(n.get(_))),this}fetch(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=I(this._tree,t);let o=n.get(_);return o===void 0&&n.set(_,o=s()),o}values(){return new D(this,T)}[Symbol.iterator](){return this.entries()}static from(t){const s=new C;for(const[n,o]of t)s.set(n,o);return s}static fromObject(t){return C.from(Object.entries(t))}}const x=(e,t,s=[])=>{if(t.length===0||e==null)return[e,s];for(const n of e.keys())if(n!==_&&t.startsWith(n))return s.push([e,n]),x(e.get(n),t.slice(n.length),s);return s.push([e,t]),x(void 0,"",s)},k=(e,t)=>{if(t.length===0||e==null)return e;for(const s of e.keys())if(s!==_&&t.startsWith(s))return k(e.get(s),t.slice(s.length))},I=(e,t)=>{const s=t.length;t:for(let n=0;e&&n<s;){for(const u of e.keys())if(u!==_&&t[n]===u[0]){const i=Math.min(s-n,u.length);let r=1;for(;r<i&&t[n+r]===u[r];)++r;const d=e.get(u);if(r===u.length)e=d;else{const c=new Map;c.set(u.slice(r),d),e.set(t.slice(n,n+r),c),e.delete(u),e=c}n+=r;continue t}const o=new Map;return e.set(t.slice(n),o),o}return e},ot=(e,t)=>{const[s,n]=x(e,t);if(s!==void 0){if(s.delete(_),s.size===0)W(n);else if(s.size===1){const[o,u]=s.entries().next().value;q(n,o,u)}}},W=e=>{if(e.length===0)return;const[t,s]=O(e);if(t.delete(s),t.size===0)W(e.slice(0,-1));else if(t.size===1){const[n,o]=t.entries().next().value;n!==_&&q(e.slice(0,-1),n,o)}},q=(e,t,s)=>{if(e.length===0)return;const[n,o]=O(e);n.set(o+t,s),n.delete(o)},O=e=>e[e.length-1],ut=(e,t)=>{const s=e._idToShortId.get(t);if(s!=null)return e._storedFields.get(s)},it=/[\n\r -#%-*,-/:;?@[-\]_{}\u00A0\u00A1\u00A7\u00AB\u00B6\u00B7\u00BB\u00BF\u037E\u0387\u055A-\u055F\u0589\u058A\u05BE\u05C0\u05C3\u05C6\u05F3\u05F4\u0609\u060A\u060C\u060D\u061B\u061E\u061F\u066A-\u066D\u06D4\u0700-\u070D\u07F7-\u07F9\u0830-\u083E\u085E\u0964\u0965\u0970\u09FD\u0A76\u0AF0\u0C77\u0C84\u0DF4\u0E4F\u0E5A\u0E5B\u0F04-\u0F12\u0F14\u0F3A-\u0F3D\u0F85\u0FD0-\u0FD4\u0FD9\u0FDA\u104A-\u104F\u10FB\u1360-\u1368\u1400\u166E\u1680\u169B\u169C\u16EB-\u16ED\u1735\u1736\u17D4-\u17D6\u17D8-\u17DA\u1800-\u180A\u1944\u1945\u1A1E\u1A1F\u1AA0-\u1AA6\u1AA8-\u1AAD\u1B5A-\u1B60\u1BFC-\u1BFF\u1C3B-\u1C3F\u1C7E\u1C7F\u1CC0-\u1CC7\u1CD3\u2000-\u200A\u2010-\u2029\u202F-\u2043\u2045-\u2051\u2053-\u205F\u207D\u207E\u208D\u208E\u2308-\u230B\u2329\u232A\u2768-\u2775\u27C5\u27C6\u27E6-\u27EF\u2983-\u2998\u29D8-\u29DB\u29FC\u29FD\u2CF9-\u2CFC\u2CFE\u2CFF\u2D70\u2E00-\u2E2E\u2E30-\u2E4F\u3000-\u3003\u3008-\u3011\u3014-\u301F\u3030\u303D\u30A0\u30FB\uA4FE\uA4FF\uA60D-\uA60F\uA673\uA67E\uA6F2-\uA6F7\uA874-\uA877\uA8CE\uA8CF\uA8F8-\uA8FA\uA8FC\uA92E\uA92F\uA95F\uA9C1-\uA9CD\uA9DE\uA9DF\uAA5C-\uAA5F\uAADE\uAADF\uAAF0\uAAF1\uABEB\uFD3E\uFD3F\uFE10-\uFE19\uFE30-\uFE52\uFE54-\uFE61\uFE63\uFE68\uFE6A\uFE6B\uFF01-\uFF03\uFF05-\uFF0A\uFF0C-\uFF0F\uFF1A\uFF1B\uFF1F\uFF20\uFF3B-\uFF3D\uFF3F\uFF5B\uFF5D\uFF5F-\uFF65]+/u,M="or",$="and",rt="and_not",ct=(e,t)=>{e.includes(t)||e.push(t)},N=(e,t)=>{for(const s of t)e.includes(s)||e.push(s)},P=({score:e},{score:t})=>t-e,lt=()=>new Map,b=e=>{const t=new Map;for(const s of Object.keys(e))t.set(parseInt(s,10),e[s]);return t},G=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,ht={[M]:(e,t)=>{for(const s of t.keys()){const n=e.get(s);if(n==null)e.set(s,t.get(s));else{const{score:o,terms:u,match:i}=t.get(s);n.score=n.score+o,n.match=Object.assign(n.match,i),N(n.terms,u)}}return e},[$]:(e,t)=>{const s=new Map;for(const n of t.keys()){const o=e.get(n);if(o==null)continue;const{score:u,terms:i,match:r}=t.get(n);N(o.terms,i),s.set(n,{score:o.score+u,terms:o.terms,match:Object.assign(o.match,r)})}return s},[rt]:(e,t)=>{for(const s of t.keys())e.delete(s);return e}},dt=(e,t,s,n,o,u)=>{const{k:i,b:r,d}=u;return Math.log(1+(s-t+.5)/(t+.5))*(d+e*(i+1)/(e+i*(1-r+r*n/o)))},at=e=>(t,s,n)=>{const o=typeof e.fuzzy=="function"?e.fuzzy(t,s,n):e.fuzzy||!1,u=typeof e.prefix=="function"?e.prefix(t,s,n):e.prefix===!0;return{term:t,fuzzy:o,prefix:u}},H=(e,t,s,n)=>{for(const o of Object.keys(e._fieldIds))if(e._fieldIds[o]===s){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${n}" was not present in field "${o}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},ft=(e,t,s,n)=>{if(!e._index.has(n)){H(e,s,t,n);return}const o=e._index.fetch(n,lt),u=o.get(t);u==null||u.get(s)==null?H(e,s,t,n):u.get(s)<=1?u.size<=1?o.delete(t):u.delete(s):u.set(s,u.get(s)-1),e._index.get(n).size===0&&e._index.delete(n)},gt={k:1.2,b:.7,d:.5},mt={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(it),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{typeof console?.[e]=="function"&&console[e](t)},autoVacuum:!0},J={combineWith:M,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:gt},pt={combineWith:$,prefix:(e,t,s)=>t===s.length-1},Ft={batchSize:1e3,batchWait:10},U={minDirtFactor:.1,minDirtCount:20},_t={...Ft,...U},K=Symbol("*"),yt=(e,t)=>{const s=new Map,n={...e._options.searchOptions,...t};for(const[o,u]of e._documentIds){const i=n.boostDocument?n.boostDocument(u,"",e._storedFields.get(o)):1;s.set(o,{score:i,terms:[],match:{}})}return s},X=(e,t=M)=>{if(e.length===0)return new Map;const s=t.toLowerCase(),n=ht[s];if(!n)throw new Error(`Invalid combination operator: ${t}`);return e.reduce(n)||new Map},S=(e,t,s,n,o,u,i,r,d=new Map)=>{if(o==null)return d;for(const c of Object.keys(u)){const a=u[c],h=e._fieldIds[c],g=o.get(h);if(g==null)continue;let m=g.size;const p=e._avgFieldLength[h];for(const l of g.keys()){if(!e._documentIds.has(l)){ft(e,h,l,s),m-=1;continue}const f=i?i(e._documentIds.get(l),s,e._storedFields.get(l)):1;if(!f)continue;const y=g.get(l),F=e._fieldLength.get(l)[h],v=dt(y,m,e._documentCount,F,p,r),z=n*a*f*v,A=d.get(l);if(A){A.score+=z,ct(A.terms,t);const w=G(A.match,s);w?w.push(c):A.match[s]=[c]}else d.set(l,{score:z,terms:[t],match:{[s]:[c]}})}}return d},At=(e,t,s)=>{const n={...e._options.searchOptions,...s},o=(n.fields||e._options.fields).reduce((l,f)=>({...l,[f]:G(n.boost,f)||1}),{}),{boostDocument:u,weights:i,maxFuzzy:r,bm25:d}=n,{fuzzy:c,prefix:a}={...J.weights,...i},h=e._index.get(t.term),g=S(e,t.term,t.term,1,h,o,u,d);let m,p;if(t.prefix&&(m=e._index.atPrefix(t.term)),t.fuzzy){const l=t.fuzzy===!0?.2:t.fuzzy,f=l<1?Math.min(r,Math.round(t.term.length*l)):l;f&&(p=e._index.fuzzyGet(t.term,f))}if(m)for(const[l,f]of m){const y=l.length-t.term.length;if(!y)continue;p?.delete(l);const F=a*l.length/(l.length+.3*y);S(e,t.term,l,F,f,o,u,d,g)}if(p)for(const l of p.keys()){const[f,y]=p.get(l);if(!y)continue;const F=c*l.length/(l.length+y);S(e,t.term,l,F,f,o,u,d,g)}return g},Y=(e,t,s={})=>{if(t===K)return yt(e,s);if(typeof t!="string"){const a={...s,...t,queries:void 0},h=t.queries.map(g=>Y(e,g,a));return X(h,a.combineWith)}const{tokenize:n,processTerm:o,searchOptions:u}=e._options,i={tokenize:n,processTerm:o,...u,...s},{tokenize:r,processTerm:d}=i,c=r(t).flatMap(a=>d(a)).filter(a=>!!a).map(at(i)).map(a=>At(e,a,i));return X(c,i.combineWith)},Q=(e,t,s={})=>{const n=Y(e,t,s),o=[];for(const[u,{score:i,terms:r,match:d}]of n){const c=r.length||1,a={id:e._documentIds.get(u),score:i*c,terms:Object.keys(d),queryTerms:r,match:d};Object.assign(a,e._storedFields.get(u)),(s.filter==null||s.filter(a))&&o.push(a)}return t===K&&s.boostDocument==null&&e._options.searchOptions.boostDocument==null||o.sort(P),o},Ct=(e,t,s={})=>{s={...e._options.autoSuggestOptions,...s};const n=new Map;for(const{score:u,terms:i}of Q(e,t,s)){const r=i.join(" "),d=n.get(r);d!=null?(d.score+=u,d.count+=1):n.set(r,{score:u,terms:i,count:1})}const o=[];for(const[u,{score:i,terms:r,count:d}]of n)o.push({suggestion:u,terms:r,score:i/d});return o.sort(P),o};class Et{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(t?.fields==null)throw new Error('SlimSearch: option "fields" must be provided');const s=t.autoVacuum==null||t.autoVacuum===!0?_t:t.autoVacuum;this._options={...mt,...t,autoVacuum:s,searchOptions:{...J,...t.searchOptions||{}},autoSuggestOptions:{...pt,...t.autoSuggestOptions||{}}},this._index=new C,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=U,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[s,n]of this._index){const o={};for(const[u,i]of n)o[u]=Object.fromEntries(i);t.push([s,o])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,serializationVersion:2}}addFields(t){for(let s=0;s<t.length;s++)this._fieldIds[t[s]]=s}}const zt=({index:e,documentCount:t,nextId:s,documentIds:n,fieldIds:o,fieldLength:u,averageFieldLength:i,storedFields:r,dirtCount:d,serializationVersion:c},a)=>{if(c!==1&&c!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const h=new Et(a);h._documentCount=t,h._nextId=s,h._documentIds=b(n),h._idToShortId=new Map,h._fieldIds=o,h._fieldLength=b(u),h._avgFieldLength=i,h._storedFields=b(r),h._dirtCount=d||0,h._index=new C;for(const[g,m]of h._documentIds)h._idToShortId.set(m,g);for(const[g,m]of e){const p=new Map;for(const l of Object.keys(m)){let f=m[l];c===1&&(f=f.ds),p.set(parseInt(l,10),b(f))}h._index.set(g,p)}return h},B=(e,t)=>{const s=e.toLowerCase(),n=t.toLowerCase(),o=[];let u=0,i=0;const r=(c,a=!1)=>{let h="";i===0?h=c.length>20?`… ${c.slice(-20)}`:c:a?h=c.length+i>100?`${c.slice(0,100-i)}… `:c:h=c.length>20?`${c.slice(0,20)} … ${c.slice(-20)}`:c,h&&o.push(h),i+=h.length,a||(o.push(["mark",t]),i+=t.length,i>=100&&o.push(" …"))};let d=s.indexOf(n,u);if(d===-1)return null;for(;d>=0;){const c=d+n.length;if(r(e.slice(u,d)),u=c,i>100)break;d=s.indexOf(n,u)}return i<100&&r(e.slice(u),!0),o},wt=(e,t)=>t.contents.reduce((s,[,n])=>s+n,0)-e.contents.reduce((s,[,n])=>s+n,0),xt=(e,t)=>Math.max(...t.contents.map(([,s])=>s))-Math.max(...e.contents.map(([,s])=>s)),Z=(e,t,s={})=>{const n={};return Q(t,e,{boost:{h:2,t:1,c:4},prefix:!0,...s}).forEach(o=>{const{id:u,terms:i,score:r}=o,d=u.includes("@"),c=u.includes("#"),[a,h]=u.split(/[#@]/),g=Number(a),m=i.sort((l,f)=>l.length-f.length).filter((l,f)=>i.slice(f+1).every(y=>!y.includes(l))),{contents:p}=n[g]??={title:"",contents:[]};if(d)p.push([{type:"customField",id:g,index:h,display:m.map(l=>o.c.map(f=>B(f,l))).flat().filter(l=>l!==null)},r]);else{const l=m.map(f=>B(o.h,f)).filter(f=>f!==null);if(l.length&&p.push([{type:c?"heading":"title",id:g,...c&&{anchor:h},display:l},r]),"t"in o)for(const f of o.t){const y=m.map(F=>B(f,F)).filter(F=>F!==null);y.length&&p.push([{type:"text",id:g,...c&&{anchor:h},display:y},r])}}}),V(n).sort(([,o],[,u])=>"max"==="total"?wt(o,u):xt(o,u)).map(([o,{title:u,contents:i}])=>{if(!u){const r=ut(t,o);r&&(u=r.h)}return{title:u,contents:i.map(([r])=>r)}})},tt=(e,t,s={})=>{const n=Ct(t,e,{fuzzy:.2,maxFuzzy:3,...s}).map(({suggestion:o})=>o);return e.includes(" ")?n:n.filter(o=>!o.includes(" "))},bt=et(V(JSON.parse("{\"/\":{\"documentCount\":300,\"nextId\":300,\"documentIds\":{\"0\":\"1\",\"1\":\"2\",\"2\":\"2#how-to-find-the-bugs-in-the-program\",\"3\":\"2#parallel-problem-bugs\",\"4\":\"2#unnecessary-block\",\"5\":\"2#conditional-competition\",\"6\":\"2#how-to-find-bugs\",\"7\":\"2#how-did-i-find-bugs-in-the-pro\",\"8\":\"2#step1-try-to-run-the-code-and-monitor-the-output\",\"9\":\"2#step2-read-the-codes-and-understand-the-brief-frame\",\"10\":\"2#step3-set-some-log-information\",\"11\":\"2#how-did-i-fix-the-bugs\",\"12\":\"2#step1-fix-unbalanced-work\",\"13\":\"2#step2-exit-sucessfully\",\"14\":\"2#my-answer\",\"15\":\"2@0\",\"16\":\"2@1\",\"17\":\"3\",\"18\":\"3#installation\",\"19\":\"3#step1-unzip-the-baseenv-tar-gz\",\"20\":\"3#step2-load-the-docker-image-from-the-tar-file\",\"21\":\"3#step3-start-the-container-with-the-following-command\",\"22\":\"3#task\",\"23\":\"3#task1-compile-the-mpich\",\"24\":\"3#step1-set-mpich-configure-and-make-it\",\"25\":\"3#task2-configure-the-network-for-containers\",\"26\":\"3#task3-configure-ssh-for-the-containers\",\"27\":\"3#task4-run-the-mpi-program-in-the-cluster\",\"28\":\"3@0\",\"29\":\"3@1\",\"30\":\"4\",\"31\":\"4#为什么ai需要分布式系统\",\"32\":\"4#什么是分布式系统\",\"33\":\"4#为什么ai训练需要分布式系统\",\"34\":\"4#分布式系统如何发挥作用\",\"35\":\"4#数据分布式的一些基本想法\",\"36\":\"4#数据并行-dp\",\"37\":\"4#典型数据并行的流程\",\"38\":\"4#horovod\",\"39\":\"4#ring-allreduce\",\"40\":\"4#reduce-scatter\",\"41\":\"4#all-gather\",\"42\":\"4#horovod工作\",\"43\":\"4#使用方法\",\"44\":\"4#pytorch-ddp\",\"45\":\"4#python前端api\",\"46\":\"4#梯度同步算法\",\"47\":\"4#传统做法\",\"48\":\"4#改进做法\",\"49\":\"4#集合通讯库\",\"50\":\"4#fsdp并行\",\"51\":\"4#zero\",\"52\":\"4#abstract\",\"53\":\"4#extended-introduction\",\"54\":\"4#where-did-all-the-memory-go\",\"55\":\"4#model-states-optimizer-states-gradients-and-parameters\",\"56\":\"4#residual-memory-consumption\",\"57\":\"4#temporary-buffers\",\"58\":\"4#memory-fragmentation\",\"59\":\"4#zero-insights-and-overview\",\"60\":\"4#insights-and-overview-zero-dp\",\"61\":\"4#insights-and-overview-zero-r\",\"62\":\"4#reducing-activation-memory\",\"63\":\"4#managing-temporary-buffers\",\"64\":\"4#managing-fragmented-memory\",\"65\":\"4#deep-dive-into-zero-dp\",\"66\":\"4#zero1\",\"67\":\"4#zero2\",\"68\":\"4#zero3\",\"69\":\"4#deep-dive-into-zero-r\",\"70\":\"4#partitioned-activation-checkpointing\",\"71\":\"4#constant-size-buffers\",\"72\":\"4#memory-defragmentation\",\"73\":\"4#zero-offload\",\"74\":\"4#背景\",\"75\":\"4#efficiency\",\"76\":\"4#unique-optimal-offload-strategy\",\"77\":\"4#zero-offload-schedule\",\"78\":\"4#单卡策略\",\"79\":\"4#多卡策略\",\"80\":\"4#流水线并行\",\"81\":\"4#gpipe\",\"82\":\"4#gpipe-abstract\",\"83\":\"4#design\",\"84\":\"4#naive-model-parallelism\",\"85\":\"4#pipeline-parallelism-1-split-into-micro-batches\",\"86\":\"4#pipeline-parallelism-2-re-materialization\",\"87\":\"4#张量并行-tp\",\"88\":\"4#megatron-lm\",\"89\":\"4#model-parallel-transformers\",\"90\":\"4#_3d并行\",\"91\":\"4#贡献\",\"92\":\"4#数据并行\",\"93\":\"4#考虑流水线并行\",\"94\":\"4#考虑张量并行\",\"95\":\"4#性能分析\",\"96\":\"4#自动并行\",\"97\":\"4#flexflow\",\"98\":\"4#简介\",\"99\":\"4#概述\",\"100\":\"4#执行模拟器\",\"101\":\"4#执行优化器\",\"102\":\"4@0\",\"103\":\"4@1\",\"104\":\"5\",\"105\":\"5#prog1-mandelbort-threads\",\"106\":\"5#环境配置\",\"107\":\"5#任务分析\",\"108\":\"5#任务实现\",\"109\":\"5@0\",\"110\":\"5@1\",\"111\":\"6\",\"112\":\"6#week1\",\"113\":\"6#lecture\",\"114\":\"6#welcome\",\"115\":\"6#functions\",\"116\":\"6#name\",\"117\":\"6#built-in\",\"118\":\"6#name-by-ourselves\",\"119\":\"6#define\",\"120\":\"6#defining-functions\",\"121\":\"6#environment-diagrams\",\"122\":\"6#print-and-none\",\"123\":\"6#lab\",\"124\":\"6#lab0-getting-started\",\"125\":\"6#introduction\",\"126\":\"6#setup\",\"127\":\"6#first-assignment\",\"128\":\"6#what-would-python-do\",\"129\":\"6#implementing-functions\",\"130\":\"6#running-tests\",\"131\":\"6#appendix-useful-python-command-line-options\",\"132\":\"6@0\",\"133\":\"6@1\",\"134\":\"7\",\"135\":\"8\",\"136\":\"8#pa1-automatic-differentiation\",\"137\":\"8#question-1-auto-diff-library\",\"138\":\"8#part-1-operators\",\"139\":\"8#divop\",\"140\":\"8@0\",\"141\":\"8@1\",\"142\":\"9\",\"143\":\"9#序列并行\",\"144\":\"9#megatron\",\"145\":\"9#abstract\",\"146\":\"9#activation-memory\",\"147\":\"9#tensor-parallel\",\"148\":\"9#sequence-parallel\",\"149\":\"9#deepspeed-ulysses\",\"150\":\"9#简介\",\"151\":\"9#deepspeed-ulysses的核心设计\",\"152\":\"9#sequence-parallelism\",\"153\":\"9#论文背景\",\"154\":\"9#主要工作\",\"155\":\"9#ring-attention\",\"156\":\"9#背景\",\"157\":\"9#ring-attention-1\",\"158\":\"9#distflashattn\",\"159\":\"9#背景-1\",\"160\":\"9#论文方法\",\"161\":\"9#token-level-workload-imbalance\",\"162\":\"9#prohibitive-communication-overhead\",\"163\":\"9#loadbalanced-scheduling-with-communication-and-computation-overlap\",\"164\":\"9@0\",\"165\":\"9@1\",\"166\":\"10\",\"167\":\"10#pa1-the-simplest-computer\",\"168\":\"10#infrastructure\",\"169\":\"10@0\",\"170\":\"10@1\",\"171\":\"11\",\"172\":\"11#写在开始之前\",\"173\":\"11#introduction\",\"174\":\"11#梯度下降\",\"175\":\"11#数值微分-numeric-differentiation\",\"176\":\"11#autodiff-library\",\"177\":\"11#课程已实现的算子\",\"178\":\"11#自己实现的算子\",\"179\":\"11#divop\",\"180\":\"11@0\",\"181\":\"11@1\",\"182\":\"12\",\"183\":\"12#题目引入\",\"184\":\"12#题目分析\",\"185\":\"12#斯特林公式\",\"186\":\"12#代码实现\",\"187\":\"12#总结\",\"188\":\"12@0\",\"189\":\"12@1\",\"190\":\"13\",\"191\":\"13#_1-dragon-string-龙字符串\",\"192\":\"13#题目描述\",\"193\":\"13#约束条件\",\"194\":\"13#解题思路\",\"195\":\"13#易错点\",\"196\":\"13#代码实现\",\"197\":\"13#复杂度分析\",\"198\":\"13#_2-yes字符串判断\",\"199\":\"13#题目描述-1\",\"200\":\"13#约束条件-1\",\"201\":\"13#解题思路-1\",\"202\":\"13#易错点-1\",\"203\":\"13#代码实现-1\",\"204\":\"13#复杂度分析-1\",\"205\":\"13#_3-奇偶判断\",\"206\":\"13#题目描述-2\",\"207\":\"13#约束条件-2\",\"208\":\"13#解题思路-2\",\"209\":\"13#易错点-2\",\"210\":\"13#代码实现-2\",\"211\":\"13#复杂度分析-2\",\"212\":\"13#_4-字符统计问题\",\"213\":\"13#题目描述-3\",\"214\":\"13#解题思路-3\",\"215\":\"13#易错点-3\",\"216\":\"13#代码实现-3\",\"217\":\"13#复杂度分析-3\",\"218\":\"13#_5-投票统计\",\"219\":\"13#题目描述-4\",\"220\":\"13#解题思路-4\",\"221\":\"13#易错点-4\",\"222\":\"13#代码实现-4\",\"223\":\"13#复杂度分析-4\",\"224\":\"13#_6-字符替换\",\"225\":\"13#题目描述-5\",\"226\":\"13#解题思路-5\",\"227\":\"13#易错点-5\",\"228\":\"13#代码实现-5\",\"229\":\"13#复杂度分析-5\",\"230\":\"13#_7-矩阵操作\",\"231\":\"13#题目描述-6\",\"232\":\"13#解题思路-6\",\"233\":\"13#易错点-6\",\"234\":\"13#代码实现-6\",\"235\":\"13#复杂度分析-6\",\"236\":\"13#总结\",\"237\":\"13@0\",\"238\":\"13@1\",\"239\":\"14\",\"240\":\"14#摘要\",\"241\":\"14#引言\",\"242\":\"14#第一章-推理并行的基础范式与理论边界\",\"243\":\"14#_1-1-数据并行-data-parallelism-dp-服务能力水平扩展的直接范式\",\"244\":\"14#_1-2-流水线并行-pipeline-parallelism-pp-提升硬件利用率与吞吐量的关键\",\"245\":\"14#_1-3-张量并行-tensor-parallelism-tp-降低计算延迟的核心手段\",\"246\":\"14#第二章-现代推理系统的核心技术与架构演进\",\"247\":\"14#_2-1-混合并行-hybrid-parallelism\",\"248\":\"14#_2-2-i-o与内存管理优化-从根本上提升系统效率\",\"249\":\"14#第三章-算法优化\",\"250\":\"14#_3-1-模型压缩与量化-model-compression-quantization\",\"251\":\"14#_3-2-解码加速-推测解码-speculative-decoding\",\"252\":\"14#第四章-特定领域并行策略\",\"253\":\"14#_4-1-案例一-组件级与任务级并行——以stable-diffusion为例\",\"254\":\"14#_4-2-案例二-序列并行-sequence-parallelism-——应对超长上下文挑战\",\"255\":\"14#_4-3-案例三-复杂单一样本的混合并行——alphafold3的系统设计启示\",\"256\":\"14#第五章-工程实践\",\"257\":\"14#附录\",\"258\":\"14#a-1-张量并行-tensor-parallelism-tp-的数学原理\",\"259\":\"14#a-1-1-transformer中mlp层的并行化\",\"260\":\"14#a-1-2-多头注意力-mha-层的并行化\",\"261\":\"14#a-2-序列并行-sequence-parallelism-sp\",\"262\":\"14#a-3-多维张量并行-2d-tensor-parallelism\",\"263\":\"14#a-4-推测解码-speculative-decoding\",\"264\":\"14#a-5-流水线并行-pp-的调度与开销分析\",\"265\":\"14#a-5-1-流水线气泡的量化分析\",\"266\":\"14#a-5-2-交错式流水线调度-interleaved-scheduling\",\"267\":\"14#a-6-序列并行-sp-中layernorm的分布式计算\",\"268\":\"14#a-7-激活感知权重化-awq-的核心原理\",\"269\":\"14@0\",\"270\":\"14@1\",\"271\":\"15\",\"272\":\"15#文生图\",\"273\":\"15#distrifusion\",\"274\":\"15#特点\",\"275\":\"15#以往方法\",\"276\":\"15#相关工作\",\"277\":\"15#背景知识\",\"278\":\"15#本文方法\",\"279\":\"15#位移补丁并行\",\"280\":\"15#总结\",\"281\":\"15#pipefusion\",\"282\":\"15#摘要\",\"283\":\"15#问题\",\"284\":\"15#本文方法-1\",\"285\":\"15#xdit\",\"286\":\"15#overview\",\"287\":\"15#dit-主干网络混合并行\",\"288\":\"15#parallel-vae\",\"289\":\"15#简单灵活的开发接口\",\"290\":\"15#pipefusion-1\",\"291\":\"15#usp-混合序列并行\",\"292\":\"15#cfg-parallel\",\"293\":\"15#hybrid-parallel\",\"294\":\"15#parallel-vae-1\",\"295\":\"15@0\",\"296\":\"15@1\",\"297\":\"16\",\"298\":\"17\",\"299\":\"18\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1,8],\"1\":[4],\"2\":[7,10],\"3\":[3],\"4\":[2,9],\"5\":[2,7],\"6\":[4,6],\"7\":[8,18],\"8\":[9,68],\"9\":[8,52],\"10\":[5,110],\"11\":[6],\"12\":[4,104],\"13\":[3,70],\"14\":[2,140],\"15\":[null,null,1],\"16\":[null,null,3],\"17\":[4],\"18\":[1,17],\"19\":[3,29],\"20\":[8,5],\"21\":[7,54],\"22\":[1],\"23\":[3,30],\"24\":[7,188],\"25\":[6,25],\"26\":[6,70],\"27\":[7,70],\"28\":[null,null,1],\"29\":[null,null,3],\"30\":[2],\"31\":[1],\"32\":[1,13],\"33\":[1,24],\"34\":[1],\"35\":[1,83],\"36\":[3],\"37\":[1,16],\"38\":[1,31],\"39\":[2,8],\"40\":[2,14],\"41\":[2,3],\"42\":[1,22],\"43\":[1,62],\"44\":[2,18],\"45\":[1,64],\"46\":[1,3],\"47\":[1,5],\"48\":[1,32],\"49\":[1,28],\"50\":[1],\"51\":[1,18],\"52\":[1,15],\"53\":[2,24],\"54\":[6,8],\"55\":[6,41],\"56\":[3],\"57\":[2,3],\"58\":[2,6],\"59\":[4,2],\"60\":[5,17],\"61\":[5],\"62\":[3,11],\"63\":[3,2],\"64\":[4,2],\"65\":[5],\"66\":[1,4],\"67\":[1,4],\"68\":[1,13],\"69\":[5],\"70\":[3,12],\"71\":[3,3],\"72\":[2,4],\"73\":[2,14],\"74\":[1,16],\"75\":[1,34],\"76\":[4,52],\"77\":[3],\"78\":[1,103],\"79\":[1,40],\"80\":[1],\"81\":[1,16],\"82\":[2,21],\"83\":[1],\"84\":[3,44],\"85\":[7,13],\"86\":[5,8],\"87\":[3],\"88\":[2,20],\"89\":[3,137],\"90\":[1,21],\"91\":[1,20],\"92\":[1,7],\"93\":[1,28],\"94\":[1,2],\"95\":[1,48],\"96\":[1],\"97\":[1,28],\"98\":[1,39],\"99\":[1,46],\"100\":[1,11],\"101\":[1,22],\"102\":[null,null,1],\"103\":[null,null,3],\"104\":[3],\"105\":[3],\"106\":[1,38],\"107\":[1,46],\"108\":[1,51],\"109\":[null,null,2],\"110\":[null,null,3],\"111\":[1],\"112\":[1],\"113\":[1],\"114\":[1,39],\"115\":[1],\"116\":[1],\"117\":[2,90],\"118\":[3,33],\"119\":[1,59],\"120\":[2,38],\"121\":[2,9],\"122\":[3,12],\"123\":[1],\"124\":[3,10],\"125\":[1,24],\"126\":[1,2],\"127\":[2],\"128\":[5,80],\"129\":[2,25],\"130\":[2,27],\"131\":[6,28],\"132\":[null,null,1],\"133\":[null,null,3],\"134\":[1,35],\"135\":[1],\"136\":[3],\"137\":[5],\"138\":[3,21],\"139\":[1,48],\"140\":[null,null,2],\"141\":[null,null,4],\"142\":[4],\"143\":[1],\"144\":[1,13],\"145\":[1,21],\"146\":[2,92],\"147\":[2,20],\"148\":[2,81],\"149\":[2,28],\"150\":[1,28],\"151\":[2,29],\"152\":[2,12],\"153\":[1,7],\"154\":[1,14],\"155\":[2,14],\"156\":[1,3],\"157\":[2,55],\"158\":[1,6],\"159\":[1,5],\"160\":[1,4],\"161\":[4,16],\"162\":[3,10],\"163\":[7,8],\"164\":[null,null,1],\"165\":[null,null,3],\"166\":[7,38],\"167\":[4],\"168\":[1,116],\"169\":[null,null,1],\"170\":[null,null,2],\"171\":[5,10],\"172\":[1,28],\"173\":[1,16],\"174\":[1,39],\"175\":[4,28],\"176\":[2,8],\"177\":[1,217],\"178\":[1],\"179\":[1,48],\"180\":[null,null,1],\"181\":[null,null,2],\"182\":[2],\"183\":[1,23],\"184\":[1,52],\"185\":[1,45],\"186\":[1,33],\"187\":[1,1],\"188\":[null,null,1],\"189\":[null,null,2],\"190\":[3],\"191\":[5],\"192\":[1,12],\"193\":[1,5],\"194\":[1,13],\"195\":[1,14],\"196\":[1,22],\"197\":[1,6],\"198\":[2],\"199\":[1,6],\"200\":[1,5],\"201\":[1,8],\"202\":[1,15],\"203\":[1,33],\"204\":[1,7],\"205\":[2],\"206\":[1,3],\"207\":[1,5],\"208\":[1,7],\"209\":[1,15],\"210\":[1,30],\"211\":[1,6],\"212\":[2],\"213\":[1,3],\"214\":[1,6],\"215\":[1,9],\"216\":[1,37],\"217\":[1,8],\"218\":[2],\"219\":[1,6],\"220\":[1,6],\"221\":[1,18],\"222\":[1,45],\"223\":[1,8],\"224\":[2],\"225\":[1,3],\"226\":[1,7],\"227\":[1,9],\"228\":[1,32],\"229\":[1,6],\"230\":[2],\"231\":[1,3],\"232\":[1,6],\"233\":[1,16],\"234\":[1,37],\"235\":[1,7],\"236\":[1,20],\"237\":[null,null,1],\"238\":[null,null,1],\"239\":[1],\"240\":[1,52],\"241\":[1,44],\"242\":[1],\"243\":[1,34],\"244\":[1,45],\"245\":[1,92],\"246\":[1],\"247\":[1,30],\"248\":[1,32],\"249\":[1],\"250\":[1,26],\"251\":[1,28],\"252\":[1],\"253\":[1,24],\"254\":[1,24],\"255\":[1,40],\"256\":[1,35],\"257\":[1],\"258\":[1,9],\"259\":[1,104],\"260\":[1,61],\"261\":[1,70],\"262\":[1,95],\"263\":[1,111],\"264\":[1,7],\"265\":[1,66],\"266\":[1,21],\"267\":[1,81],\"268\":[1,136],\"269\":[null,null,1],\"270\":[null,null,3],\"271\":[4],\"272\":[1,24],\"273\":[1,29],\"274\":[1,5],\"275\":[1,18],\"276\":[1,27],\"277\":[1,48],\"278\":[1,28],\"279\":[1,45],\"280\":[1,12],\"281\":[1,10],\"282\":[1,5],\"283\":[1,12],\"284\":[1,16],\"285\":[1,69],\"286\":[1],\"287\":[2,13],\"288\":[2,16],\"289\":[1],\"290\":[1,1],\"291\":[2,5],\"292\":[2,23],\"293\":[2,76],\"294\":[2,24],\"295\":[null,null,1],\"296\":[null,null,3],\"297\":[1,3],\"298\":[1],\"299\":[1]},\"averageFieldLength\":[2.044644211134477,31.204280703781208,0.5578315612772637],\"storedFields\":{\"0\":{\"h\":\"个人介绍\",\"t\":[\"计算机大二本科小白，\",\"主攻CV/NLP等AI方向，\",\"同时对分布式开发、并行运算感兴趣，\",\"算法还是0基础新人，\",\"路漫漫其修远兮。\"]},\"1\":{\"h\":\"ASC2025 TEST_0_CONCURRENCY\"},\"2\":{\"h\":\"How to find the bugs in the program\",\"t\":[\"reference: https://nj.gitbooks.io/c/content/content/chapter10/chapter10-chinese.html\"]},\"3\":{\"h\":\"parallel problem bugs\"},\"4\":{\"h\":\"unnecessary block\",\"t\":[\"dead lock\",\"live lock(condition not met)\",\"I/O block\"]},\"5\":{\"h\":\"conditional competition\",\"t\":[\"data race\",\"destroy constant\",\"life circle issues\"]},\"6\":{\"h\":\"how to find bugs\",\"t\":[\"read codes\",\"set breakpoint\",\"use tools\"]},\"7\":{\"h\":\"how did I find bugs in the pro\",\"t\":[\"below steps are tying to test a progect when i didn't know if there were bugs.\"]},\"8\":{\"h\":\"step1: try to run the code and monitor the output\",\"t\":[\"We can compile the code to a executable file by the instruction in the terminal, just a try.\",\"gcc condvar.cc -o try -pthread -lstdc++\",\"then run \\\"try\\\", the output is \\\"error: data[0] = 718613\\\".\",\"so we can confirm there are some bugs in the codes, then try to find them.\",\"we need to use some tools, such as ThreadSanitizer, to confirm the specific issue type.\",\"g++ condvar.cc -fsanitize=thread -fPIE -pie -g -o TSan\",\"then run the \\\"TSan\\\"\",\"the output is \\\"WARNING: ThreadSanitizer: data race (pid=30965)\\\". so we can confirm there exist \\\"data race\\\" issue. then read codes to try to find if not \\\"join()\\\" or \\\"condition variable\\\" are here.\"]},\"9\":{\"h\":\"step2: read the codes and understand the brief frame\",\"t\":[\"\\\"create_func\\\" is a lambda expression and define the tasks of each thread. \",\"wait \\\"m_waiting\\\" to start work\",\"fetch a task, decrease the waiting task count\",\"thread of array position += 1\",\"one thread finish the task then notify the main thread by \\\"notify_one()\\\"\",\"the first \\\"for circle\\\" is set to creat the threads\",\"the second \\\"for circle\\\" is set to finish the main thread's task \",\"update the waiting task count and notify all threads to start working\",\"wait for all threads to finish and clear the finished task count, enter the next round\",\"the last \\\"for circle\\\" is set to check.\"]},\"10\":{\"h\":\"step3: set some log information\",\"t\":[\"if we are back to the step1, you will find seem \\\"warning\\\" is after the data check, meaning we can't find the data error, so we must set some log information.\",\"I choice to set the below codes.\",\"// run 1000000 rounds for (int i = 0; i < 1000000; i++) { // update the waiting task count lock lk_waiting(m_waiting); waiting = 5; std::fill(worktime.begin(), worktime.end(), true); // notify all threads to start working cv_waiting.notify_all(); lk_waiting.unlock(); // wait for all threads to finish lock lk_finished(m_finished); cv_finished.wait(lk_finished, []() { return finished == 5; }); // clear the finished task count, enter the next round finished = 0; lk_finished.unlock(); // log information printf(\\\"finish %d %d %d %d %d %d\\\\n\\\", i, data[0], data[1], data[2], data[3], data[4]); }\",\"after recompile and run it again, I found suprisely the five data sum up to 5000000, but one of them was not 1000000.\",\"the answer is found out, threads of the task didn't work balancedly, some time some threads may work more than one time in one circle, so we just need to make sure just once.\"]},\"11\":{\"h\":\"how did I fix the bugs\"},\"12\":{\"h\":\"step1: fix unbalanced work\",\"t\":[\"I choice to introduce a new condition variable worktime for 5 threads, to make sure just work one time in one circle.\",\"vec_t<bool> worktime (5, 0);\",\"then I add them into every threads as a judgement for starting to get work.\",\"auto create_func = [](int tid) { return [tid]() { while (true) { // wait for the signal to start working or exit lock lk_waiting(m_waiting); // add worktime cv_waiting.wait(lk_waiting, [tid]() { return waiting > 0 && worktime[tid] || exit_flag.load(); }); if (exit_flag.load() && !worktime[tid]) { break; } if (worktime[tid]) { // fetch a task, decrease the waiting task count waiting--; worktime[tid] = false; lk_waiting.unlock(); // do the work data[tid] += 1; // increase the finished task count lock lk_finished(m_finished); finished++; cv_finished.notify_one(); lk_finished.unlock(); } } }; };\",\"and update the worktime at the beginning of every circle.\",\"for (int i = 0; i < 1000000; i++) { // update the waiting task count lock lk_waiting(m_waiting); waiting = 5; //update the worktime std::fill(worktime.begin(), worktime.end(), true); // notify all threads to start working cv_waiting.notify_all(); lk_waiting.unlock(); // wait for all threads to finish lock lk_finished(m_finished); cv_finished.wait(lk_finished, []() { return finished == 5; }); // clear the finished task count, enter the next round finished = 0; lk_finished.unlock(); printf(\\\"finish %d %d %d %d %d %d\\\\n\\\", i, data[0], data[1], data[2], data[3], data[4]); }\",\"then we can get check passed!\"]},\"13\":{\"h\":\"step2: exit sucessfully\",\"t\":[\"this issue is easy to find out the reason, we can see it in the \\\"WARNING: ThreadSanitizer: data race (pid=30965)\\\".\",\"By analysing the warning, we can easily find the issue occur when the main thread finished and try to destroy the global variables, but other threads are using them.\",\"so we just need to use a variable to notify the thread to stop and use join() function to let main thread wait other threads.\",\"// notify the thread to stop exit_flag.store(true); cv_waiting.notify_all(); // use join() to wait for (auto &t : threads) { if (t -> joinable()) { t -> join(); } delete t; }\",\"then we can exit sucessfully!\",\"check passed!\"]},\"14\":{\"h\":\"my answer\",\"t\":[\"/* * This task has 5 threads, each thread will increase the corresponding element * in the data vector by 1. The main thread will run 1000000 rounds, in each * round, it will notify all threads to start working, and wait for all threads * to finish. * * After 1000000 rounds, the main thread will check if all elements in the * data vector are 1000000. * * 1. Find the bug in the code, and fix it. * 2. The code can bot exit normally, fix it, ensure all working threads are * finished before the main thread exit. * */ #include <condition_variable> #include <cstdio> #include <cstdlib> #include <mutex> #include <thread> #include <vector> #include <atomic> using std::mutex; using std::thread; using lock = std::unique_lock<std::mutex>; using condvar_t = std::condition_variable; template <typename T> using vec_t = std::vector<T>; mutex m_waiting; mutex m_finished; mutex m_workonce; condvar_t cv_waiting; condvar_t cv_finished; condvar_t cv_workonce; std::atomic<bool> exit_flag(false); int waiting = 0; int finished = 0; // the data to be processed vec_t<int> data(5, 0); vec_t<bool> worktime (5, 0); auto main() -> int { vec_t<thread*> threads(5); auto create_func = [](int tid) { return [tid]() { while (true) { // wait for the signal to start working or exit lock lk_waiting(m_waiting); cv_waiting.wait(lk_waiting, [tid]() { return waiting > 0 && worktime[tid] || exit_flag.load(); }); if (exit_flag.load() && !worktime[tid]) { break; } if (worktime[tid]) { // fetch a task, decrease the waiting task count waiting--; worktime[tid] = false; lk_waiting.unlock(); // do the work data[tid] += 1; // increase the finished task count lock lk_finished(m_finished); finished++; cv_finished.notify_one(); lk_finished.unlock(); } } }; }; for (int i = 0; i < 5; i++) { threads[i] = new thread(create_func(i)); } // run 1000000 rounds for (int i = 0; i < 1000000; i++) { // update the waiting task count lock lk_waiting(m_waiting); waiting = 5; std::fill(worktime.begin(), worktime.end(), true); // notify all threads to start working cv_waiting.notify_all(); lk_waiting.unlock(); // wait for all threads to finish lock lk_finished(m_finished); cv_finished.wait(lk_finished, []() { return finished == 5; }); // clear the finished task count, enter the next round finished = 0; lk_finished.unlock(); printf(\\\"finish %d %d %d %d %d %d\\\\n\\\", i, data[0], data[1], data[2], data[3], data[4]); } exit_flag.store(true); cv_waiting.notify_all(); for (auto &t : threads) { if (t -> joinable()) { t -> join(); } delete t; } // check, all element in data should be 1000000 for (int i = 0; i < 5; i++) { if (data[i] != 1000000) { printf(\\\"error: data[%d] = %d\\\\n\\\", i, data[i]); return 1; } } printf(\\\"check passed\\\\n\\\"); return 0; }\"]},\"15\":{\"c\":[\"ASC\"]},\"16\":{\"c\":[\"TEST\",\"ASC\",\"2025competition\"]},\"17\":{\"h\":\"ASC2025 TEST_1_cluster\"},\"18\":{\"h\":\"Installation\",\"t\":[\"it's easy to finish the envirment settings, so I just talk about some issues occuered.\"]},\"19\":{\"h\":\"step1: unzip the\",\"t\":[\"tar -xvf baseenv.tar.gz\",\"may the .tar.gz file be unziped to many files. my solution is packing them in a new .tar file by tar cvf bassenv.tar bassenv.\",\"then we can get the package baseenv.tar.\"]},\"20\":{\"h\":\"step2: Load the Docker image from the .tar file\",\"t\":[\"docker load -i baseenv.tar\"]},\"21\":{\"h\":\"step3: start the container with the following command\",\"t\":[\"#node1 docker run -it --cap-add NET_ADMIN --name node1 baseenv:latest /bin/bash #node2 docker run -it --cap-add NET_ADMIN --name node2 baseenv:latest /bin/bash\",\"However, if we want to make sure not connect to the internet from the container, we can use the below instruction:\",\"#node1 docker run -it --cap-add NET_ADMIN --sysctl net.bridge.bridge-nf-call-iptables=0 --name node1 baseenv:latest /bin/bash\",\"reference: https://cloud.tencent.com/developer/article/2335144\",\"this allows us to access intranet but not to access the Internet in the container.\"]},\"22\":{\"h\":\"task\"},\"23\":{\"h\":\"task1: compile the\",\"t\":[\"reference: https://blog.csdn.net/u014185088/article/details/121482116\",\"after create the continer, we can cd root and ues ls to find the mpich-4.3.0b1, then unzip it.\"]},\"24\":{\"h\":\"step1: set mpich configure and make it\",\"t\":[\"cd mpich-4.3.0b1 # use --disable-fortran to ban the fortran setting ./configure --prefix=/root/mpich --disable-fortran # make it make && make install\",\"then set PATH:\",\"vim ~/.bashrc # write export MPIPATH=/root/mpich export MPIPATHBIN=$MPIPATH/bin export MPIPATHINCLUDE=$MPIPATH/include export MPIPATHLIB=$MPIPATH/lib export MPIPATHSHARE=$MPIPATH/share export PATH=$PATH:$MPIPATHBIN:$MPIPATHINCLUDE:$MPIPATHLIB:$MPIPATHSHARE\",\"then we can see the mpicc version:\",\"let's try a mpi programm in a continer.\",\"I copy a test.cpp from the Internet.\",\"#include <stdio.h> #include <mpich/mpi.h> #include <string.h> #include <stdlib.h> #include <math.h> #include <time.h> #include<omp.h> #include<iostream> using namespace std; //生成随机矩阵 int *generate_matrix(int size) { srand((unsigned)time(NULL) + (unsigned)rand()); int *matrix; matrix = (int *)malloc(sizeof(int) * size*size); for (int i = 0; i < size*size; i++) { matrix[i] = rand() % 10; } return matrix; } //输出矩阵 void print_matrx(int *a, int size) { for (int i = 0; i < size; i++) { for (int j = 0; j < size; j++) { printf(\\\"%d \\\", a[i*size + j]); } printf(\\\"\\\\n\\\"); } printf(\\\"\\\\n\\\"); } //矩阵相乘 int * Multiplication(int a[], int b[], int size, int line) { int *result; int temp = 0; result = (int *)malloc(sizeof(int) * size*size); //#pragma omp parallel for num_threads(2) for (int i = 0; i < line; i++) { for (int j = 0; j < size; j++) { temp = 0; for (int k = 0; k < size; k++) temp += a[i*size + k] * b[k*size + j]; result[i*size + j] = temp; } } return result; } int main(int argc, char *argv[]) { clock_t time1, time2; int size = 16, rank, line, num; time1 = clock(); MPI_Init(&argc, &argv); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &num); int *matrix1; int *matrix2; int *matrix3; int *resultMg; int *revMg; int *resultMg0; line = size / num ; //num为进程数，line为每个进程的行数 matrix1 = (int*)malloc(sizeof(int)*size*size); matrix2 = (int*)malloc(sizeof(int)*size*size); matrix3 = (int*)malloc(sizeof(int)*size*size); resultMg = (int*)malloc(sizeof(int)*size*line); resultMg0 = (int*)malloc(sizeof(int)*size*line); revMg = (int*)malloc(sizeof(int)*size*line); if (rank == 0) { matrix1 = generate_matrix(size); matrix2 = generate_matrix(size); printf(\\\"matrix1 is :\\\\n\\\"); print_matrx((int *)matrix1, size); printf(\\\"matrix2 is :\\\\n\\\"); print_matrx((int *)matrix2, size); resultMg0=Multiplication(matrix1,matrix2, size, line); for (int m = 0; m < line; m++) for (int n = 0; n < size; n++) matrix3[m*size + n] = resultMg0[m*size + n]; for (int i = 1; i < num; i++) MPI_Send(matrix2, size*size, MPI_INT, i, 0, MPI_COMM_WORLD); for (int i = 1; i < num; i++) MPI_Send(matrix1 + i*line*size, size*line, MPI_INT, i, 1, MPI_COMM_WORLD); for (int i = 1; i < num; i++) { MPI_Recv(resultMg, line*size, MPI_INT, i, 3, MPI_COMM_WORLD, MPI_STATUS_IGNORE); for (int m = 0; m < line; m++) for (int n = 0; n < size; n++) matrix3[(i*line + m)*size + n] = resultMg[m*size + n]; } time2 = clock(); print_matrx((int *)matrix3, size); cout << time2 - time1 << endl; free(matrix1); free(matrix2); free(matrix3); free(revMg); free(resultMg); } else { MPI_Recv(matrix2, size*size, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE); MPI_Recv(revMg, size*line, MPI_INT, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE); resultMg = Multiplication(revMg, matrix2, size, line); MPI_Send(resultMg, line*size, MPI_INT, 0, 3, MPI_COMM_WORLD); } MPI_Finalize(); return 0; }\",\"then use the below instruction copy the code to the continer.\",\"docker cp /media/user/Data/biancheng/code/ASC/ASC_2025/test/optimize-tasks/1.cluster/test.cpp node1:/root\",\"try to compile the test.cpp.\",\"oops, we seem to need to use the absolute address.\",\"// fix the code #include <mpich/mpi.h> // right code #include \\\"/root/mpich/include/mpi.h\\\"\",\"then we can find mpi\",\"run the programm\",\"mpiexec -n 4 ./mpi\",\"check passed!\"]},\"25\":{\"h\":\"task2: configure the network for containers\",\"t\":[\"creat a new net for continers\",\"docker network create cluster\",\"and add the node1 & node2 into the net.\",\"docker network connect cluster node1 docker network connect cluster node2\",\"then try to ping node2 from node1.\",\"check passed!\"]},\"26\":{\"h\":\"task3: configure ssh for the containers\",\"t\":[\"Firstly, we need to set a password for ssh.\",\"passwd\",\"my password is set to asc2025.\",\"we need to change the PermitRootLogin prohibit-password to PermitRootLogin yes, and change port from 22 to 8080.\",\"vim /etc/ssh/sshd_config\",\"use docker commit to build a new image.\",\"docker commit -m 'ssh' -a 'node1:ssh' node1 node1:ssh\",\"then use the new image to build two new nodes to set port.\",\"docker run -it -d -p8088:8080 --cap-add NET_ADMIN --sysctl net.bridge.bridge-nf-call-iptables=0 --name node1_ssh node1:ssh /bin/bash\",\"start the new continer, and start the ssh.\",\"/usr/sbin/sshd\",\"then we can use password to connect with the continer from the host.\",\"then we can try to login in the node1 from node2.\",\"check passed!\"]},\"27\":{\"h\":\"task4: run the MPI program in the cluster\",\"t\":[\"then we can use a test programm to test the connection.\",\"#include \\\"/root/mpich/include/mpi.h\\\" #include <stdio.h> #include <math.h> int main(int argc, char *argv[]) { int myid, numprocs; int namelen; char processor_name[MPI_MAX_PROCESSOR_NAME]; MPI_Init(&argc,&argv); MPI_Comm_rank(MPI_COMM_WORLD,&myid); MPI_Comm_size(MPI_COMM_WORLD,&numprocs); MPI_Get_processor_name(processor_name,&namelen); fprintf(stderr,\\\"Hello World! Process %d of %d on %s\\\\n\\\", myid, numprocs, processor_name); MPI_Finalize(); }\",\"we can change the name of the container from the host configure, just I didn't do it.\",\"check passed!\",\"tips: the container should exist the hello and the environment of the MPI.\"]},\"28\":{\"c\":[\"ASC\"]},\"29\":{\"c\":[\"TEST\",\"ASC\",\"2025competition\"]},\"30\":{\"h\":\"MLSys_分布式开发\"},\"31\":{\"h\":\"为什么AI需要分布式系统\"},\"32\":{\"h\":\"什么是分布式系统\",\"t\":[\"分布式系统顾名思义，就是将单一计算机节点要做的任务分布在多个计算机节点上完成的，各个节点之间通过网络进行通讯的计算机系统。\",\"这种系统的好处是显而易见的，我们可以把一台机器的Task进行切分，可能大幅度提升计算效率；我们可以把一台机器存不下的任务放到多个结点里面，拓展数据规模。\",\"但同时也可能引入更多的问题，多台机器之间通讯耗费的时间会不会比原先计算的时间更长？切分后的任务如何再将结果重新合在一起？这些都问题都有进一步研究的价值。\"]},\"33\":{\"h\":\"为什么AI训练需要分布式系统\",\"t\":[\"在训练AI时，无论在CV方向还是NLP等其他方向，无论使用哪一种模型，总是绕不开训练集和测试集这两部分，我们总要拿出一部分数据用来预训练模型，另一部分用来测试模型效果。\",\"数据集越大，训练效果越好，这基本上是在不考虑机器性能上限的情况下，我们针对AI训练达成的一种共识，所以在训练AI时我们希望能使用尽可能大的数据集，使用尽可能多的参数，来对尽可能多的标签进行刻画。但可惜一台机器的性能总是有限的。\",\"这个时候就不得不引入分布式系统来改善这种局面了，如果我们一台机器、一张GPU/TPU没办法高效完成我们的运算，那我们可以分到多张卡上面；如果我们一台机器存储不开我们的数据集，那我们可以分到多台设备上，或者考虑让CPU也存储一部分模型数据。这就是AI使用分布式系统想要解决的问题。\"]},\"34\":{\"h\":\"分布式系统如何发挥作用\"},\"35\":{\"h\":\"数据分布式的一些基本想法\",\"t\":[\"如果我们希望分布式系统在AI训练中发挥他的力量，我们要怎么做呢？大家都对ML有一些基本的认识了，无论哪一种模型，在训练过程中总会有数据（输入、输出），模型（参数、激活函数）等等需要保存的东西。\",\"现在假定我们要做一个训练任务，我们有我们自己的很大的数据集，我希望能把这个数据很快的训练完，那我们就可以考虑把数据切分成好几份，然后给不同的GPU去算每一个单独的部分，这种每个一份数据切分成多份，给不同的GPU做计算的方式，但每一个GPU均做保留完整的模型做计算，被我们称作数据并行（Data Patallel，简称DP）。\",\"既然可能有很大的数据集需要切分，那自然也可能有很大的模型进行切分，让每一个GPU仅保留几个隐藏层的模型（参数、激活函数），这样可以训练更大的模型，提升训练精度，这种按层切分模型，不切分数据的方式，被称作模型并行（Model Patallel，简称MP）。\",\"这种按层切分的方式固然可以增加可容纳的模型的大小，但是仅让一个卡存模型的几层在计算某些必须要用到之前的数据的模型时可能不尽如人意，通讯成本会比较高昂。\",\"为了解决层切分的弊端，我们可以考虑将计算过程中的算子/计算矩阵进行切分，让每一张卡只保留必须的参数和算子，产生部分输出，这样就可以将每一部分计算矩阵进行并发处理，我们将这种方式称作张量并行/算子并行（Tensor Patallel，简称TP），谷歌专门为TP开发了TPU进行并发适配，TP也是应用较广的基本并发方式。\",\"既然我们有了对数据切分的方法，有了对模型算子切分的方法，那我们也可以考虑进行结合，既切分优化器状态，又切分模型的参数、梯度，这种并行方式被称作完全分片数据并行（Fully Sharded Data Parallel，简称FSDP）。\",\"前面所提及的方法，是在利用切分层内数据/优化器状态/模型绕过MP方法按层切分时可能带来的通讯瓶颈，但是也可以利用类似CPU指令流水线执行的方式，进行数据计算，切分模型让不同GPU延后计算开始时间，从而保证通讯无boundary，这种方式被称作流水线并行（Pipe Parallel，简称PP）。\",\"在探讨了DP、PP、TP基本并行方式后，我们可以考虑将三种并行方式进行综合，考虑一些综合利用并行方式的策略，这种并行考量被称为3D并行。\",\"事实上对于采用哪种并行模式，要用多少张卡进行并行，并行中使用的参数如何调整，是一件非常复杂的事情，我们期望有可以自动为我们选定并行方法的策略，这种策略被称作自动并行。\"]},\"36\":{\"h\":\"数据并行（DP）\"},\"37\":{\"h\":\"典型数据并行的流程\",\"t\":[\"分配n块计算GPU（图中0-2）;1块梯度收集GPU\",\"每块GPU均拷贝一份完整的模型\",\"把一份Data（也可以是一个batch）切分成若干份给不同的GPU\",\"每一块GPU完成Forward和Backward后，计算出本地的梯度\",\"把本地梯度push到梯度收集GPU，梯度收集GPU聚合梯度\",\"计算GPU从聚合GPU中pull完整梯度，更新模型参数，保证各个计算GPU模型同步\",\"聚合再下发梯度操作被称为AllReduce\"]},\"38\":{\"h\":\"Horovod\",\"t\":[\"Horovod: fast and easy distributed deep learning in TensorFlow\",\"https://arxiv.org/abs/1802.05799https://github.com/uber/horovod\",\"传统的DP在带来使用大数据集可能的同时，又同时增加了额外的通讯开销，本文还指出DP代码重构成本较大。\",\"本文通过在不同数量卡上训练结果进行说明：\",\"可以明显看到GPU数量增多时，TensorFlow框架下的通讯开销越大，在128卡时甚至已经超过训练开销。\",\"为了解决传统DP计算节点和聚合节点比例不好确定导致的通讯成本/计算成本过大的问题，本文指出了ring-allreduce的方式。\"]},\"39\":{\"h\":\"Ring-AllReduce\",\"t\":[\"假设有4块GPU，每块GPU上的数据也对应被切成4份。AllReduce就是要让每块GPU上的数据都变成箭头右边汇总的样子。\",\"Ring-AllReduce将这个过程分为Reduce-Scatter和All-Gather。\"]},\"40\":{\"h\":\"Reduce-Scatter\",\"t\":[\"定义网络拓扑关系，使得每个GPU只和其相邻的两块GPU通讯。每次发送对应位置的数据进行累加。每一次累加更新都形成一个拓扑环。\",\"3次更新之后，每块GPU上都有一块数据拥有了对应位置完整的聚合（图中红色）。此时，Reduce-Scatter阶段结束。进入All-Gather阶段。目标是把红色块的数据广播到其余GPU对应的位置上。\"]},\"41\":{\"h\":\"All-Gather\",\"t\":[\"相邻GPU对应位置进行通讯，对应位置数据不再做相加，而是直接替换\"]},\"42\":{\"h\":\"Horovod工作\",\"t\":[\"将百度的 TensorFlow ring-allreduce 算法的实现转化为一个独立的 Python 包，命名为 Horovod\",\"使用 NCCL 库实现了 TensorFlow ring-allreduce，并优化了性能\",\"添加了对单机多卡的支持\",\"改进了 API，添加 broadcast 操作，仅需 4 步即可使用 Horovod\"]},\"43\":{\"h\":\"使用方法\",\"t\":[\"import tensorflow as tf import horovod.tensorflow as hvd # 初始化 Horovod hvd.init() # 固定 GPU 以处理本地 rank（每个进程一个 GPU） config = tf.ConfigProto() config.gpu_options.visible_device_list = str(hvd.local_rank()) # 构建模型... loss = ... opt = tf.train.AdagradOptimizer(0.01) # 添加 Horovod 分布式优化器 opt = hvd.DistributedOptimizer(opt) # 添加hook，在初始化期间将变量从 rank 0 广播到所有其他进程 hooks = [hvd.BroadcastGlobalVariablesHook(0)] # 创建训练操作 train_op = opt.minimize(loss) # MonitoredTrainingSession 负责会话初始化、从检查点恢复、保存到检查点以及在完成或发生错误时关闭 with tf.train.MonitoredTrainingSession(checkpoint_dir=\\\"/tmp/train_logs\\\", config=config, hooks=hooks) as mon_sess: while not mon_sess.should_stop(): # 执行同步训练 mon_sess.run(train_op)\"]},\"44\":{\"h\":\"PyTorch DDP\",\"t\":[\"PyTorch Distributed: Experiences on Accelerating Data Parallel Training\",\"https://arxiv.org/abs/2006.15704https://github.com/pytorch/pytorch/\"]},\"45\":{\"h\":\"Python前端API\",\"t\":[\" 1 import torch 2 import torch.nn as nn 3 import torch.nn.parallel as par 4 import torch.optim as optim 5 6 # initialize torch.distributed properly 7 # with init_process_group 8 9 # setup model and optimizer 10 net = nn.Linear(10, 10) 11 net = par.DistributedDataParallel(net) 12 opt = optim.SGD(net.parameters(), lr=0.01) 13 14 # run forward pass 15 inp = torch.randn(20, 10) 16 exp = torch.randn(20, 10) 17 out = net(inp) 18 19 # run backward pass 20 nn.MSELoss()(out, exp).backward() 21 22 # update parameters 23 opt.step()\",\"PyTorch在设计API时做到了仅调用第11行代码中的DistributedDataParallel部分，就可以实现从本机训练到分布式训练的部署。\"]},\"46\":{\"h\":\"梯度同步算法\",\"t\":[\"论文中所提到的DP背景本篇blog前文均有提及，不再赘述。\"]},\"47\":{\"h\":\"传统做法\",\"t\":[\"batch较小时，通讯效率低\",\"计算与聚合之间存在间隔，很难做到即时通讯。\"]},\"48\":{\"h\":\"改进做法\",\"t\":[\"PyTorch使用Gradient Bucketing，在小batch时，选择收集到一定量的梯度，再做聚合和同步。\",\"PyTorch使用hook机制，在反向传播计算完成后，调用自定义函数，当在同一个bucket中的梯度的hook都被调用后，就调用AllReduce对该bucket进行通信。\",\"这种方式有两个问题需要注意：\",\"（1）由于每个机器（进程）是独立计算的，因此不同机器之间处理的bucket的顺序将会不一致，这会导致梯度同步结果出现错误。因此，我们需要保证不同机器处理bucket的顺序一致。\",\"使用参数的反序作为梯度放入bucket的顺序。依据是，后向传播的顺序与梯度更新的顺序大致可认为是相同的。\",\"（2）不同迭代中，使用的参数可能不相同，使得某些参数的梯度不需要用到。\",\"前向传播结束后从输出开始遍历计算图，记录哪些参数参与计算，哪些参数没有参与计算，对于没有参与计算的参数，则直接标记为ready。\"]},\"49\":{\"h\":\"集合通讯库\",\"t\":[\"PyTorch DDP支持三种通讯库：NCCL，Gloo和MPI。DDP支持用户使用统一的API ProcessGroup来调用不同的集合通讯库。\",\"NCCL doc:https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api.html\",\"Gloo doc:https://docs.solo.io/gateway/latest\",\"MPI doc:https://www.open-mpi.org/doc/\"]},\"50\":{\"h\":\"FSDP并行\"},\"51\":{\"h\":\"ZeRO\",\"t\":[\"ZeRO: memory optimizations toward Training trillion parameter Models\",\"https://github.com/microsoft/DeepSpeedhttps://arxiv.org/abs/1910.02054\"]},\"52\":{\"h\":\"Abstract\",\"t\":[\"深度学习模型在训练万亿级别参数时存在根本性限制，现有DP（数据并行）、MP（模型并行）方法无法满足日益增长的参数量需求，ZeRO（零冗余优化器）消除了数据和模型并行训练中的内存冗余，同时保持了低通信量和高计算粒度（计算与通信比率的定量或定性度量），使我们能够根据设备数量按比例扩展模型大小，同时保持高效率，实现超线性优化。\"]},\"53\":{\"h\":\"Extended Introduction\",\"t\":[\"巨量参数可以大幅提升NLP处理能力，但参数量的上升在传统的单机GPU、TPU运算中无以为继，简单增加机器数量也用处不大。\",\"现有的PP（流水线并行）、MP都在通信和计算效率之间权衡，但重点是计算规模和速度。\",\"现有系统在模型训练中内存消耗的全部范围，并将其分为两部分:\",\"1）对于大型模型，【大部分内存】被模型状态占用，包括优化器状态、梯度和参数（不得不保存的模型内容）。\",\"2）剩余内存被激活、临时缓冲区和不可用的碎片内存所消耗，本文将其统称为残差状态（residual states）。\"]},\"54\":{\"h\":\"Where Did All the Memory Go\",\"t\":[\"在模型计算的过程中，只有很少一部分被过程中产生的冗余量使用了（residual states），大部分内存都被用于模型本身数据的存储和运算（model states），ZeRO使用不同的方式优化这两种内存使用。\"]},\"55\":{\"h\":\"Model States: Optimizer States, Gradients and Parameters\",\"t\":[\"论文以Adam优化器为例说明了模型造成的内存浪费是一件难以接受的事情，Adam本身对每一个参数都需要保留momentum和variance两个参数进行数据更新。\",\"看上去这是仅仅由1变2的内存保留变化，但是由于NVIDIA对于fp16精度计算的高优化度，要服务于计算速度内存变化就会发生膨胀。\",\"一般来说，在计算时，参数、输入、输出，均采用fp16半精度运算，但是考虑在更新权重的时候，可能在模型训练过程中梯度不大等原因，如果还是使用半精度进行运算，可能并不会产生权重累计，导致模型训练失效，所以此时应采用fp32的全精度计算，这又是一个仅在计算时才能用到的额外copy。\",\"FP32:1位符号位，8位指数位，23位尾数位 FP16:1位符号位，5位指数位，10位尾数位 BF16:1位符号位，8位指数位，7位尾数位\",\"假设有Ψ个参数，那就有4Ψ个byte（fp16）存储模型的参数和输入，同时又有12Ψ个byte（fp32）存储momentum和variance（Adam），也就是单计算仅需4Ψ，但是在更新参数进行新的计算时，需要额外的12Ψ，本文将其记作2Ψ+2Ψ+KΨ，其中K取决于模型。\"]},\"56\":{\"h\":\"Residual Memory Consumption\"},\"57\":{\"h\":\"Temporary buﬀers\",\"t\":[\"是指通讯过程中、计算过程中产生的一些临时数据.\"]},\"58\":{\"h\":\"Memory Fragmentation\",\"t\":[\"是指碎片化内存，用pytorch等使用虚拟内存分配方式的库时，可能内存池的维护并不能做到完全利用，论文假设有30%内存其实根本无法使用。\"]},\"59\":{\"h\":\"ZeRO: Insights and Overview\",\"t\":[\"这一部分主要介绍作者使用ZeRO的一些想法。\"]},\"60\":{\"h\":\"Insights and Overview: ZeRO-DP\",\"t\":[\"a）数据并行比模型并行更好，效率更高，因为通讯更少，计算粒度更精细\",\"b）数据并行内存使用并不高效，因为每一个分发计算都需要完全copy所有数据\",\"c）两种并行都需要存储模型状态变量，单就这一块部分内存而言，两者使用均不高效。\",\"我们可以考虑在某一个GPU中存储和更新参数，而在其他GPU需要使用其进行计算时，再进行通讯获取参数，从而降低内存占用。\"]},\"61\":{\"h\":\"Insights and Overview: ZeRO-R\"},\"62\":{\"h\":\"Reducing Activation Memory\",\"t\":[\"a）MP占据模型内存，但是需要时常更新\",\"b）大模型即使的带宽小的情况下，也可以考虑每个GPU都各自保存和重算部分数据\",\"ZeRO考虑可以使用将模型内存切分成多分，分开重算，多次通讯的方式收集数据，减少内存使用。\"]},\"63\":{\"h\":\"Managing Temporary buﬀers\",\"t\":[\"对于临时缓存采用开一段固定大小内存的方式进行反复存储。\"]},\"64\":{\"h\":\"Managing fragmented Memory.\",\"t\":[\"内存碎片通过对不同寿命的内存进行整理，减少内存释放和分配的时间\"]},\"65\":{\"h\":\"Deep Dive into ZeRO-DP\"},\"66\":{\"h\":\"ZeRO1\",\"t\":[\"对优化器本身存储的fp32数据进行切分，使每个GPU仅留1/N份数据。\"]},\"67\":{\"h\":\"ZeRO2\",\"t\":[\"对数据并行中存储的fp16梯度进行切分，使每个GPU仅留1/N份数据。\"]},\"68\":{\"h\":\"ZeRO3\",\"t\":[\"对数据并行中存储的fp16参数进行切分，使每个GPU仅留1/N份数据，使用时通讯获取完整参数。\",\"ZeRO1/2依托allreduce算法实现，NVIDIA本身支持这种优化并不会有通讯增加，但是ZeRO3需要对参数进行切分和更新，每次都会有Ψ 的额外通讯开销，但是事实上可以考虑在不同隐藏层中实现异步更新参数 ，使得参数额外开销尽可能减少。\"]},\"69\":{\"h\":\"Deep Dive into ZeRO-R\"},\"70\":{\"h\":\"Partitioned Activation Checkpointing\",\"t\":[\"采用Megatron的模型并行方式，每个GPU保存1/N参数，对切分后部分输入分开运算，使用reduce-scatter更新各个参数状态，与ZeRO-DP的区别是，计算参数时，每个GPU都没有保存或通讯获取完整的参数，而是对输出进行通讯和更新。\"]},\"71\":{\"h\":\"Constant Size Buﬀers\",\"t\":[\"使用固定大小buffer指定数据的单批发送量，保证带宽不浪费。\"]},\"72\":{\"h\":\"Memory Defragmentation\",\"t\":[\"将数据池分为两部分，一部分存储大批量的计算数据，另一部分动态存储临时数据。\"]},\"73\":{\"h\":\"ZeRO-Offload\",\"t\":[\"ZeRO-Offload：Democratizing Billion-Scale Model Training\",\"https://arxiv.org/pdf/2101.06840https://arxiv.org/pdf/2101.06840\"]},\"74\":{\"h\":\"背景\",\"t\":[\"GPU内存占用是一件非常昂贵的事情，在过去训练中人们往往忽略了CPU的计算潜力，高估了GPU的存储性能。\",\"ZeRO-Offload 优化：尽量减少数据在 GPU 与 CPU 之间的移动，并减少 CPU 计算时间，同时最大限度地节省 GPU 上的内存。\"]},\"75\":{\"h\":\"Efficiency\",\"t\":[\"论文提出了一种名为Efficiency的offload策略，通过分析确定了CPU和GPU设备之间的最佳计算和数据划分策略，以在三个关键方面达到最优化：\",\"在CPU上的计算量比GPU少多个数量级，防止CPU性能瓶颈；\",\"最小化CPU和GPU之间的通信量，防止通信瓶颈；\",\"在实现最小通信量的同时，可证明地最大化节约GPU内存。\",\"offload 优化器计算要求CPU进行O(M)次计算，而GPU需进行O(MB)次计算，其中M和B分别为模型规模和 batch size 。在大多数情况下， batch size 较大，CPU计算量并不是瓶颈，但对于小 batch size，CPU计算量可能成为瓶颈。为了解决这个问题，采取了两种优化措施：\",\"高效的CPU优化器，其速度比现有技术快6倍；\",\"延迟一步的参数更新，允许将CPU优化器步骤与GPU计算重叠，同时确保准确性。这两种措施共同保证了ZeRO-Offload在小 batch size 下也能保持效率。\"]},\"76\":{\"h\":\"Unique Optimal Offload Strategy\",\"t\":[\"为了确定最佳的下载策略，ZeRO-Offload将深度学习训练建模为数据流图，将该图分割为CPU和GPU设备之间的部分。\",\"训练的计算复杂度通常为O(MB)，其中M为模型大小，B为有效batch size。为避免CPU计算成为瓶颈，只有那些计算复杂度低于O(MB)的计算才能转移到CPU上\",\"FWD 和 BWD 的计算复杂度都是O(MB)，必须在GPU上进行，而其余的计算，如范数计算、权重更新等，其复杂度为O(M)，可以转移到CPU上\",\"还需要最小化 CPU 与 GPU 的通信带宽，如图中所示，最小通信量为 BWD后 GPU 发送到 CPU 的 2M 梯度与 CPU 发送到 GPU 的 2M 参数，只有将 fp32 模型状态（momentum 32、variance 32和p32），Param Update 和 float2half 计算放置在一起，为一个 CPU 上的 Update Super Node，才能达成最小通信量策略\"]},\"77\":{\"h\":\"ZeRO-Offload Schedule\"},\"78\":{\"h\":\"单卡策略\",\"t\":[\"ZeRO-Offload将数据进行分区，将fp16参数存储在GPU上，fp16梯度和所有优化器状态存储在CPU上。\",\"在训练过程中，首先通过 FWD 计算损失。由于fp16参数已经位于GPU上，因此这部分计算不需要与CPU进行通信。\",\"在 BWD 过程中，不同参数的梯度在后向调度的不同位置计算。ZeRO-Offload可以立即将这些梯度逐个或切分传输到 CPU 内存中。\",\"因此，在将梯度传输到CPU内存之前，只需要在GPU内存中临时保存少量的梯度。此外，每个梯度传输可以与反向计算重叠，消除大部分通信成本。\",\"在 BWD 之后，ZeRO-Offload在CPU上直接更新fp32参数和剩余的优化器状态，并将更新后的 fp32 参数从 CPU 内存复制到 GPU 内存中的fp16参数中。\",\"下图GPU与CPU二次通信应该是从CPU到GPU\",\"for_parallel rank in range(world_size): # 初始化每个进程的层 initialize_layers() for batch in dataset: # 前向传播 x = forward(batch) # 计算损失并反向传播 compute_loss(x, batch).backward() # 反向传播梯度 backward(x.grad) # 更新参数 step() def _is_owner(i): # 判断当前进程是否拥有第 i 层 return True if rank owns i else False def initialize_layers(): for i in range(num_layers): l = layers[i] # 在 GPU 上分配半精度参数 allocate_on_gpu l.param_fp16 if _is_owner(i): # 在 CPU 上分配全精度参数、优化器状态和梯度 allocate_on_cpu l.param_fp32 allocate_on_cpu l.optim_states_fp32 allocate_on_cpu l.param_grad def forward(x): # 前向传播逻辑 for i in range(num_layers): x = layers[i].forward(x) return x def backward(dx): # 反向传播逻辑 for i in range(num_layers, 0, -1): dx = layers[i].backward(dx) # 将梯度减少到拥有该层的进程 reduce(layers[i].grad, dest_rank = _owner_rank(i)) if _is_owner(i): # 将梯度复制到 CPU l.cpu_grad.copy(l.grad) else: pass # 删除 GPU 上的梯度 del layers[i].grad def step(): # 参数更新逻辑 for i in range(num_layers): l = layers[i] if _is_owner(i): # 在 CPU 上更新参数 update_in_cpu(l.optim_states_fp32, l.cpu_grad, l.param_fp32) # 将更新后的参数复制回 GPU l.param_fp16.copy(l.param_fp32) # 广播更新后的参数 BROADCAST(l.param_fp16, src = _owner_rank(i))\"]},\"79\":{\"h\":\"多卡策略\",\"t\":[\"ZeRO-Offload 将梯度和优化器状态在不同的 GPU 之间进行 partition，并且每个 GPU 将自己的 part offload 到 CPU 内存中，存储持续整个训练过程\",\"BWD 过程中，在 GPU 上 reduce-scatter 计算梯度并平均，每个 GPU 仅将属于其 part 的平均梯度 offload 到 CPU 内存中\",\"一旦梯度在 CPU 上可用，优化器状态 part 对应的每个 DP 进程直接在 CPU 上并行更新对应的参数 part\",\"更新完成后，参数 part 发送到 GPU，在 GPU 上对参数进行类似 ZeRO-2 的 all-gather 操作\"]},\"80\":{\"h\":\"流水线并行\"},\"81\":{\"h\":\"GPipe\",\"t\":[\"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism\",\"https://arxiv.org/abs/1811.06965\"]},\"82\":{\"h\":\"Gpipe Abstract\",\"t\":[\"增大模型规模可以提升模型效果，但是单卡/单机GPU内存有限，必须引入分布式系统\",\"GPipe使用模型并行（MP）方案，将模型切分成一连串stage，每个stage放在独立的设备（GPU/TPU）上，实现对超大规模模型的支持\",\"利用Pipeline（PP）的方案，提高了模型并行模式下的设备利用率\",\"最终GPipe通过更大规模的模型和更大的batch_size，在图像和NLP的模型上都得到更好的模型效果。\"]},\"83\":{\"h\":\"Design\"},\"84\":{\"h\":\"Naive Model Parallelism\",\"t\":[\"论文中提到的MP就是传统的MP方案，不是Magatron-LM提出的新MP方案（事实上是TP），对数据按层切分，而非对所有输入、输出、参数都按块切分。\",\"模型有 12 层（layer），可以切为 4 份（4个cell），每份 3 层。然后每份放到一块 GPU 上\",\"第 k 个 cell 的模型参数，就放在第 k 块 GPU 上。\",\"Fk和 Bk 分别表示第 k 个 cell 的 forward 和 backward 计算\",\"单批量以这种顺序进行计算，Fk和 Bk 分别表示第 k 个批次的forward和backward运算（注意与上张图不同），每一种颜色代表一块 GPU，每一列代表一个时间段\",\"问题是：每块 GPU 都会有大量的空闲时间\"]},\"85\":{\"h\":\"Pipeline Parallelism 1 - Split into micro-batches\",\"t\":[\"单batch（mini-batch）切成更小的micro-batch，然后流水线并行（类似CPU指令执行）\",\"为什么不流水线并行batch，而是切分后再流水线并行？ 多batch可以提速，但占用空间会多很多 多batch训练时，可能会导致梯度更新不稳定，结果收敛不明显\"]},\"86\":{\"h\":\"Pipeline Parallelism 2 - re-materialization\",\"t\":[\"GPU 只保留最开始的输入，中间结果全部丢掉；计算梯度时，再重新计算这些中间结果。\",\"减少计算时内存占用，但要增加计算时长。\"]},\"87\":{\"h\":\"张量并行（TP）\"},\"88\":{\"h\":\"Megatron-LM\",\"t\":[\"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\",\"https://arxiv.org/abs/1909.08053https://github.com/NVIDIA/Megatron-LM\"]},\"89\":{\"h\":\"Model Parallel Transformers\",\"t\":[\"在MLP中最常用的操作是MM + GELU，即输入与参数矩阵相乘 和 激活函数\",\"一种方法是沿着其行(row)将weight矩阵A分割，并沿着其列(columns)输入X分割，来实现tensor-model-parallel，从下图中我们可以看出，在该方法下，需要通过同步来保障语义对等。\",\"另一种方法是沿着它的列(columns)分割A，这种方法的好处是保障了各自在独立计算时的语义对等，不需要进行额外通讯\",\"不过由于我们使用的A是同一个，在反向传播时要保证在保存不同切分W的GPU中均及时更新A，在PyTorch中可以通过下面的代码简单实现：\",\"class f(torch.autograd.Function): def forward(ctx, x): return x def backward(ctx, gradient): all_reduce(gradient) return gradient\",\"通过结合上述两种方法，可以在transformer中实现简单并发，针对MLP、SA两个环节做了如下优化，实现MLP的整体的tensor-model-parallel且语义和原始MLP对等：\",\"在优化 embedding 层时，PyTorch 将 embedding 操作视为对输入进行索引操作，即在权重矩阵上执行 index_select 操作。实际上，这个操作等价于先对输入进行 one-hot 编码，然后与权重矩阵进行矩阵乘法（mm）操作。可以将 embedding 层视为线性层来处理，并使用模型并行操作来优化。\",\"class VocabParallelEmbedding(): def __init__(self, num_embeddings, embedding_dim, init_method=init.xavier_normal_): super(VocabParallelEmbedding, self).__init__() ... # 通过获取当前 tensor_model_parallel 的 rank 来确定当前卡要 embedding 的 category id，初始化权重 self.vocab_start_index, self.vocab_end_index = VocabUtility.vocab_range_from_global_vocab_size( self.num_embeddings, get_tensor_model_parallel_rank(), self.tensor_model_parallel_size) self.num_embeddings_per_partition = self.vocab_end_index - self.vocab_start_index self.weight = Parameter(torch.empty( self.num_embeddings_per_partition, self.embedding_dim, dtype=args.params_dtype)) ... def forward(self, input_): if self.tensor_model_parallel_size > 1: # 把当前卡上不要的 category idx mask 掉 input_mask = (input_ < self.vocab_start_index) | \\\\ (input_ >= self.vocab_end_index) # Mask 掉的输入 masked_input = input_.clone() - self.vocab_start_index masked_input[input_mask] = 0 else: masked_input = input_ # 获取嵌入向量 output_parallel = F.embedding(masked_input, self.weight, self.padding_idx, self.max_norm, self.norm_type, self.scale_grad_by_freq, self.sparse) # 把 mask 掉的 category idx 对应的嵌入向量处理成 0 if self.tensor_model_parallel_size > 1: output_parallel[input_mask, :] = 0.0 # 在所有模型并行 GPU 上进行 reduce 操作 output = reduce_from_tensor_model_parallel_region(output_parallel) return output\"]},\"90\":{\"h\":\"3D并行\",\"t\":[\"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM\",\"https://arxiv.org/pdf/2104.04473https://github.com/NVIDIA/Megatron-LM\"]},\"91\":{\"h\":\"贡献\",\"t\":[\"如何合理的使用上面的多种并行技术一直是一个困难的命题，本文中研究人员提出了一种名为PTD-P的策略，在一个大规模的GPU集群上达到超过50%的效能。\",\"本文探讨了以下几种因素对效率的影响：\",\"不同的并行策略：通常张量并行只适用于一个multi-GPU服务器内部，而流水线并行则几乎只用于更大的模型（多机集群）\",\"流水线并行中的schedule：对通信、pipeline bubble的大小、内存都有重要的影响\",\"参数的选取（如microbatch的大小）：对于内存使用和kernel计算效率、pipeline bubble大小也有影响\"]},\"92\":{\"h\":\"数据并行\",\"t\":[\"问题：\",\"batch过小，会导致单块GPU利用率降低，而通信成本大大增长\",\"理论上GPU数量等于batch size，这也限制了可以使用的卡的规模\"]},\"93\":{\"h\":\"考虑流水线并行\",\"t\":[\"流水线并行技术将一个模型（Transformer）中的不同层发放到多块GPU上，将原本一个batch的数据分解为多个更小的micro batch，通过编排forward pass和backward pass可以来获取不同的性能。 为了确保optimizer semantics（也就是端到端的计算能够正确的完成），不同GPU之间需要进行定期同步---在每一个batch计算完毕时执行pipeline flush操作（完成一次周期内所有micro batch的操作，此时不再提供任何其他新的输入），我们称这个过程中的等待时间为pipeline bubble。 在流水线并行技术中，micro batch的数量/pipeline尺寸（并行使用的GPU的数量）越大，通常pipeline flush消耗的时间则越小。\"]},\"94\":{\"h\":\"考虑张量并行\",\"t\":[\"见前文TP内容。\"]},\"95\":{\"h\":\"性能分析\",\"t\":[\"令：\",\"(p,t,d)分别表示PP/TP/DP并行维度\",\"n:GPU总数，有$ n=ptd $\",\"B:Global Batch Size\",\"Micro Batch Size\",\"m=B/(b*d)：每Pipeline中的microbatch的数量\",\"可以得到以下结论：\",\"对于多个GPU集群，通常建议在集群上采用流水线并行，每一个集群上的节点假如包含g个GPU卡，通常在一个节点内部采用不超过g张卡做张量并行\",\"考虑使用模型并行（包括流水线并行和张量并行）和数据并行时，通常建议在GPU内存满足的情况下最大化模型并行（t和p），然后通过数据并行技术扩展集群上进行加速\",\"microbatch的选择也非常重要，取决于最终的throughput（吞吐量）和memory，需要综合流水线并行p、数据并行d、全局BatchSize B考虑\",\"Activation Recompute也是一种可以考虑的方法，在大模型训练中内存往往比算力更加宝贵，所以一种可取的方法是将一部中间过程结果丢弃，在需要时（如进行backward更新）重新计算，以节省内存。\"]},\"96\":{\"h\":\"自动并行\"},\"97\":{\"h\":\"FlexFlow\",\"t\":[\"Beyond Data and Model Parallelism for Deep Neural Networks\",\"https://arxiv.org/pdf/1807.05358\",\"本文引入了 SOAP（一个更加全面的并行策略搜索空间） 和 FlexFlow（一个高效的并行策略搜索引擎）, 为了加速并行策略的搜索， FlexFlow 引入了一个 novel execution simulator, 可以准确预测并行策略的性能。\"]},\"98\":{\"h\":\"简介\",\"t\":[\"论文提出从 SOAP 4个维度描述搜索空间:\",\"Operator: 描述不同的 Operator 的并行方法 -- OP并行(如何将一个 大OP 拆分到 多个 devices 上)\",\"Sample: 对于单个 Operator, Sample 描述训练数据如何并行 -- 数据并行\",\"Parameter: 对于单个 Operator, Parameter 描述参数如何并行 -- 模型并行\",\"Attribute: 对于单个 Sample, Attribute 描述单个 Sample 不同属性如何并行 -- Sample并行(如何将一个 Sample 拆分到多个 devices 上)\",\"关键的问题是: 如何快速搜索 SOAP 空间\",\"快速，增量的执行模拟器 -- execution simulator\",\"Markov Chain Monte Carlo (马尔可夫链蒙特卡洛) 搜索算法 -- execution optimizer\"]},\"99\":{\"h\":\"概述\",\"t\":[\"同时输入 Compute Graph 和 Device Topology，FlexFlow负责将 Operator 分发到不同的设备上，确定 Operator 的执行顺序。\",\"SOAP 搜索空间对于不同的 Operator 【卷积、池化、全连接层、激活函数、归一化、嵌入、矩阵乘法、损失函数】，可拆分的维度并不一样，但大多数 Operator(除去BatchNormalization)，都支持 Sample 维度的拆分, 大多数 Operator 都能支持2种或者更多的拆分方式。 假设模型包含 N 个 Operator，那并行配置 至少有 2^N 种。当然，实际情况种，通常还会加一些约束条件, 比如 co-location（操作符的共置）, 但即使这样，要在合理时间内找到最优解依然比较困难。\"]},\"100\":{\"h\":\"执行模拟器\",\"t\":[\"接受 operator graph, device topology 和 parallelization strategy 作为输入，预测模型的执行时间。\"]},\"101\":{\"h\":\"执行优化器\",\"t\":[\"执行优化器，接受 operator graph 和 device topology 作为输入，自动查找高效的 parallelization strategy. 论文基于当前的 parallelization strategy 随机选择 operator，并随机替换被选择的 operator 的 parallelization strategy, 进而生成 proposals。同时，MCMC算法内部维护一个 最优 parallelization strategy, 依据如下规则进行更新:\"]},\"102\":{\"c\":[\"SOSD\"]},\"103\":{\"c\":[\"分布式系统\",\"MLSys\",\"并行运算\"]},\"104\":{\"h\":\"CS149 Lab Assignment1\"},\"105\":{\"h\":\"Prog1_mandelbort_threads\"},\"106\":{\"h\":\"环境配置\",\"t\":[\"本人使用OS为Ubuntu 22.04, 还是建议使用Linux系统做Lab, 很多环境配置会方便一些.\",\"CS149_Asst1并不需要额外配置运行环境, 下载解压一下编译环境就好啦! 下载包:\",\" wget https://github.com/ispc/ispc/releases/download/v1.21.0/ispc-v1.21.0-linux.tar.gz\",\"解压包:\",\" tar -xvf ispc-v1.21.0-linux.tar.gz\",\"配置环境路径:\",\" export PATH=$PATH:${HOME}/Downloads/ispc-v1.21.0-linux/bin\",\"环境配置完成后就可以clone repo到本地来开始lab了:\",\" git clone https://github.com/stanford-cs149/asst1.git\"]},\"107\":{\"h\":\"任务分析\",\"t\":[\"Pro1的内容主要是为了让学生了解std::thread的并行机制和\\\"多线程不一定高效率\\\"的并发事实, 所以难度并不算大~~(这是我的事后诸葛亮)~~, 整体框架已经在源码中基本完成了.完成后可以通过make + ./mandelbort --<args>检验正确与否.\",\"task :\",\"创建线程0和线程1, 分别计算图像的上下两个部分, 即将图像的不同空间交给不同线程计算, 这被称为空间分解(spatial decomposition).\",\"扩展代码使其能够使用2, 3, 4, 5, 6, 7, 8个线程, 进行空间分解, 生成加速图, 假设加速是否与线程数线性相关并加以验证.\",\"在workerThreadStart()的开头和结尾插入计时代码, 验证并解释task2中提出的猜想.\",\"修改一开始的线程分配方式, 实现将两个图片都拉到8线程时7-8倍加速比的效果, 找到适应任何线程数的泛型分配方式(不需要线程之间进行响应和同步), 报告最后得出的8线程加速比.\",\"使用16个线程运行改进后代码, 回答性能是否明显高于8线程并解释原因.\",\"事实上task中给的提示还是比较明显的, 在task1中解释了空间分解的概念, 那么通过对图片本身的上下多份分割,就可以解决这个问题,要注意分割的时候会不会漏行.\"]},\"108\":{\"h\":\"任务实现\",\"t\":[\"我们将一开始就对任务给出多线程的解决方式, 并在后续针对数据结果决定是否要进行优化.\",\"首先我们可以根据阅读mandelbrotSerial.cpp中的源码, 得到mandelbrotSerial()函数事实上是用来计算Mandelbrot图像的, 可以简单分析一下mandelbrotSerial()函数的各个参数:\",\" void mandelbrotSerial( float x0, float y0, float x1, float y1, // 复平面左上和右下两个点坐标 int width, int height, // 图像宽度和高度 int startRow, int numRows, // 开始行和总计算行数 int maxIterations, // 最大迭代次数 int output[]); // 每个点的迭代次数\",\"不难发现只要我们给出startRow, numRows, 其余保持图像默认参数, 就可以完成计算了. 所以可以给出函数workerThreadStart(WorkerArgs * const args)的代码:\",\" size_t rows = args -> height / args -> numThreads; // 确定要计算的行数 if (args -> height % args -> numThreads) { // 如果该遇到整除要加一行避免遗漏 rows++; } size_t startRow = args -> threadId * rows; // 确定开始行 // 如果已经到最后部分不够切分, 直接处理最后部分 rows = rows > args -> height - startRow ? args -> height - startRow : rows; // 调用mandelbrotSerial mandelbrotSerial(args -> x0, args -> y0, args -> x1, args -> y1, args -> width, args -> height, startRow, rows, args -> maxIterations, args -> output);\"]},\"109\":{\"c\":[\"CS149_Lab\"]},\"110\":{\"c\":[\"公开课\",\"并行计算\",\"Lab\"]},\"111\":{\"h\":\"CS61A\"},\"112\":{\"h\":\"week1\"},\"113\":{\"h\":\"Lecture\"},\"114\":{\"h\":\"Welcome\",\"t\":[\"The first lecture just a simple explaination of CS61A and explnation of the Expression Tree which is composed of a operator and several operands, so I just paste the picture of the Tree.\",\"Easy but interesting. If you learned data structure, you may think this is n-ary tree.\"]},\"115\":{\"h\":\"Functions\"},\"116\":{\"h\":\"name\"},\"117\":{\"h\":\"built-in\",\"t\":[\"Like c/c++, python also has its own way to name or rename a parameter, a function or a variable and so on. The lecture will introduce it at beginning.\",\"Ctrl + L is used to flash the terminal in python session.\",\">>> pi Traceback (most recent call last): File \\\"<stdin>\\\", line 1, in <module> NameError: name 'pi' is not defined\",\"oops! No pi! This means the parameter is not a built-in parameter. We have to import it.\",\">>> from math import pi >>> pi 3.141592653589793\",\"The same goes for imported functions.\",\">>> from math import sin >>> sin(pi) 1.2246467991473532e-16 >>> sin(pi/2) 1.0\",\"You may find it not zero after sin(pi), because of the type of data storage in the computer or just a data cut. Not important.\"]},\"118\":{\"h\":\"name by ourselves\",\"t\":[\"We can also name our own parameter:\",\">>> radius = 10 >>> radius 10\",\"Even multiple variables named simultaneously is also supported:\",\">>> area, circ = pi * radius *radius, 2 * pi * radius >>> area 314.1592653589793 >>> circ 62.83185307179586\",\"The same goes for imported functions.\"]},\"119\":{\"h\":\"define\",\"t\":[\"We can edit our functions by def:\",\">>> def square(x): ... return mul(x, x) ... >>> square(11) 121\",\"If we change the value of parameter, the value of expression named by us will not change, but if define a functions will solve the issue:\",\">>> def area(): ... return pi * radius * radius ... >>> area() 314.1592653589793 >>> radius = 20 >>> area() 1256.6370614359173\",\"A function differs from a name in that its return expression here gets re-evaluated every time it's called.\"]},\"120\":{\"h\":\"defining functions\",\"t\":[\"structure of defining:\",\">>> def <name>(<formal parameters>) return <return expression>\",\"Fuctions will be evaluated when it is called.\",\"The lecture also introduce the difference between local frames and global frames. I passed it for my c&c++ basics.\"]},\"121\":{\"h\":\"Environment Diagrams\",\"t\":[\"Online Python Tutor:\",\"https://pythontutor.com/visualize.html#\"]},\"122\":{\"h\":\"Print and None\",\"t\":[\"Just talk about the difference of print and evaluate and Nonetype.\",\">>> None >>> print(None) None\"]},\"123\":{\"h\":\"Lab\"},\"124\":{\"h\":\"Lab0:Getting Started\",\"t\":[\"Download starter files from: https://cs61a.org/lab/lab00/lab00.zip\"]},\"125\":{\"h\":\"Introduction\",\"t\":[\"This lab explains how to setup your computer to complete assignments and introduces some of the basics of Python.\",\"Components cheek passed, so skip them.\"]},\"126\":{\"h\":\"Setup\",\"t\":[\"skip.\"]},\"127\":{\"h\":\"First Assignment\"},\"128\":{\"h\":\"What Would Python Do?\",\"t\":[\"Enter the following in your terminal to begin this section:\",\"# we don't have edu email of berkeley, so we need to add --local python ok -u --local\",\"Esay to pass:\",\"===================================================================== Assignment: Lab 0 OK, version v1.18.1 ===================================================================== ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Unlocking tests At each \\\"? \\\", type what you would expect the output to be. Type exit() to quit --------------------------------------------------------------------- Python Basics > Suite 1 > Case 1 (cases remaining: 1) What would Python display? If you get stuck, try it out in the Python interpreter! >>> x = 20 >>> x + 2 ? 22 -- OK! -- >>> x ? 20 -- OK! -- >>> y = 5 >>> y = y + 3 >>> y * 2 ? 16 -- OK! -- >>> y + x ? 28 -- OK! -- ---------------------------------------------------------------------\"]},\"129\":{\"h\":\"Implementing Functions\",\"t\":[\"lazy to think:\",\"def twenty_twenty_four(): \\\"\\\"\\\"Come up with the most creative expression that evaluates to 2024 using only numbers and the +, *, and - operators. >>> twenty_twenty_four() 2024 \\\"\\\"\\\" return 2024 * 1 + 1 - 1\"]},\"130\":{\"h\":\"Running Tests\",\"t\":[\"use ok to check:\",\"python ok --local\",\"check passed:\",\"===================================================================== Assignment: Lab 0 OK, version v1.18.1 ===================================================================== ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Running tests --------------------------------------------------------------------- Test summary 2 test cases passed! No cases failed.\"]},\"131\":{\"h\":\"Appendix: Useful Python Command Line Options\",\"t\":[\"-i: opens an interactive session\",\"python3 -i lab00.py >>> 1 1\",\"-m doctest: run the examples in the docstrings of functions\",\"# doctest >>> twenty_twenty_four() 2024\",\"If all doctest passed, no outputs.\",\"python3 -m doctest lab00.py\"]},\"132\":{\"c\":[\"CS61A\"]},\"133\":{\"c\":[\"MOOC\",\"CS\",\"berkeley\"]},\"134\":{\"h\":\"\",\"t\":[\"MZ��\\u0003���\\u0004���������������@�����������������������������������(\\u0001��\\u000e\\u001f�\\u000e�� �!�\\u0001L�!This program cannot be run in DOS mode.\",\"$��������������������������������'���\\u000f��������������������������,��Z����Z���Ӌ�Z�����Z����Z� ���Z����Rich��������������������������PE��d�\\u0007��1�e����������\\\" \\u0002\\u000e\\u001d�. ��Z\\b�����0x\\b��\\u0010������\\u0001����\\u0010���\\u0002��\\u0006�������\\u0006���������\\u0014��\\u0004��Μ\\u0014�\\u0003�\\u0001��\\u0010������\\u0010��������\\u0010������\\u0010����������\\u0010����\\u000f�ܩ\\u0003�<.\\u0013�P�����\\u0014��\\u0001����\\u0013�Ƞ���h\\u0014�(����\\u0014�d\\u0016���� �T��������������������� �(���\\u0010� �8\\u0001�����������@ �\\u0005��������������������������.text����, ��\\u0010���. ��\\u0004�������������� ��`.rdata����\\u0006��@ ���\\u0007��2 �������������@��@.data���|����@\\u0013��x���2\\u0013�������������@�\"]},\"135\":{\"h\":\"CSE234\"},\"136\":{\"h\":\"PA1: Automatic differentiation\"},\"137\":{\"h\":\"Question 1: Auto Diff Library\"},\"138\":{\"h\":\"Part 1: Operators\",\"t\":[\"The list of operators that you will need to implement are:\",\"DivOp\",\"DivByConstOp\",\"TransposeOp\",\"ReLUOp\",\"SqrtOp\",\"PowerOp\",\"MeanOp\",\"MatMulOp\",\"SoftmaxOp\",\"LayerNormOp\"]},\"139\":{\"h\":\"DivOp\",\"t\":[\"class DivOp(Op): \\\"\\\"\\\"Op to element-wise divide two nodes.\\\"\\\"\\\" def __call__(self, node_A: Node, node_B: Node) -> Node: return Node( inputs=[node_A, node_B], op=self, name=f\\\"({node_A.name}/{node_B.name})\\\", ) def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: \\\"\\\"\\\"Return the element-wise division of input values.\\\"\\\"\\\" assert len(input_values) == 2 \\\"\\\"\\\"TODO: your code here\\\"\\\"\\\" def gradient(self, node: Node, output_grad: Node) -> List[Node]: \\\"\\\"\\\"Given gradient of division node, return partial adjoint to each input.\\\"\\\"\\\" \\\"\\\"\\\"TODO: your code here\\\"\\\"\\\"\"]},\"140\":{\"c\":[\"CSE234\",\"Lab\"]},\"141\":{\"c\":[\"公开课\",\"MLSys\",\"course\",\"Lab\"]},\"142\":{\"h\":\"MLSys_分布式开发（选读）\"},\"143\":{\"h\":\"序列并行\"},\"144\":{\"h\":\"Megatron\",\"t\":[\"Reducing Activation Recomputation in Large Transformer Models\",\"https://arxiv.org/pdf/2205.05198\"]},\"145\":{\"h\":\"Abstract\",\"t\":[\"在大模型训练过程中显存占用过大往往成为瓶颈，一般会通过重计算的方式降低显存占用，但会带来额外的计算代价。本文提出sequece parallel(序列并行,简称SP)和selective activation recomputation两种方法，可以结合TP有效减少不必要的计算量。\",\"下图中绿色部分表示不同参数级别模型中需要用于保存activation需要的显存大小，蓝色部分表示不同参数级别模型中需要用于保存parameter和optimizer state需要的显存大小。红色线表示baseline(A100的显存)80G。\",\"通过对比可以发现,原本单A100跑不了的模型,经过SP优化后可以在单A100上运行了,这就给我们加大数据量和多机并行提供了极大的便利\"]},\"146\":{\"h\":\"Activation Memory\",\"t\":[\"本文以Transformer结构为例估算Activation Memory，Activation指FWD和BWD梯度计算中创建的所有tensor。不包含模型参数大小和优化器中状态大小，但是包含dropout用到的mask tensor。\",\"本文推导与假设中用到了以下几个参量:\",\"本文假设h极大(实际上一般也确实极大), 认为2sb远小于sbh, 即只考虑中间过程的Memory(shb), 忽略输入输出的Memory\",\"对于Attention模块,这一部分依赖于softmax实现:\",\"Attention(Q,K,V)=softmax(dk​​QKT​)V\",\"具体实现图例见下:\",\"对于Attention块来说，输入的元素个数为sbh个，每个元素以半精度(2 bytes)来进行存储的话，对应输入的元素大小为2sbh bytes\",\"Attention块中包含一个self-attention、一个linear(线性映射层)和attention dropout层。对于linear需要保存输入的Activation大小为2sbh, 对于attention dropout层需要mask的大小为sbh(对于一个元素的mask只用1个bytes)，对于self-attention块的Activation Memory的计算有以下几块：\",\"Query(Q),Key(K),Value(V) matrix mul：input共享，元素个数为sbh个，总大小是 2sbh bytes。\",\"QKT矩阵相乘：需要分别创建保存Q和K的矩阵，每个矩阵元素总大小为2sbh bytes, 总共大小为4sbh bytes\",\"原始Self-Attention例子(此处X切分仅作示意,实际上是按行切分的):\",\"dropout的mask层矩阵的大小与softmax的输出一样，元素个数都是as2b个，但mask单个元素的大小只用1 bytes即可，总的大小为 as2b bytes\",\"softmax的输出也会用于反向的计算，需要缓存下来，对应大小 as2b bytes\",\"V矩阵的大小之前没有统计，和Q、K矩阵一样，大小也是2sbh bytes\",\"Attention 模块总的大小为 11sbh + 5as2b bytes。\",\"MLP的Activation大小计算：MLP中有两层线性layer，分别存储输入矩阵大小为2sbh bytes和8sbh bytes；GeLU的反向也需要对输入进行缓存，大小为8sbh bytes; dropout层需要sbh bytes; 总大小为19sbh。\",\"LayerNorm的Activation大小计算：每个LayerNorm层的输入需要2sbh大小，有两个LayerNorm层，总大小为4sbh bytes.\",\"一层transformer的memory总的大小为:\",\"ActivationMemoryPerLayer=sbh(34+5has​)\"]},\"147\":{\"h\":\"Tensor Parallel\",\"t\":[\"在TP并行中只在Attention和MLP两个地方进行了并行计算，对于两块的输入并没有并行操作。\",\"图中f和f​互为共轭(conjugate)，f在前向时不做操作，反向时执行all-reduce;f​在前向时执行all-reduce, 反向时不做操作。\",\"考虑TP并行(并行度为t)，并行部分有MLP的Linear部分(18sbh bytes)和Attention的QKV部分(6sbh bytes)， ActivationMemoryPerLayer的值降为：\",\"ActivationMemoryPerLayer=sbh(10+t24​+5htas​)\"]},\"148\":{\"h\":\"Sequence Parallel\",\"t\":[\"在Tensor模型并行基础上提出了Sequence Parallel，对于非TP并行的部分在sequence维度都是相互独立的，所以可以在sequence维度上进行拆分(即sequence parallel)。\",\"拆分后如下图，f和f​替换为g和g​​在前向是reduce-scatter, 反向是all-gather通信。\",\"以MLP为例，详细说明拆分步骤:\",\"MLP层由两个Linear层组成，对应的计算公式如下, 其中X的大小为s×b×h;A和B是Linear的权重weight矩阵，大小为h×4h和4h×h\",\"论文做如下符号说明:\",\"YZWV​=LayerNorm(X),=GeLU(YA),=ZB,=Dropout(W),​\",\"对X按sequence维度切分， X=[X1s​,X2s​]，LayerNorm的结果Y=[Y1s​,Y2s​]；\",\"考虑GeLU非线性，进行all-gather，计算Z=GeLU(YA)；\",\"对A进行列切分的tensor并行，得到结果YA1c​​和YA2c​\",\"对B进行行切分的tensor并行，得到结果Z1h​B1r​和Z2h​B2r​\",\"得到W1​和W2​后进行reduce-scatter\",\"具体过程见下:\",\"[Y1s​,Y2s​]Y[Z1h​,Z2h​]​W1​[W1s​,W2s​]​[V1s​,V2s​]​​​=LayerNorm([X1s​,X2s​]),​=g(Y1s​,Y2s​),=[GeLU(YA1c​), GeLU(YA2c​)],=Z1h​B1r​ and W2​=Z2h​B2r​,=gˉ​(W1​,W2​),=[Dropout(W1s​), Dropout(W2s​)].​\",\"TP在一次前向和后向总共有4次的all-reduce操作，在SP一次前向和后向总共有4次all-gather和4次reduce-scatter操作。ring all-reduce 执行过程中有两步，先是一个reduce-scatter然后一个all-gather，SP没有引入更多的通信代价。\",\"ActivationMemoryPerLayer​=sbh(t10​+t24​+5htas​)=tsbh​(34+5has​)​\"]},\"149\":{\"h\":\"DeepSpeed-Ulysses\",\"t\":[\"DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Modelshttps://arxiv.org/pdf/2309.14509\",\"DeepSpeed在知乎也有官号, 这里仅作简述, 官号本身讲的也非常不错, 链接在这儿:https://zhuanlan.zhihu.com/p/652206513\"]},\"150\":{\"h\":\"简介\",\"t\":[\"长序列在LLM应用中非常重要, 长上下文的保存有助于LLM推理, 需求大token和长Sequence的输入\",\"现有的DP TP PP不能解决序列维度的扩展问题\",\"现有的序列并行方法依托内存通讯, 不够高效\",\"DeepSpeed-Ulysses将各个样本在序列维度上分割给参与的GPU。在attention计算之前，它对已分割的查询(Q)、键(K)和值(V)执行all-to-all通信操作，以使每个GPU接收完整的序列，但仅用于注意力头的非重叠子集。 这使得参与的GPU可以并行计算不同的注意力头。最后，DeepSpeed-Ulysses还使用另一个all-to-all来在注意力头上收集结果，同时重新在序列维度上进行分区。\"]},\"151\":{\"h\":\"DeepSpeed-Ulysses的核心设计\",\"t\":[\"与已知的Transformer架构一样，设计由N个输入序列在P个可用设备上分区组成。每个本地N/P分区都被投影到查询（Q）、键（K）和值（V）嵌入中。接下来，(QKV) 嵌入通过参与计算设备之间的高度优化的全对全集合（all-to-all collectives）进行全局的 QKV 收集。在全对全集合后，每个头的注意力计算形式为：\",\"Outputcontext=Softmax(d​QKT​)V\",\"注意力计算后，另一个全对全集合将注意力计算的输出上下文张量转换为序列(N/P)并行，用于Transformer模型层的剩余模块中的后续操作。\"]},\"152\":{\"h\":\"Sequence Parallelism\",\"t\":[\"Sequence Parallelism: Long Sequence Training from System Perspectivehttps://arxiv.org/pdf/2105.13120\"]},\"153\":{\"h\":\"论文背景\",\"t\":[\"和Ulysses一样, 这个SP也是目的也是解决输入序列规模问题, 而不是像Megtron-LM一样解决计算过程中的内存占用问题.\",\"文章将自己的SP流程和PP TP模型做了比较:\"]},\"154\":{\"h\":\"主要工作\",\"t\":[\"本文认为SP最主要的问题是跨设备如何计算sub-sequences的attention scores问题，为了解决该问题，本文设计出Ring Self-Attention来解决此问题.\",\"感觉想法来源于Ring-AllReduce, 而且只优化了自注意力部分\",\"RSA感觉就是将输入序列进行切分, 通过将序列整体切分成小的chunk, 使得每一个chunk都尽可能大, 用commnication换取序列长度\"]},\"155\":{\"h\":\"Ring-Attention\",\"t\":[\"Ring Attention with Blockwise Transformers for Near-Infinite Contexthttps://arxiv.org/pdf/2310.01889\"]},\"156\":{\"h\":\"背景\",\"t\":[\"老生常谈的transformer长序列要求，不做赘述。\"]},\"157\":{\"h\":\"Ring Attention\",\"t\":[\"本文提出以分块方式执行Self-Attention和FWD计算，多机分布序列维度，从而实现并发计算和通信. 由于该方法将环中主机设备之间的Key Value Block通信与compute重叠，因此将其命名：环注意(Ring Attention)\",\"具体实现：\",\"该方法在主机设备之间构建Self-Attention的外循环，每个主机设备具有一个query block，并通过Key Value Block遍历主机Ring，以逐块的方式进行注意力和前馈网络计算。 当计算Self-Attention时，每个主机将Key Value Block发送到下一个主机，同时从前一个主机接收Key Value Block。\",\"对于内循环，每个设备计算其各自的Self-Attention和FWD。在内循环期间，每个设备将用于compute的Key Value Block的副本发送到Ring中的下一个设备，同时从前一个设备接收Key Value Block。 由于块计算比块传输需要更长的时间，与标准Transformer相比，此过程不会增加开销。\",\"这种Ring Attention方式可以突破序列长度限制（对于单卡内存需求来说，而非模型整体来说），因为我们在Attention之前矩阵乘就已经切分了Squence，让每一个卡分批成环去跑一小批token\",\"这种方式理论上并不影响训练结果，因为最小训练单位还是一个token（对Squence做切分时的原则）\",\"天才般的想法！（我觉得）好吧也需要堆卡出奇迹\",\"那么代价呢？ 文章的训练测试规模比较小，能否在大规模训练时取得想象中的线性效果，还是未知数 文章只对Attention和FWD操作做了优化，基础操作还有进一步优化的空间，可以考虑采用4D并行。\"]},\"158\":{\"h\":\"DISTFLASHATTN\",\"t\":[\"https://arxiv.org/pdf/2310.03294\"]},\"159\":{\"h\":\"背景\",\"t\":[\"部分SP方法，如Ring Attention缺少高效的Attention实现。（前文提及的基础操作还有进一步优化的空间）\"]},\"160\":{\"h\":\"论文方法\",\"t\":[\"文章针对三个challenge提出了三个解决方法，解决了高校Attention实现、分布式应用等问题。\"]},\"161\":{\"h\":\"token-level workload imbalance\",\"t\":[\"这主要是由于causal mask引起的attention计算问题，如果简单分块计算约会有1/2的计算资源浪费。 因为一般causal mask就是会用矩阵以对角线为界mask数据, 按照ring拓扑结构进行计算也会有约一半的CPU处于等待状态。\",\"针对这个问题，论文中提出Token-level workload balancing的调度算法，通过给空闲的worker来fetch部分key-value数据，计算后再传递回去。\"]},\"162\":{\"h\":\"Prohibitive communication overhead\",\"t\":[\"需要通信汇总不同worker上的attention结果，本文提出通过计算attn(qp​,kr​,vr​,sp​) prefetch张量来完成通信workerp⟵kr+1​,vr+1​​workerr+1的覆盖，实现依赖于P2P的通信模式。\"]},\"163\":{\"h\":\"Loadbalanced scheduling with communication and computation overlap\",\"t\":[\"huggingface中采用的Recomputation的检查点放置不合理，导致相对高的还原计算代价。 下面是两种重计算策略的对比：\",\"DFA相较HF每一个FlashAttention+FFN约能够减少一个FA的计算量。FlashAttention部分的梯度更新为KQV三个矩阵，可以通过FA的输出结果完成更新，因此保存FA的结果便可计算三者的矩阵。\"]},\"164\":{\"c\":[\"SOSD\"]},\"165\":{\"c\":[\"分布式系统\",\"MLSys\",\"并行运算\"]},\"166\":{\"h\":\"PA -- A new journey for the OS\",\"t\":[\"This blog will begin from the half of PA1 Because I thought \\\"STFW\\\" can solve the problems before. I have to confirm that I have install the Ubuntu 22.04 for my CS task, so I look through the PA0 in a fast way.\"]},\"167\":{\"h\":\"PA1 The simplest computer\"},\"168\":{\"h\":\"infrastructure\",\"t\":[\"There always exist infrastructure where exist codes.\",\"The course build a significant infrainstructure called Simple Debugger(sdb) in the NEMU.\",\"NEMU is regarded as a programm to excute other guest programm which means NEMU can know all information of the guest programm.\",\"However, the information is hard to be caught by the debugger out of the NEMU, such as GDB.(set a breakpoint by GDB for the guest programm is also hard)\",\"Inorder to improve the efficiency of debugging, we need to build a simple Debugger at the monitor.\",\"Below are the format and functions:\",\"instruction\",\"format\",\"example\",\"explanation\",\"help\",\"help\",\"help\",\"print help of instructions\",\"continue\",\"c\",\"c\",\"continue to run the programm is suspended\",\"quit\",\"q\",\"q\",\"quit NEMU\",\"run by step\",\"si [N]\",\"si 10\",\"run N instructions by step then stop, default N is equal to 1\",\"print program status\",\"info SUBCMD\",\"info rinfo w\",\"print register status print monitor message\",\"scale Memory\",\"x N EXPR\",\"x 10 $esp\",\"figure out the value of EXPR and set it as the begining of the Memory. output N*4 bytes as hex format\",\"figure out the expression value\",\"p EXPR\",\"p $eax + 1\",\"figure out EXPR value\",\"set monitor\",\"w EXPR\",\"w *0x2000\",\"when EXPR changes, suspend the program\",\"delete monitor\",\"d N\",\"d 2\",\"delete monitor whose label is N\"]},\"169\":{\"c\":[\"计算机体系结构\"]},\"170\":{\"c\":[\"系统\",\"计算机组成\"]},\"171\":{\"h\":\"What's Automatic Differentiation?\",\"t\":[\"Reference: https://huggingface.co/blog/andmholm/what-is-automatic-differentiation\"]},\"172\":{\"h\":\"写在开始之前\",\"t\":[\"这篇blog是本人结合Reference对Automatic Differentiation(自动微分)的总结, 目的主要是回顾Machine Learning基础并完成CSE234的PA1, 考虑到CSE234课程要求, 涉及到课程具体代码实现的部分将以注释&伪代码的形式实现, 希望读者能自行实现Automatic Differentiation, Base is not only base.\",\"同Reference blog的建议一样, 本文也建议读者有一些微积分 / 线性代数 / 机器学习的基础, 有一些导数 or 梯度等概念将不会重新简述, 本文希望从应用和实现的角度对autodiff library 中的算子做再实现(也是CSE234的课程要求).\"]},\"173\":{\"h\":\"Introduction\",\"t\":[\"在Machine Learning中, 神经网络运用了广泛的数学理论, 逐步从简单处理二分类或者多分类的简单架构逐步发展到现在拥有对话能力的LLM, 核心问题一直是优化问题, 即如何让模型进行学习?\",\"现在使用的最多的, 也是被广泛认为最好的数学方法, 是gradient descent(梯度下降)方法及其变体. 梯度下降法是一种优化算法, 目的是通过一步步迭代来提升模型的性能, 下面是对这个算法的详细解释:\"]},\"174\":{\"h\":\"梯度下降\",\"t\":[\"使用一个目标函数(Loss), 计算输入的真实值与对这组输入的预测值之间的损失(or error).\",\"通过求损失关于模型每个参数的偏导(求梯度)的方式, 来找出模型对损失的影响.\",\"通过将每个参数减去各自的梯度(梯度缩放由学习率超参进行调整, 当然我们也可以通过编写调度器实现学习率的动态调整), 朝着loss最小化的方向调整模型参数.\",\"清除所有梯度, 然后重复1-3的训练过程, 直到模型达到最佳性能(不幸的是过度训练可能导致参数在某一个方向上下降过多, 又称过拟合, 所以我们一般需要人为设置训练轮次来恰当的结束训练).\",\"这个过程显然需要一个足够强大的深度神经网络来支撑, 当然如何找到这样一个自动优化过程是复杂且艰巨的, 我们不得不感谢前人在这方面作出的杰出贡献(https://arxiv.org/pdf/1502.05767), it's an honor to follow them.\",\"在开始讨论自动微分之前, 我们先看一下更为朴素的数值微分和符号微分.\"]},\"175\":{\"h\":\"数值微分(Numeric Differentiation)\",\"t\":[\"数值微分是一种极为朴实无华的实现, 经典的极限定义可以帮我们很好的理解这一点:\",\"dxdf​=h→0lim​hf(x+h)−f(x)​\",\"对于上式求导的含义我们不再赘述, 但是我们必须要注意到神经网络实际上要对多维的array做算术运算, 简单对某个x取极限并没有意义, 在ML领域, 对某个参数求数值偏导的定义, 可以认为是这样的:\",\"∂θi​∂f​≈hf(θ+h⋅ei​)−f(θ)​+O(h)\",\"上式是 forward difference, 用于实现多变量函数f中参数向量θ中单个参数θi​的偏导数. ei​表示一个单位向量，第i个元素为 1，其他元素均为 0.\"]},\"176\":{\"h\":\"Autodiff Library\",\"t\":[\"这是CSE234(2025 winter) PA1一些Autodiff的算子实现, 尊重课程要求, 仅对原课程代码已经实现的算子分享全部代码, 其余算子均为个人实现, 仅提供注释供参考~\"]},\"177\":{\"h\":\"课程已实现的算子\",\"t\":[\"class Op: \\\"\\\"\\\"The class of operations performed on nodes.\\\"\\\"\\\" def __call__(self, *kwargs) -> Node: \\\"\\\"\\\"Create a new node with this current op. Returns ------- The created new node. \\\"\\\"\\\" raise NotImplementedError def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: \\\"\\\"\\\"Compute the output value of the given node with its input node values given. Parameters ---------- node: Node The node whose value is to be computed input_values: List[torch.Tensor] The input values of the given node. Returns ------- output: torch.Tensor The computed output value of the node. \\\"\\\"\\\" raise NotImplementedError def gradient(self, node: Node, output_grad: Node) -> List[Node]: \\\"\\\"\\\"Given a node and its output gradient node, compute partial adjoints with regards to each input node. Parameters ---------- node: Node The node whose inputs' partial adjoints are to be computed. output_grad: Node The output gradient with regard to given node. Returns ------- input_grads: List[Node] The list of partial gradients with regard to each input of the node. \\\"\\\"\\\" raise NotImplementedError class PlaceholderOp(Op): \\\"\\\"\\\"The placeholder op to denote computational graph input nodes.\\\"\\\"\\\" def __call__(self, name: str) -> Node: return Node(inputs=[], op=self, name=name) def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: raise RuntimeError( \\\"Placeholder nodes have no inputs, and there values cannot be computed.\\\" ) def gradient(self, node: Node, output_grad: Node) -> List[Node]: raise RuntimeError(\\\"Placeholder nodes have no inputs.\\\") class AddOp(Op): \\\"\\\"\\\"Op to element-wise add two nodes.\\\"\\\"\\\" def __call__(self, node_A: Node, node_B: Node) -> Node: return Node( inputs=[node_A, node_B], op=self, name=f\\\"({node_A.name}+{node_B.name})\\\", ) def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: \\\"\\\"\\\"Return the element-wise addition of input values.\\\"\\\"\\\" assert len(input_values) == 2 return input_values[0] + input_values[1] def gradient(self, node: Node, output_grad: Node) -> List[Node]: \\\"\\\"\\\"Given gradient of add node, return partial adjoint to each input.\\\"\\\"\\\" return [output_grad, output_grad] class AddByConstOp(Op): \\\"\\\"\\\"Op to element-wise add a node by a constant.\\\"\\\"\\\" def __call__(self, node_A: Node, const_val: float) -> Node: return Node( inputs=[node_A], op=self, attrs={\\\"constant\\\": const_val}, name=f\\\"({node_A.name}+{const_val})\\\", ) def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: \\\"\\\"\\\"Return the element-wise addition of the input value and the constant.\\\"\\\"\\\" assert len(input_values) == 1 return input_values[0] + node.constant def gradient(self, node: Node, output_grad: Node) -> List[Node]: \\\"\\\"\\\"Given gradient of add node, return partial adjoint to the input.\\\"\\\"\\\" return [output_grad] class MulOp(Op): \\\"\\\"\\\"Op to element-wise multiply two nodes.\\\"\\\"\\\" def __call__(self, node_A: Node, node_B: Node) -> Node: return Node( inputs=[node_A, node_B], op=self, name=f\\\"({node_A.name}*{node_B.name})\\\", ) def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: \\\"\\\"\\\"Return the element-wise multiplication of input values.\\\"\\\"\\\" assert len(input_values) == 2 return input_values[0] * input_values[1] def gradient(self, node: Node, output_grad: Node) -> List[Node]: \\\"\\\"\\\"Given gradient of multiplication node, return partial adjoint to each input.\\\"\\\"\\\" return [output_grad * node.inputs[1], output_grad * node.inputs[0]] class MulByConstOp(Op): \\\"\\\"\\\"Op to element-wise multiply a node by a constant.\\\"\\\"\\\" def __call__(self, node_A: Node, const_val: float) -> Node: return Node( inputs=[node_A], op=self, attrs={\\\"constant\\\": const_val}, name=f\\\"({node_A.name}*{const_val})\\\", ) def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: \\\"\\\"\\\"Return the element-wise multiplication of the input value and the constant.\\\"\\\"\\\" assert len(input_values) == 1 return input_values[0] * node.constant def gradient(self, node: Node, output_grad: Node) -> List[Node]: \\\"\\\"\\\"Given gradient of multiplication node, return partial adjoint to the input.\\\"\\\"\\\" return [output_grad * node.constant] class GreaterThanOp(Op): \\\"\\\"\\\"Op to compare if node_A > node_B element-wise.\\\"\\\"\\\" def __call__(self, node_A: Node, node_B: Node) -> Node: return Node( inputs=[node_A, node_B], op=self, name=f\\\"({node_A.name}>{node_B.name})\\\", ) def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: \\\"\\\"\\\"Return element-wise comparison result as float tensor.\\\"\\\"\\\" assert len(input_values) == 2 return (input_values[0] > input_values[1]).float() def gradient(self, node: Node, output_grad: Node) -> List[Node]: \\\"\\\"\\\"Comparison operations have gradient of 0.\\\"\\\"\\\" return [zeros_like(node.inputs[0]), zeros_like(node.inputs[1])] class SubOp(Op): \\\"\\\"\\\"Op to element-wise subtract two nodes.\\\"\\\"\\\" def __call__(self, node_A: Node, node_B: Node) -> Node: return Node( inputs=[node_A, node_B], op=self, name=f\\\"({node_A.name}-{node_B.name})\\\", ) def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: \\\"\\\"\\\"Return the element-wise subtraction of input values.\\\"\\\"\\\" assert len(input_values) == 2 return input_values[0] - input_values[1] def gradient(self, node: Node, output_grad: Node) -> List[Node]: \\\"\\\"\\\"Given gradient of subtraction node, return partial adjoint to each input.\\\"\\\"\\\" return [output_grad, mul_by_const(output_grad, -1)] class ZerosLikeOp(Op): \\\"\\\"\\\"Zeros-like op that returns an all-zero array with the same shape as the input.\\\"\\\"\\\" def __call__(self, node_A: Node) -> Node: return Node(inputs=[node_A], op=self, name=f\\\"ZerosLike({node_A.name})\\\") def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: \\\"\\\"\\\"Return an all-zero tensor with the same shape as input.\\\"\\\"\\\" assert len(input_values) == 1 return torch.zeros_like(input_values[0]) def gradient(self, node: Node, output_grad: Node) -> List[Node]: return [zeros_like(node.inputs[0])] class OnesLikeOp(Op): \\\"\\\"\\\"Ones-like op that returns an all-one array with the same shape as the input.\\\"\\\"\\\" def __call__(self, node_A: Node) -> Node: return Node(inputs=[node_A], op=self, name=f\\\"OnesLike({node_A.name})\\\") def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: \\\"\\\"\\\"Return an all-one tensor with the same shape as input.\\\"\\\"\\\" assert len(input_values) == 1 return torch.ones_like(input_values[0]) def gradient(self, node: Node, output_grad: Node) -> List[Node]: return [zeros_like(node.inputs[0])] class SumOp(Op): \\\"\\\"\\\" Op to compute sum along specified dimensions. Note: This is a reference implementation for SumOp. If it does not work in your case, you can modify it. \\\"\\\"\\\" def __call__(self, node_A: Node, dim: tuple, keepdim: bool = False) -> Node: return Node( inputs=[node_A], op=self, attrs={\\\"dim\\\": dim, \\\"keepdim\\\": keepdim}, name=f\\\"Sum({node_A.name})\\\", ) def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: assert len(input_values) == 1 return input_values[0].sum(dim=node.dim, keepdim=node.keepdim) def gradient(self, node: Node, output_grad: Node) -> List[Node]: dim = node.attrs['dim'] keepdim = node.attrs[\\\"keepdim\\\"] if keepdim: return [output_grad] else: reshape_grad = expand_as_3d(output_grad, node.inputs[0]) return [reshape_grad] class ExpandAsOp(Op): \\\"\\\"\\\"Op to broadcast a tensor to the shape of another tensor. Note: This is a reference implementation for ExpandAsOp. If it does not work in your case, you can modify it. \\\"\\\"\\\" def __call__(self, node_A: Node, node_B: Node) -> Node: return Node( inputs=[node_A, node_B], op=self, name=f\\\"broadcast({node_A.name} -> {node_B.name})\\\", ) def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: \\\"\\\"\\\"Return the broadcasted tensor.\\\"\\\"\\\" assert len(input_values) == 2 input_tensor, target_tensor = input_values return input_tensor.expand_as(target_tensor) def gradient(self, node: Node, output_grad: Node) -> List[Node]: \\\"\\\"\\\"Given the gradient of the broadcast node, compute partial adjoint to input.\\\"\\\"\\\" return [sum_op(output_grad,dim=0), zeros_like(output_grad)] class ExpandAsOp3d(Op): \\\"\\\"\\\"Op to broadcast a tensor to the shape of another tensor. Note: This is a reference implementation for ExpandAsOp3d. If it does not work in your case, you can modify it. \\\"\\\"\\\" def __call__(self, node_A: Node, node_B: Node) -> Node: return Node( inputs=[node_A, node_B], op=self, name=f\\\"broadcast({node_A.name} -> {node_B.name})\\\", ) def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: \\\"\\\"\\\"Return the broadcasted tensor.\\\"\\\"\\\" assert len(input_values) == 2 input_tensor, target_tensor = input_values print('expand_op',input_tensor.shape, target_tensor.shape) return input_tensor.unsqueeze(1).expand_as(target_tensor) def gradient(self, node: Node, output_grad: Node) -> List[Node]: \\\"\\\"\\\"Given the gradient of the broadcast node, compute partial adjoint to input.\\\"\\\"\\\" return [sum_op(output_grad,dim=(0, 1)), zeros_like(output_grad)] class LogOp(Op): \\\"\\\"\\\"Logarithm (natural log) operation.\\\"\\\"\\\" def __call__(self, node_A: Node) -> Node: return Node( inputs=[node_A], op=self, name=f\\\"Log({node_A.name})\\\", ) def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: \\\"\\\"\\\"Return the natural logarithm of the input.\\\"\\\"\\\" assert len(input_values) == 1, \\\"Log operation requires one input.\\\" return torch.log(input_values[0]) def gradient(self, node: Node, output_grad: Node) -> List[Node]: \\\"\\\"\\\"Given the gradient of the Log node, return the partial adjoint to the input.\\\"\\\"\\\" input_node = node.inputs[0] return [output_grad / input_node] class BroadcastOp(Op): def __call__(self, node_A: Node, input_shape: List[int], target_shape: List[int]) -> Node: return Node( inputs=[node_A], op=self, attrs={\\\"input_shape\\\": input_shape, \\\"target_shape\\\": target_shape}, name=f\\\"Broadcast({node_A.name}, {target_shape})\\\", ) def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: \\\"\\\"\\\"Return the broadcasted tensor.\\\"\\\"\\\" assert len(input_values) == 1 return input_values[0].expand(node.attrs[\\\"target_shape\\\"]) def gradient(self, node: Node, output_grad: Node) -> List[Node]: \\\"\\\"\\\"Given gradient of broadcast node, return partial adjoint to input. For broadcasting, we need to sum out the broadcasted dimensions to get back to the original shape. \\\"\\\"\\\" if \\\"input_shape\\\" not in node.attrs: raise ValueError(\\\"Input shape is not set. Make sure compute() is called before gradient()\\\") input_shape = node.attrs[\\\"input_shape\\\"] output_shape = node.attrs[\\\"target_shape\\\"] dims_to_sum = [] for i, (in_size, out_size) in enumerate(zip(input_shape[::-1], output_shape[::-1])): if in_size != out_size: dims_to_sum.append(len(output_shape) - 1 - i) grad = output_grad if dims_to_sum: grad = sum_op(grad, dim=dims_to_sum, keepdim=True) if len(output_shape) > len(input_shape): grad = sum_op(grad, dim=list(range(len(output_shape) - len(input_shape))), keepdim=False) return [grad]\"]},\"178\":{\"h\":\"自己实现的算子\"},\"179\":{\"h\":\"DivOP\",\"t\":[\"class DivOp(Op): \\\"\\\"\\\"Op to element-wise divide two nodes.\\\"\\\"\\\" def __call__(self, node_A: Node, node_B: Node) -> Node: return Node( inputs=[node_A, node_B], op=self, name=f\\\"({node_A.name}/{node_B.name})\\\", ) def compute(self, node: Node, input_values: List[torch.Tensor]) -> torch.Tensor: \\\"\\\"\\\"Return the element-wise division of input values.\\\"\\\"\\\" assert len(input_values) == 2 \\\"\\\"\\\"TODO: your code here\\\"\\\"\\\" def gradient(self, node: Node, output_grad: Node) -> List[Node]: \\\"\\\"\\\"Given gradient of division node, return partial adjoint to each input.\\\"\\\"\\\" \\\"\\\"\\\"TODO: your code here\\\"\\\"\\\"\"]},\"180\":{\"c\":[\"Base\"]},\"181\":{\"c\":[\"MLSys\",\"Base\"]},\"182\":{\"h\":\"阶乘的位数估算--数学在计算机算法研究中的作用\"},\"183\":{\"h\":\"题目引入\",\"t\":[\"算法与数据结构实验题 1.10 单身狗进化 这一天晚上，弯通又做梦了，并且梦到了一个帅气的男孩纸！这个男孩给了弯通一个数字 n。男孩离开前告诉弯通，n!（n 的阶乘）的位数就是距离弯通脱单的天数。矜（ji）持（ke）的弯通想知道自己还有多久能脱单，快写个程序帮助他！ 输入: 输入第一行为一个正整数 n（1<=n<=25000）。 输出: n阶乘的位数\"]},\"184\":{\"h\":\"题目分析\",\"t\":[\"这道题看上去还挺有意思的很符合大学生的心理状态, 实际上就是要求阶乘的位数倒也没有拐弯抹角. 但是我们都知道, 要是用递归或者循环写阶乘, 这将是一件极为恐怖的事情. 在数据存储(空间复杂度)&计算用时(时间复杂度)上的开销, 将成为任何一台机器的噩梦, 更不可能过测试了. 举个栗子:\",\" int n; long long ans = 1; std::cin >> n; for (int i = 1; i < MAX; i++) ans *= i; std::cout << ans;\",\"大家可以简单跑一下这个程序, 然后就会发现, 在n = 27的时候, 就已经溢出了, 完全无法满足题目要求. 这就是第一种错误的可能, 忘记了估计数据规模, 随便算算就存爆了.\",\"还有一种可能, 就是采用高精度的算法, 将阶乘结果用表存储, 每个内存存有限位数据, 在乘法时做类似竖式乘法的高精度运算.\",\"这种方式能不能过这个题我没有试过因为我懒, 但是一般来说高精度阶乘的时间复杂度是O(n2)\",\"程序代码的复杂度和 n = 25000 所要存储的数据规模, 也会是比较大的开销.\",\"下文将介绍一种用数学方法巧妙估算阶乘结果规模的方式.\"]},\"185\":{\"h\":\"斯特林公式\",\"t\":[\"n!≈2πn​(en​)n\",\"这个公式以詹姆斯·斯特林的名字命名，虽然亚伯拉罕·棣美弗早于斯特林提出了一个类似的公式，但结果较不精确. 当n很大的时候，n阶乘的计算量十分大，所以斯特林公式十分好用，而且，即使在n很小的时候，斯特林公式的取值已经十分准确.\",\"可以通过计算对比来估计一下斯特林公式算出结果, 和阶乘计算结果的误差程度.\",\"我们可以看到, 随着n的增大, 斯特林公式估算的误差已经降到了十万分之一以下, 这对估算阶乘的规模来说是完全可以接受的误差.\",\"通过斯特林公式我们可以简单估算阶乘的位数, 我们知道对于一个n进制数x, 都可以对其取$ [\\\\log_{n}x] + 1 $来得到这个n进制数的位数, 我们将进一步推导用斯特林公式估算阶乘位数N的公式.\",\"N=[log10​[2πn​(en​)n]]+1\",\"其中内层中括号标记运算顺序, 外层中括号意为高斯取整(即向下取整).\",\"N=[21​log10​(2πn)+nlog10​(en​)]+1\",\"通过代入n, 即可轻松求得n!的位数, 时间复杂度是梦寐以求的O(1), 即常数时间复杂度.\"]},\"186\":{\"h\":\"代码实现\",\"t\":[\"代码实现没什么好说的, 套公式罢了, 由于我之前已经测试过最大数据规模, 所以ans也是为了省事儿用的int偷懒是可耻的\",\" #include <stdio.h> #include <math.h> #define PI 3.141592654 #define E 2.71828182846 int pos(int n) { int s = 1; if(n > 3) s = log10(2*PI*n) / 2 + n * log10(n/E) + 1; return s; } int main() { int num, ans; scanf(\\\"%d\\\", &num); ans = pos(num); printf(\\\"%d\\\", ans); }\"]},\"187\":{\"h\":\"总结\",\"t\":[\"数即一切\"]},\"188\":{\"c\":[\"数据结构与算法\"]},\"189\":{\"c\":[\"算法\",\"数学\"]},\"190\":{\"h\":\"ACM Try -- 01\"},\"191\":{\"h\":\"1. Dragon String (龙字符串)\"},\"192\":{\"h\":\"题目描述\",\"t\":[\"给定一个正整数N，要求输出一个特定的字符串，该字符串由一个'L'、N个'o'、一个'n'和一个'g'按顺序组成。\"]},\"193\":{\"h\":\"约束条件\",\"t\":[\"1 ≤ N ≤ 2024\",\"N为整数\"]},\"194\":{\"h\":\"解题思路\",\"t\":[\"这是一道简单的字符串构造题，主要考察基础的字符串操作和循环控制。解题步骤如下：\",\"读取输入整数N\",\"输出字符'L'\",\"循环N次输出字符'o'\",\"最后输出字符'n'和'g'\"]},\"195\":{\"h\":\"易错点\",\"t\":[\"输出字符的大小写问题：'L'必须是大写，'o'、'n'、'g'必须是小写\",\"循环次数控制：容易多输出或少输出一个'o'\",\"输出顺序：必须严格按照L->o->n->g的顺序\"]},\"196\":{\"h\":\"代码实现\",\"t\":[\"#include <bits/stdc++.h> using namespace std; int main() { int n; cin >> n; cout << 'L'; while (n-- > 0) { cout << 'o'; } cout << 'n' << 'g'; return 0; }\"]},\"197\":{\"h\":\"复杂度分析\",\"t\":[\"时间复杂度：O(N)\",\"空间复杂度：O(1)\"]},\"198\":{\"h\":\"2. YES字符串判断\"},\"199\":{\"h\":\"题目描述\",\"t\":[\"给定T个测试用例，每个用例包含一个字符串，判断该字符串是否等于\\\"YES\\\"（不区分大小写）。\"]},\"200\":{\"h\":\"约束条件\",\"t\":[\"1 ≤ T ≤ 103\",\"输入字符串只包含大小写英文字母\"]},\"201\":{\"h\":\"解题思路\",\"t\":[\"这是一道字符串比较题，主要考察字符串处理和大小写转换。解题要点：\",\"将输入字符串转换为小写\",\"与标准字符串\\\"yes\\\"比较\",\"注意处理大小写不敏感的比较\"]},\"202\":{\"h\":\"易错点\",\"t\":[\"输入字符串长度未判断：虽然题目保证输入合法，但在实际工程中应该加上长度判断\",\"输出大小写问题：输出\\\"YES\\\"或\\\"NO\\\"时必须全部大写\",\"字符串比较前的转换：必须先将输入字符串转换为小写，再进行比较\",\"返回值处理：main函数末尾漏掉return 0\"]},\"203\":{\"h\":\"代码实现\",\"t\":[\"#include <bits/stdc++.h> #include <cctype> #include <cstdio> using namespace std; int main() { int n; cin >> n; string boolstr; while (n -- > 0) { cin >> boolstr; // 将整个字符串转换为小写 for(char &c : boolstr) { c = tolower(c); } if (boolstr == \\\"yes\\\") { printf(\\\"YES\\\\n\\\"); } else { printf(\\\"NO\\\\n\\\"); } } }\"]},\"204\":{\"h\":\"复杂度分析\",\"t\":[\"时间复杂度：O(T×L)，其中L为字符串长度\",\"空间复杂度：O(1)\"]},\"205\":{\"h\":\"3. 奇偶判断\"},\"206\":{\"h\":\"题目描述\",\"t\":[\"给定N个大整数，判断每个数的奇偶性。\"]},\"207\":{\"h\":\"约束条件\",\"t\":[\"1 ≤ N ≤ 100\",\"输入整数不超过10^60\"]},\"208\":{\"h\":\"解题思路\",\"t\":[\"这是一道大数处理题，但有一个重要的数学性质：一个数的奇偶性只取决于其最后一位数字。因此：\",\"将输入数字以字符串形式读入\",\"只需判断最后一个字符对应的数字的奇偶性\",\"无需进行完整的大数运算\"]},\"209\":{\"h\":\"易错点\",\"t\":[\"大数处理方式：试图将输入转换为整数类型会导致溢出\",\"字符转数字：忘记将字符转换为数字（减去'0'）就直接判断\",\"输出格式：输出\\\"odd\\\"或\\\"even\\\"时必须全部小写\",\"边界情况：没有考虑输入为单个数字的情况\"]},\"210\":{\"h\":\"代码实现\",\"t\":[\"#include <bits/stdc++.h> using namespace std; int main() { int n; cin >> n; string num; while (n-- > 0) { cin >> num; // 只需要检查最后一位数字即可判断奇偶性 if ((num.back() - '0') % 2 == 1) cout << \\\"odd\\\\n\\\"; else cout << \\\"even\\\\n\\\"; } return 0; }\"]},\"211\":{\"h\":\"复杂度分析\",\"t\":[\"时间复杂度：O(N)\",\"空间复杂度：O(1)\"]},\"212\":{\"h\":\"4. 字符统计问题\"},\"213\":{\"h\":\"题目描述\",\"t\":[\"给定一个字符串和目标次数m，需要计算使每个字符出现次数达到m次所需添加的最少字符数。\"]},\"214\":{\"h\":\"解题思路\",\"t\":[\"这是一道哈希统计题，主要思路如下：\",\"使用哈希数组统计每个字符的出现次数\",\"对于每个字符，计算需要补充多少个才能达到目标次数m\",\"累加所有需要补充的字符数\"]},\"215\":{\"h\":\"易错点\",\"t\":[\"哈希数组大小：vector初始化大小必须足够存储所有可能的字符\",\"字符映射：将字符转换为数组下标时的计算可能越界\",\"补充计算：当字符出现次数已经超过m时，不需要再补充\",\"测试用例处理：忘记处理多组测试用例的情况\"]},\"216\":{\"h\":\"代码实现\",\"t\":[\"#include <bits/stdc++.h> using namespace std; int main() { int cnt; cin >> cnt; while (cnt -- > 0) { int m, n; int count = 0; cin >> n >> m; string line; vector<int> hash(7); cin >> line; for (auto &ch : line) { hash[Hash(int(ch - 'A'))]++; } for (auto k : hash) { if (m > k) count += m - k; } printf(\\\"%d\\\\n\\\", count); } return 0; }\"]},\"217\":{\"h\":\"复杂度分析\",\"t\":[\"时间复杂度：O(T×N)，其中T为测试用例数，N为字符串长度\",\"空间复杂度：O(1)，因为哈希数组大小固定\"]},\"218\":{\"h\":\"5. 投票统计\"},\"219\":{\"h\":\"题目描述\",\"t\":[\"给定n个人在m天内对规则k的投票情况，判断规则k是否符合民意。规则符合民意的条件是：在过半数的天数中，有过半数的人支持该规则。\"]},\"220\":{\"h\":\"解题思路\",\"t\":[\"这是一道二维统计题，需要两层判断：\",\"对每一天统计支持规则k的人数，判断是否过半\",\"统计符合条件的天数是否过半\",\"使用排序来简化判断过半的操作\"]},\"221\":{\"h\":\"易错点\",\"t\":[\"过半数计算：(n+1)/2 而不是 n/2，需要考虑奇数情况\",\"数组初始化：right数组的大小应该是m而不是当前的m值（因为m在循环中会减少）\",\"排序后的判断：需要判断right[0]和right[(days-1)/2]，而不是简单统计1的个数\",\"变量保存：需要保存原始的m值用于最后判断\"]},\"222\":{\"h\":\"代码实现\",\"t\":[\"#include <bits/stdc++.h> using namespace std; int main() { int n, m, k; cin >> n >> m >> k; vector<int> right(m, 0); int days = m; while (m -- > 0) { int cnt = 0; int num = 0; for (int i = 0; i < n; i++) { cin >> num; if (num == k) cnt++; } if (cnt >= (n + 1) / 2) { right[m] = 1; } } sort(right.begin(), right.end(), [](int x, int y){return x > y;}); if (right[0] == 1 && right[(days - 1) / 2] == 1) printf(\\\"YES\\\"); else printf(\\\"NO\\\"); return 0; }\"]},\"223\":{\"h\":\"复杂度分析\",\"t\":[\"时间复杂度：O(M×N + MlogM)\",\"空间复杂度：O(M)\"]},\"224\":{\"h\":\"6. 字符替换\"},\"225\":{\"h\":\"题目描述\",\"t\":[\"给定一个字符串S和Q次操作，每次操作将字符串中的某个字符全部替换为另一个字符。\"]},\"226\":{\"h\":\"解题思路\",\"t\":[\"这是一道字符串模拟题，主要考察字符串的遍历和替换操作：\",\"读入初始字符串\",\"对每次操作，遍历整个字符串进行替换\",\"注意替换操作要同时进行，避免连锁反应\"]},\"227\":{\"h\":\"易错点\",\"t\":[\"替换顺序：每次操作必须同时替换所有匹配的字符，不能边替换边比较\",\"自身替换：当c和d相同时也需要正确处理\",\"字符串修改：使用引用或指针修改字符串时的正确性\",\"输出格式：最后需要输出换行符\"]},\"228\":{\"h\":\"代码实现\",\"t\":[\"#include <iostream> #include <string> using namespace std; int main() { int N; string S; int Q; cin >> N >> S >> Q; for(int i = 0; i < Q; i++) { char c, d; cin >> c >> d; for(int j = 0; j < N; j++) { if(S[j] == c) { S[j] = d; } } } cout << S << endl; return 0; }\"]},\"229\":{\"h\":\"复杂度分析\",\"t\":[\"时间复杂度：O(Q×N)\",\"空间复杂度：O(N)\"]},\"230\":{\"h\":\"7. 矩阵操作\"},\"231\":{\"h\":\"题目描述\",\"t\":[\"给定一个n×n的矩阵和m次操作，每次操作可以交换两行或两列。\"]},\"232\":{\"h\":\"解题思路\",\"t\":[\"这是一道矩阵模拟题，但有一个优化技巧：\",\"不直接修改矩阵，而是维护行列的映射关系\",\"每次交换只需要修改映射数组\",\"最后输出时根据映射关系重建矩阵\"]},\"233\":{\"h\":\"易错点\",\"t\":[\"下标转换：输入的行列号从1开始，需要减1后再使用\",\"映射数组初始化：必须正确初始化row和col数组为0到n-1\",\"矩阵访问：输出时使用映射数组访问matrix[row[i]][col[j]]而不是直接访问\",\"输出格式：最后一个数字后不能有空格，需要换行\"]},\"234\":{\"h\":\"代码实现\",\"t\":[\"#include <iostream> #include <vector> using namespace std; int main() { int n, m; cin >> n >> m; vector<vector<int>> matrix(n, vector<int>(n)); for (int i = 0; i < n; i++) { for (int j = 0; j < n; j++) { cin >> matrix[i][j]; } } vector<int> row(n), col(n); for (int i = 0; i < n; i++) { row[i] = i; col[i] = i; } for (int i = 0; i < m; i++) { int op, x, y; cin >> op >> x >> y; x--; y--; if (op == 1) { swap(row[x], row[y]); } else { swap(col[x], col[y]); } } for (int i = 0; i < n; i++) { for (int j = 0; j < n; j++) { cout << matrix[row[i]][col[j]] << (j == n-1 ? '\\\\n' : ' '); } } return 0; }\"]},\"235\":{\"h\":\"复杂度分析\",\"t\":[\"时间复杂度：O(n² + m)\",\"空间复杂度：O(n²)\"]},\"236\":{\"h\":\"总结\",\"t\":[\"这组题目涵盖了多个基础算法知识点：\",\"字符串处理（题目1、2、6）\",\"大数处理技巧（题目3）\",\"哈希统计（题目4）\",\"二维统计与排序（题目5）\",\"矩阵操作与状态维护（题目7）\",\"每道题都体现了一些重要的编程思想：\",\"善用数学性质简化问题（题目3）\",\"使用哈希表优化统计（题目4）\",\"排序简化判断（题目5）\",\"状态映射代替直接修改（题目7）\",\"这些思想在更复杂的算法题中都会经常用到。\"]},\"237\":{\"c\":[\"算法与数据结构\"]},\"238\":{\"c\":[\"算法\"]},\"239\":{\"h\":\"大规模AI模型的infer并行计算范式\"},\"240\":{\"h\":\"\",\"t\":[\"随着模型参数规模迈入万亿级别，其在推理（Inference）阶段的部署面临着内存墙（Memory Bound）、计算墙（Compute Bound）与I/O墙（I/O Bound）等多重挑战。本文系统性地梳理和剖析大规模模型推理所采用的高性能并行计算范式。文章首先对数据并行（Data Parallelism, DP）、流水线并行（Pipeline Parallelism, PP）和张量并行（Tensor Parallelism, TP）等基础概念进行界定，并探讨其在推理场景下的应用边界与理论局限。其次，深入探讨了以连续批处理（Continuous Batching）和分页注意力（PagedAttention）为代表的现代推理系统的核心架构演进。在此基础上，本文进一步剖析了模型量化（Quantization）与推测解码（Speculative Decoding）等前沿算法优化。为将理论与实践相结合，本文选取了三个典型案例进行深度研究：面向特定架构的组件级并行（以Stable Diffusion为例）、应对超长上下文挑战的序列并行，以及在复杂单一样本场景下的混合并行策略（以AlphaFold3为例）。最后，通过一个在HPC环境下部署CPU推理集群的工程案例，展示了如何在现实约束下进行系统架构设计。\"]},\"241\":{\"h\":\"\",\"t\":[\"推理成为新的瓶颈\",\"后训练时代（Post-Training Era）的来临：随着模型即服务（Model-as-a-Service, MaaS）的兴起，推理取代训练，成为AI应用的主要成本中心和性能瓶颈。\",\"推理阶段的三大核心挑战： \",\"内存墙 (Memory Wall)：模型权重与动态KV缓存的巨大内存占用，常常超出单张加速卡的承载能力。\",\"计算墙 (Compute Wall)：自回归解码（Autoregressive Decoding）带来的序列化计算，以及单次前向传播巨大的浮点运算量（FLOPs），对计算单元构成严峻考验。\",\"吞吐墙 (Throughput Wall)：在满足严格延迟（Latency）约束下，如何最大化并发处理能力，是实现经济高效服务的关键。\",\"本文结构与贡献\",\"系统性构建推理并行技术的分类体系。\",\"深入分析关键系统优化与算法优化的内在机理。\",\"通过前沿案例研究，展示多维并行策略的综合应用。\",\"提供一个从理论到工程实践的完整审视视角。\"]},\"242\":{\"h\":\"\"},\"243\":{\"h\":\"\",\"t\":[\"核心思想：通过模型副本（Model Replication）实现请求级并行。每个工作单元（Worker）都持有一个完整的模型副本，并将输入数据集切分为多个片段，并行处理。\",\"架构模式：通常由一个负载均衡器（Load Balancer）和多个独立的推理副本构成。各副本独立完成计算后，定期聚合梯度以确保模型权重的一致性。\",\"应用与理论局限： \",\"无法解决单模型瓶颈：当单个模型的体积超出单设备显存时，DP无法适用。\",\"成本：部署成本随副本数量线性增长。\",\"效率问题：在全局批量大小（Global Batch Size）固定的情况下，增加设备数会减小每个设备上的局部批量（Local Batch Size）。过小的局部批量可能导致通信开销占比过大，造成GPU计算资源闲置。\"]},\"244\":{\"h\":\"\",\"t\":[\"核心思想：算子间（Inter-Operator）的并行化。它将模型的计算图（Computational Graph）按层（Stage）进行垂直切分，不同阶段部署在不同设备上，形成一条“计算流水线”。\",\"计算模型：通过微批处理（Micro-batching）与流水线调度，让多个微批次（Micro-batches）在流水线的不同阶段重叠计算，从而提升硬件利用率。\",\"应用与理论局限： \",\"流水线气泡 (Pipeline Bubble)：在流水线的启动和排空阶段，部分设备会处于空闲状态，产生“气泡”开销。气泡大小的理论占比为 (p−1)/m，其中 p 是流水线阶段数，m 是微批次数量。这表明增加微批次数量或减少流水线阶段数可以减小气泡。\",\"延迟：PP无法降低单一样本的端到端延迟。\",\"负载均衡：各阶段计算量的不均衡是实践中的关键挑战。\"]},\"245\":{\"h\":\"\",\"t\":[\"核心思想：算子内部（Intra-Operator）的并行化，将单个计算密集型算子（如矩阵乘法）水平切分至多个计算单元。TP对节点内的高带宽通信（如NVLink）有强依赖性。\",\"数学原理：基于矩阵乘法的结合律，对权重矩阵进行行或列切分。\",\"列并行 (Column Parallelism)：将权重矩阵A按列切分 A=[A1​,A2​]。输入X广播给所有设备，各设备独立计算 XA1​ 和 XA2​。最后，通过 All-Gather 通信将结果拼接为完整的输出 Y=[Y1​,Y2​]。\",\"行并行 (Row Parallelism)：将权重矩阵A按行切分 $ A = \\\\begin{pmatrix} A_1 \\\\ A_2 \\\\end{pmatrix} $ 输入X也相应地按列切分 X=[X1​,X2​]。各设备独立计算 X1​A1​ 和 X2​A2​。最后，通过 All-Reduce 通信对部分结果求和，得到最终输出 Y=Y1​+Y2​。\",\"在Transformer中的应用 (Megatron-LM)：TP巧妙地应用于Transformer的MLP层和多头注意力（MHA）层，以最小化通信开销。\",\"MLP层：采用“列并行-行并行”的策略。第一个线性层按列并行，经过GeLU激活函数（该函数可独立在各设备上计算，无需通信），第二个线性层按行并行。这样，仅在第二次矩阵乘法后需要一次All-Reduce通信，极大优化了效率。\",\"MHA层：将Q, K, V的投影计算按“头”的数量进行切分，分配到不同GPU上。每个GPU独立计算其分配到的注意力头的得分和输出，最后通过一次All-Reduce操作将各头的输出聚合并投影，得到最终结果。\",\"理论局限：通信开销随并行度增加而上升，存在扩展性边界。当并行度过高，切分后的矩阵过小，可能导致GPU计算核心利用率下降。\"]},\"246\":{\"h\":\"\"},\"247\":{\"h\":\"\",\"t\":[\"设计哲学：为了训练和部署万亿参数级别的模型，单一的并行策略往往不足。 现代框架如Megatron-Deepspeed采用“3D并行”策略，有机结合了DP、PP和TP。 \",\"节点内（Intra-Node）使用TP：充分利用节点内GPU之间的高速NVLink总线，执行对通信带宽要求极高的张量并行。\",\"节点间（Inter-Node）使用PP：将模型的不同层（Stages）分布在不同服务器节点上，通过InfiniBand等高带宽网络传递激活值。\",\"在整个集群上使用DP：对配置好的“TP+PP”模型进行复制，以数据并行的方式扩展整个集群的吞吐能力。 这种分层设计最大化地利用了异构的通信带宽层级，实现了高效扩展。\"]},\"248\":{\"h\":\"\",\"t\":[\"调度革命：连续批处理 (Continuous Batching)：传统推理采用静态批处理，必须等待批次中所有请求都完成后才能返回结果，导致GPU在等待最慢请求时空闲。连续批处理则将请求处理看作一个连续的流，一旦某个请求完成，立刻将其从批次中移除并返回结果，同时动态地将新请求加入批次，从而消除GPU空闲，大幅提升吞吐量。\",\"内存革命：分页注意力 (PagedAttention)：该技术是vLLM等现代推理框架的核心。 它借鉴了操作系统中虚拟内存和分页的思想，解决了KV缓存的内存碎片问题。\",\"原理：将逻辑上连续的KV缓存序列，存储在物理上非连续的内存块（Block）中。通过一个“块表”来管理逻辑块到物理块的映射。\",\"优势：极大地减少了内存浪费，使得系统能够容纳更多的并发请求，吞吐量提升显著。\"]},\"249\":{\"h\":\"\"},\"250\":{\"h\":\"\",\"t\":[\"原理：通过降低权重和/或激活值的数值精度（例如从FP16到INT8或INT4），显著减少模型的内存占用和计算量。\",\"主流方法评述： \",\"GPTQ (Generalized Post-Training Quantization)：一种基于近似二阶信息的单次权重压缩方法，精度较高。\",\"AWQ (Activation-aware Weight Quantization)：该方法观察到，并非所有权重都同等重要，对模型性能影响最大的是那些接收到大幅度激活值的权重。 因此，AWQ通过保护这约1%的关键权重，在量化过程中极大地降低了精度损失，实现了优异的性能。\"]},\"251\":{\"h\":\"\",\"t\":[\"原理：该技术旨在打破自回归解码“一次生成一个Token”的序列化限制。 它利用一个轻量级的“草稿模型”（Draft Model）一次性快速生成多个候选Token，然后由原始的、更强大的“目标模型”（Target Model）进行一次并行的验证。 如果草稿被接受，解码过程就向前跳跃了多个步骤，从而实现加速。\",\"性能模型分析：其加速效果与草稿模型的接受率（Acceptance Rate）和草稿模型的延迟密切相关。 若草稿模型预测准确率高且自身速度快，则能带来显著的性能提升。反之，若草稿频繁被拒，则可能因引入额外计算而降低整体速度。\"]},\"252\":{\"h\":\"\"},\"253\":{\"h\":\"\",\"t\":[\"问题域：文生图（Text-to-Image）模型如Stable Diffusion包含多个计算特性各异的阶段，如Text Encoder、U-Net和VAE Decoder。\",\"解决方案：构建“宏观流水线”（Macro-pipeline）。将这些独立的组件视为流水线的不同阶段，分别部署在不同的硬件单元上。这实现了组件级的流水线并行，当处理多个图像生成任务时，可以极大地重叠计算，从而最大化图像生成吞吐量。\"]},\"254\":{\"h\":\"\",\"t\":[\"问题域：在处理长序列时，注意力机制中与序列长度N相关的O(N2)内存复杂度成为根本瓶颈。\",\"解决方案：序列并行（SP）沿序列（Sequence）维度对张量及计算进行切分。 这意味着，对于那些在TP中无法拆分的操作（如LayerNorm和Dropout），可以通过在序列维度上切分，将激活内存分散到多个设备上，从而支持更长的序列训练和推理。 通过与张量并行协同的通信策略（如Reduce-Scatter, All-Gather），SP避免了在单个设备上实例化完整的注意力矩阵。\"]},\"255\":{\"h\":\"\",\"t\":[\"问题域：在生物分子结构预测等场景中，往往处理的是batch_size=1的“超级样本”。 此时，数据并行失效，如何高效地对单个复杂样本进行并行计算成为核心挑战。AlphaFold3作为典型代表，其系统设计融合了多维度并行策略。\",\"多维度并行策略剖析： \",\"模型内部并行：张量并行（TP）和流水线并行（PP）是基础，用于将巨大的模型本身进行切分。\",\"算法流程并行：AlphaFold3采用了类似扩散模型的生成过程，这其中包含多次独立的采样或去噪步骤，这些步骤天然适合进行任务级并行（Task Parallelism）。\",\"多维张量并行：对于模型中特别宽的层，可以采用2D或2.5D的张量并行策略。与1D TP仅切分一个维度不同，2D TP将权重矩阵W和激活X同时在行和列维度上切分到一个二维的设备网格中，使得单个设备的显存占用从O(1/q)降低到O(1/q2)（q为设备数），从而容纳更大的模型。\"]},\"256\":{\"h\":\"\",\"t\":[\"案例背景：在基于SLURM的纯CPU高性能计算（HPC）集群上，规模化部署基于vLLM的DeepSeek模型。\",\"面临的约束：vLLM的CPU后端功能当时尚不完善（如TP/PP支持有限），且其内置的分布式后端Ray与HPC集群的作业管理系统（如SLURM）不兼容。\",\"架构设计：服务层并行化与解耦编排\",\"核心思路：构建一个由统一入口（如SGLang Router）和由SLURM管理的多个独立vLLM实例组成的解耦式架构。\",\"架构解析：该架构中，Router负责接收所有请求并进行负载均衡，然后将请求分发给由SLURM动态启动和管理的多个vLLM推理服务进程。这种方式在服务层实现了请求的并行处理，有效绕开了底层框架对特定分布式后端（Ray）的依赖和功能限制。这是一个典型的“以系统架构设计弥补框架能力不足”的工程范例。\",\"单节点优化：在每个CPU节点内部，结合AWQ量化和推测解码等算法，分别针对CPU环境下的内存带宽和单样本延迟瓶颈进行深度优化。\"]},\"257\":{\"h\":\"\"},\"258\":{\"h\":\"\",\"t\":[\"张量并行（TP）的核心是将神经网络中计算密集的算子（主要是矩阵乘法）分解到多个设备上。其精髓在于利用矩阵乘法的线性代数性质，巧妙地设计计算与通信的顺序，以最小化开销。\"]},\"259\":{\"h\":\"\",\"t\":[\"一个标准的Transformer MLP层包含两个线性变换和一个非线性激活函数（如GeLU），其计算可以表示为：\",\"Y=GeLU(XA)B\",\"其中，X是输入，A和B是权重矩阵。在Megatron-LM中，这一过程被分解为两次矩阵乘法，并应用了“列并行-行并行”的策略，以实现通信优化。假设我们有 p 个GPU。\",\"1. 第一次矩阵乘法：列并行 (Column Parallelism)\",\"权重切分：将第一个权重矩阵 A 按其列维度切分为 p 块：\",\"A=[A1​,A2​,…,Ap​] 其中，Ai​ 是分配给第 i 个GPU的子矩阵。\",\"计算流程：输入 X 被视为一个f操作（在前向传播中是广播/Identity，在反向传播中是All-Gather）。每个GPU都拥有完整的输入X，并独立计算其部分结果：\",\"Zi​=XAi​\",\"本地激活：由于GeLU是一个逐元素（element-wise）的操作，它可以直接在每个GPU上本地执行，而无需任何通信：\",\"Gi​=GeLU(Zi​)关键洞察：在此处进行非线性激活是通信优化的核心。如果先将[Z1​,Z2​,…,Zp​]通过All-Gather拼接起来再激活，将会引入一次代价高昂的通信。\",\"2. 第二次矩阵乘法：行并行 (Row Parallelism)\",\"权重切分：将第二个权重矩阵 B 按其行维度切分为 p 块：\",\"B=​B1​B2​⋮Bp​​​\",\"计算流程：每个GPU i 使用其本地的激活结果 Gi​ 和权重分块 Bi​ 计算部分输出：\",\"Yi​=Gi​Bi​\",\"结果聚合：最终的输出 Y 是所有部分输出 Yi​ 的总和。这通过一次 All-Reduce 操作完成。这被视为一个g操作（在前向传播中是All-Reduce，在反向传播中是Identity）。\",\"Y=i=1∑p​Yi​=All-Reduce({Y1​,Y2​,…,Yp​})\",\"通过这种“列并行 -> 本地激活 -> 行并行”的流程，一个完整的MLP层只在最后需要一次All-Reduce通信，极大地提升了并行效率。\"]},\"260\":{\"h\":\"\",\"t\":[\"MHA层的并行化利用了注意力头（Attention Heads）之间天然的独立性。\",\"核心思想：将总共h个注意力头均分到p个GPU上，每个GPU负责h/p个头的计算。\",\"权重切分：Query, Key, Value的投影矩阵 WQ,WK,WV 均按其输出维度（列维度）进行切分，与MLP的第一层类似。每个GPU i 持有其对应注意力头的权重分片 WiQ​,WiK​,WiV​。\",\"并行计算： \",\"输入 X 广播至所有GPU。\",\"每个GPU i 独立计算其分配到的注意力头的Q, K, V：\",\"Qi​,Ki​,Vi​=XWiQ​,XWiK​,XWiV​\",\"每个GPU i 独立计算其注意力输出：\",\"Headi​=Attention(Qi​,Ki​,Vi​)=Softmax(dk​​Qi​KiT​​)Vi​\",\"结果聚合： \",\"所有GPU的注意力头输出 {Head1​,…,Headp​} 在逻辑上需要被拼接（Concatenate）。\",\"拼接后的结果再通过输出投影矩阵 WO 进行线性变换。\",\"为了优化，这一过程也采用了与MLP层类似的行并行策略。WO 矩阵按行切分，每个GPU计算一个部分投影结果，最后通过一次 All-Reduce 将所有部分结果相加，得到最终的MHA层输出。\"]},\"261\":{\"h\":\"\",\"t\":[\"当序列长度 s 变得极长时，即使经过TP切分，激活值（Activations）的内存占用（通常与 s×b×h 成正比，b为批量大小，h为隐藏层维度）也会成为瓶颈。序列并行旨在解决此问题。\",\"核心思想：在TP已将模型参数和计算在设备间切分的基础上，进一步将那些在TP中未被切分（即被复制）的张量，沿序列维度 s 进行切分。\",\"作用域：主要作用于LayerNorm、Dropout以及各种逐元素操作，因为这些操作在纯TP中需要在每个GPU上保留完整的激活副本。\",\"计算流程 (以LayerNorm为例)： \",\"输入切分：输入张量 X（形状为 (s,b,h)）沿序列维度s被切分为p块，X_i$（形状为 (s/p,b,h)）被分发到第i$个GPU。这是一个 Scatter 操作。\",\"局部统计：每个GPU i 在其本地数据 Xi​ 上计算局部的均值 μi​ 和方差 σi2​。\",\"全局统计同步：为了得到整个序列的准确均值μ和方差σ2，需要一次跨所有GPU的 All-Reduce 操作来聚合局部统计量。\",\"本地归一化：每个GPU使用同步后的全局μ和σ2来归一化其本地数据块Xi​。\",\"输出聚合：为了让后续的TP层（如矩阵乘法）能正常工作，需要将沿序列维度切分的激活重新拼接成完整的张量。这通过一次 All-Gather 操作完成。\",\"通过在TP通信的间隙插入沿序列维度的 Scatter 和 Gather 操作，SP将激活内存的占用降低了p倍，代价是增加了额外的通信开销。\"]},\"262\":{\"h\":\"\",\"t\":[\"1D TP虽然有效，但其通信成本与并行度 p 成正比，且激活内存并未完全切分。2D TP通过将设备组织成二维网格来进一步优化。\",\"核心思想：将 p 个GPU组织成一个 pr​×pc​ 的二维网格，其中 p=pr​×pc​。模型权重和激活张量同时沿两个维度进行切分。\",\"以矩阵乘法 Y=XA 为例：\",\"数据布局： \",\"输入激活 X 沿其行维度（通常是batch/sequence维度）被切分为 pr​ 份，并分发给设备网格的各行。即，同一行内的所有GPU拥有相同的 X 分片。\",\"权重矩阵 A 沿其列维度被切分为 pc​ 份，并分发给设备网格的各列。即，同一列内的所有GPU拥有相同的 A 分片。\",\"因此，位于网格坐标 $(i, j) 的GPU拥有数据 Xi​ 和 Aj​。\",\"计算与通信流程： \",\"第一步通信 (行内广播)：在设备网格的每一行内，输入分片 Xi​ 需要被广播给该行的所有GPU。经过此步，GPU $(i, j) 持有 Xi​ 和 Aj​。\",\"局部计算：每个GPU $(i, j) 独立计算其局部乘积：\",\"Yij​=Xi​Aj​\",\"第二步通信 (列内归约)：最终的结果 Y 是所有局部结果 Yij​ 在列维度上的拼接和行维度上的求和。为了得到最终输出 Yi​，需要在设备网格的每一列内进行一次 Reduce-Scatter 或 All-Reduce 操作，将同一列的所有 Yij​ (固定j, 变化i) 的结果聚合起来。\",\"优势分析：\",\"显存：与1D TP相比，激活内存和参数内存在2D TP中都被切分。单个GPU的激活内存占用减少为 1/pr​，参数内存占用减少为 1/pc​。\",\"通信：虽然需要两次通信，但每次通信只在较小的设备组（行或列）内进行，在某些网络拓扑下可以获得更优的通信效率。\"]},\"263\":{\"h\":\"\",\"t\":[\"推测解码通过“草稿-验证”机制来打破自回归解码的序列化瓶颈。\",\"参与者：\",\"目标模型 Mt​：原始的、强大的大模型。\",\"草稿模型 Md​：一个规模小得多、速度快得多的模型。\",\"详细流程：\",\"起草阶段 (Drafting)：给定当前已生成的序列（上下文）xprefix​，使用草稿模型 Md​ 以自回归的方式快速生成一个包含 k 个Token的草稿序列 γ=(x~1​,x~2​,…,x~k​)。在此过程中，记录下草稿模型在每一步的输出概率分布 Pd​(⋅∣xprefix​,x~1…i−1​)。\",\"验证阶段 (Verification)：将前缀和整个草稿序列拼接起来，形成一个新的输入 [xprefix​,γ]。将此输入一次性传入目标模型 Mt​ 进行前向计算。这会得到目标模型对于每个位置的预测概率分布 Pt​(⋅∣xprefix​,x~1…i−1​)。\",\"接受/拒绝决策 (Acceptance/Rejection)：从草稿的第一个Token开始，逐个进行判断：\",\"对于第 i 个草稿Token x~i​： \",\"获取它在草稿模型和目标模型中的概率：pd​=Pd​(x~i​∣…) 和 pt​=Pt​(x~i​∣…)。\",\"生成一个 [0,1] 之间的随机数 r。\",\"接受条件：如果 r≤min(1,pd​pt​​)，则接受 x~i​，并继续验证下一个Token x~i+1​。\",\"拒绝条件：如果 r>pd​pt​​，则拒绝 x~i​ 以及其后的所有草稿Token。\",\"修正与补全 (Correction & Completion)：\",\"如果发生拒绝：假设在第 i 个位置发生了拒绝，那么已验证通过的序列是 (x~1​,…,x~i−1​)。此时，需要从一个“修正后”的概率分布中采样一个新的Token来补上。该修正分布为：\",\"Pcorrected​(x)∝max(0,Pt​(x∣…)−Pd​(x∣…)) 从 Pcorrected​ 中采样一个Token xnew​，本轮解码结束。最终生成的序列是 (x~1​,…,x~i−1​,xnew​)。\",\"如果所有草稿都接受：如果所有 k 个草稿Token都被接受了，那么还需要从目标模型对最后一个位置的预测 Pt​(⋅∣xprefix​,γ) 中采样一个额外的Token xk+1​。本轮解码结束。\",\"性能增益来源：核心收益在于用 k 次廉价的草稿模型前向传播和一次昂贵的目标模型前向传播，替代了 k+1 次昂贵的目标模型前向传播。只要接受率足够高，就能实现显著加速。\"]},\"264\":{\"h\":\"\",\"t\":[\"流水线并行的核心挑战在于最小化“流水线气泡”（Pipeline Bubble），即由于流水线启动和排空阶段的依赖关系，导致部分设备处于闲置状态的时间。\"]},\"265\":{\"h\":\"\",\"t\":[\"假设我们有：\",\"p: 流水线阶段（Stages）的数量，等于并行的设备数。\",\"m: 微批次（Micro-batches）的总数量。\",\"Tf​: 单个微批次在一个阶段上的前向传播时间。\",\"Tb​: 单个微批次在一个阶段上的反向传播时间。\",\"在最朴素的调度（如GPipe的“刷新式”流水线）中，所有m个微批次完成前向传播后，才统一开始反向传播。\",\"总执行时间：\",\"Ttotal​=(p+m−1)Tf​+mTb​\",\"前(p−1)Tf​是流水线“填满”的时间，mTf​是所有微批次在最后一个阶段完成前向的时间，mTb​是反向传播时间。\",\"有效计算时间（所有设备都在忙碌的时间）：\",\"Tuseful​=m×p×(Tf​+Tb​)(单个GPU视角)\",\"Tuseful​=m×(Tf​+Tb​)(整个流水线视角)\",\"气泡时间 (Bubble Time)：\",\"Tbubble​=(p−1)(Tf​+Tb​)\",\"这个公式直观地显示了流水线的“启动延迟”和“排空延迟”的总和。\",\"流水线气泡占比 (Bubble Ratio)：\",\"Ratiobubble​=Ttotal​Tbubble​​=m(Tf​+Tb​)+(p−1)Tf​(p−1)(Tf​+Tb​)​\",\"当Tf​≈Tb​时，公式简化为：\",\"Ratiobubble​≈2m+p−12(p−1)​\",\"当微批次数量m远大于阶段数p时（m≫p），该比例近似为：\",\"Ratiobubble​≈mp−1​\",\"结论：气泡开销与阶段数p成正比，与微批次数量m成反比。这是优化流水线效率的根本出发点。\"]},\"266\":{\"h\":\"\",\"t\":[\"为了减小气泡，Megatron-LM提出了交错式1F1B（1 Forward, 1 Backward）调度策略。它将m个微批次划分为更小的块，并尽早开始反向传播，从而让前向和反向计算重叠。\",\"执行流程：一个阶段在完成某个微批次的前向计算后，不是立即开始下一个微批次的前向，而是优先执行已就绪的反向计算任务。这使得计算流水更加紧密，显著减少了GPU的空闲时间。通过这种方式，气泡大小可以被有效减小，但代价是需要缓存更多的激活值，因为前向和反向传播的间隔变长了。\"]},\"267\":{\"h\":\"\",\"t\":[\"序列并行将沿序列维度(s,b,h)的张量切分为p份，每个GPU持有$(s/p, b, h)的子张量。挑战在于，LayerNorm需要计算整个序列维度的均值和方差。\",\"1. LayerNorm的原始公式: 对于输入张量X中的每一个特征向量x，LayerNorm计算如下：\",\"y=Var[x]+ϵ​x−E[x]​⋅γ+β\",\"其中E[x]和\\\\text{Var}[x]是在特征维度上计算的。但在SP中，我们需要在序列和特征两个维度上进行归一化。\",\"2. 分布式均值和方差的计算: 假设第i个GPU持有数据块Xi​，其元素数量为Ni​。\",\"均值计算 μ:\",\"每个GPU i 计算其局部元素总和∑Xi​。\",\"使用 All-Reduce 操作将所有GPU的局部总和相加，得到全局总和 ∑X=∑i=1p​(∑Xi​)。\",\"全局均值为 μ=N∑X​，其中 N=∑Ni​是总元素数量。\",\"方差计算 σ2: 方差的标准公式是 σ2=E[X2]−(E[X])2。\",\"每个GPU i 计算其局部平方和∑Xi2​。\",\"使用 All-Reduce 操作将所有GPU的局部平方和相加，得到全局平方和 ∑X2=∑i=1p​(∑Xi2​)。\",\"计算全局平方的均值 E[X2]=N∑X2​。\",\"全局方差为 σ2=E[X2]−μ2。\",\"3. SP中的完整通信流程:\",\"fSP​(Scatter): 输入X沿序列维度被分散到各个GPU。\",\"局部计算: 计算局部的∑Xi​和∑Xi2​。\",\"All-Reduce: 同步全局的∑X和∑X2。\",\"本地归一化: 每个GPU使用全局的μ和σ2来归一化其本地数据块Xi​。\",\"gSP​(All-Gather): 将归一化后的数据块Yi​重新拼接成完整的输出张量Y，以供后续层使用。\",\"通过这两次通信（All-Reduce和All-Gather），序列并行以增加通信量为代价，成功地将巨大的激活内存分散到了多个设备上。\"]},\"268\":{\"h\":\"\",\"t\":[\"AWQ (Activation-aware Weight Quantization) 的核心洞察是：量化误差的显著性，不仅取决于权重本身，更取决于与之相乘的激活值的大小。\",\"1. 问题定义: 对于一个线性层 Y=WX，量化后的计算为 Yq​=quant(W)X。朴素量化（如round-to-nearest）的目标是最小化 ∥W−quant(W)∥。但这忽略了输入X的影响。AWQ认为，我们应该最小化最终输出的误差，即 ∥WX−quant(W)X∥。\",\"2. 激活感知的重要性: AWQ发现，模型中只有一小部分（约0.1%到1%）的权重对模型性能至关重要。这些权重并非其绝对值大，而是因为它们对应的输入通道（activation channel）的数值幅度始终很大。对于这些“显著”的通道，任何微小的权重误差都会被巨大的激活值放大，从而导致显著的性能下降。\",\"3. 尺度等效变换 (Scale-Equivalent Transformation): 为了保护这些重要权重，AWQ引入了一个尺度因子s。它将变换等效地应用于激活和权重：\",\"Y=WX=(Ws−1)(sX)=W′X′\",\"这里的s是一个对角矩阵，对X的每个输入通道进行缩放。量化的目标变成了最小化：\",\"∥W′X′−quant(W′)X′∥F2​\",\"其中∥⋅∥F​是Frobenius范数。\",\"4. 最佳尺度因子s的求解: AWQ的目标是找到一个最佳的s，使得W′=Ws−1在量化时损失最小。这意味着我们希望W′的数值范围更小、更适合量化。AWQ提出，最佳的尺度因子s_j（对于第j$个输入通道）应该与该通道激活值的幅度成正比。通过分析，他们给出了一个启发式的最佳尺度因子计算公式：\",\"sj∗​=(imax​∣Xij​∣)α或更优地sj∗​=(T1​t=1∑T​∣Xij(t)​∣2)1/4\",\"其中，Xij​是输入张量X的第i个token在第j个通道的值，\\\\alpha$是一个超参数（通常取0.5）。更鲁棒的做法是使用校准数据集（calibration data）计算激活的二阶矩（或近似为L2范数），然后取其四次方根。\",\"5. 算法流程总结:\",\"分析阶段: 使用一小部分校准数据对模型进行前向传播，记录每个线性层输入激活X的统计量（如每个通道的最大绝对值或L2范数）。\",\"确定重要权重: 根据激活的统计量，识别出那些持续接收到高幅度激活值的权重通道。\",\"计算尺度因子: 对每个通道j，根据其激活统计量计算出最佳尺度因子sj​。\",\"应用尺度并量化: \",\"对权重矩阵W的每一列j（对应输入通道j），除以sj​得到Wj′​=Wj​/sj​。\",\"对缩放后的W′进行量化，得到quant(W′)。\",\"推理阶段: \",\"对于输入X，其每个通道j乘以对应的sj​，得到X′。\",\"执行矩阵乘法 quant(W′)X′。\",\"通过这种方式，AWQ将量化的“难度”从权重转移到了激活上。由于激活的缩放是无损的浮点运算，而权重在缩放后其动态范围被压缩，因此量化误差大大减小，从而在极低的位宽下（如INT4/INT3）也能保持很高的模型精度。\"]},\"269\":{\"c\":[\"SOSD\"]},\"270\":{\"c\":[\"分布式系统\",\"MLSys\",\"并行运算\"]},\"271\":{\"h\":\"Diffusion（文生图、文生视频）推理服务\"},\"272\":{\"h\":\"文生图\",\"t\":[\"随着transformer在文本生成方面逐步展现出的巨大潜力和ChatGPT等生成式对话AI的逐步商业化, 处理多模态任务的价值也不断被挖掘, 文生图文生视频等潜力巨大的任务类型开始提上研究日程.\",\"但是同生成式对话使用有限的文本量便能达成较为不错的生成效果不同, 文生视频和文生图任务由于图形任务的整体性对大块内存的使用提出了更为严苛的要求, 图块与图块 像素与像素之间的高度关联性也难以像文本推理一样通过简单的切分矩阵实现并发, 这使得最为常用的 Diffusion模型在生成时长和内存占用上的表现都差强人意.\",\"文生图 文生视频等多模态任务中的分布式推理服务就是在这种背景下被关注的.\",\"目前考虑到的针对Diffusion的优化主要集中于以下几个方面:\",\"使用更好的solver, 减少采样步数, 避免多轮采样带来的内存开销\",\"利用diffusion相邻step冗余考虑适当保存activate值, 避免不必要的重计算\",\"使用diffusion parallelism\",\"使用一些通用优化手段,像算子并行,图优化,模型压缩等等.\"]},\"273\":{\"h\":\"DistriFusion\",\"t\":[\"https://openaccess.thecvf.com/content/CVPR2024/papers/Li_DistriFusion_Distributed_Parallel_Inference_for_High-Resolution_Diffusion_Models_CVPR_2024_paper.pdf\",\"https://github.com/mit-han-lab/distrifuser\",\"2024的CVPR, 工作属于是Diffusion parallelsim的一种\"]},\"274\":{\"h\":\"特点\",\"t\":[\"无需训练\",\"加速效果较好, 可以达到6倍以上加速\",\"利用了diffusion过程相邻步之间feature map的相似性\"]},\"275\":{\"h\":\"以往方法\",\"t\":[\"常规方法: 多GPU通常仅用于批量推理. 在生成单张图像时，通常只涉及一个GPU.考虑到激活值规模庞大, 通信成本会超过分布式计算带来的节省, 张量并行等并行技术并不适合diffusion模型.\",\"传统分批方法: 将图像split成N个patch，放在N个device上进行推理，然后将N个device的结果合成为一个全分辨率的结果. 但是这种方法由于缺少各个patch间的信息感知，会生成N个小图拼接而成的大图, 边界处会出现明显的接缝, 区块间引入交互又会带来过高的同步成本\",\"用精度换效率, 但是图片不是整体了, 这也是前言讲难以像文本推理一样通过简单的切分矩阵实现并行的原因\"]},\"276\":{\"h\":\"相关工作\",\"t\":[\"difussion核心在迭代去噪点生成内容, 用巨量的计算换取极高的生成能力, 目前优化主要集中在以下几点:\",\"高效去噪: 如将高分辨率图像压缩为低分辨率的潜在表示，并在潜在空间中学习扩散模型\",\"设计无需训练的高效采样算法: 基于扩散模型与微分方程之间的联系，并利用成熟的指数积分器来减少采样步骤\",\"预训练的扩散模型中提炼出生成模型: 成效不佳\",\"优化扩散模型的神经推理\",\"DistriFusion方法: 利用多个设备上的神经网络并行性来加速扩散过程\",\"针对LLM的并行方法特点是: LLM模型尺寸较大, 但激活尺寸小, 不需要引入太多通讯开销.\",\"但difussion模型的特点是: 模型尺寸较小, 但激活尺寸大, 于是通信开销不得不成为主要矛盾, 目前主要只使用数据并行.\",\"本文方法基于patch并行性, 切分小patch分到不同设备处理, 倾向于使用AllGather而非AllReduce进行数据交互.\"]},\"277\":{\"h\":\"背景知识\",\"t\":[\"扩散模型通常会训练一个噪声预测神经网络模型（如U-Net）ϵθ​. 从纯高斯噪声xT​∼N(0,I)开始，该模型需要经过数十到数百次的迭代去噪步骤，以获得最终的清晰图像x0​，其中T表示总步数. 在时间步t给定含噪图像xt​，模型ϵθ​会将xt​、t以及额外的条件c（例如文本）作为输入，以预测xt​中的相应噪声ϵt​. 在每个去噪步骤中，可以通过以下方程推导出xt−1​:\",\"xt−1​=Update(xt​,t,ϵt​),ϵt​=ϵθ​(xt​,t,c).\",\"通过这个公式我们可以轻易发现xt−1​和xt​的强依赖关系, 这给t与t−1步的模型ϵ并行带来了极大的困难.\",\"同时本文还指出, 处理一张1024x1024的图像时，每步需要6,763 GMACs（十亿次乘加运算）。随着分辨率的增加，这种计算需求会以超过二次方的速度增加，导致生成单张高分辨率图像的延迟在实际应用中变得过高.\",\"作者还在这里cue了一下Shih等人在2023提出的ParaDiGMS, 说利用Picard迭代以数据并行的方式并行化去噪步骤, 是可能存在无效结果的&步数较大的去噪方式, 难以并行加速. 这篇文章也是NeurIPS 2023的一篇, 平均加速在2倍左右, 的确没有本文出色.\",\"本文又一次强调了模型分片到多个设备上，并使用张量并行进行推理的大通讯量的不可行性.\"]},\"278\":{\"h\":\"本文方法\",\"t\":[\"将图像分割成多个区块，在多个设备间并行化处理计算, 这存在两个选择和两个问题: \",\"独立计算各个区块并随后拼接 -> 边界处会出现明显的接缝和强烈的撕裂感\",\"区块间同步传递中间激活信息 -> 极高的通信开销, 甚至超过计算时间\",\"为解决上述问题, 本文提出了一种新的并行范式--位移区块并行.\",\"核心:利用前一步扩散过程中稍有过时或“陈旧”的激活信息，来促进区块间的交互.\",\"在计算某一区块某一层的激活信息时，并不依赖于其他区块的最新激活信息，从而使得通信可以被隐藏在后续层的计算过程中.\",\"两次扩散时间步的图相似度是很高的, 确实有相当一部分激活值完全可以\\\"暂存\\\"而不用\\\"重计算\\\"或者\\\"通讯\\\".\"]},\"279\":{\"h\":\"位移补丁并行\",\"t\":[\"在预测ϵθ​(xt​)（为简化说明，此处省略了时间步t和条件c的输入）时，首先将xt​分割成多个补丁xt(1)​, xt(2)​, ..., xt(N)​，其中N表示设备的数量. 在例图使用了N=2.每个设备都拥有模型ϵθ​的一个副本，并将独立且并行地处理一个单独的补丁.上标(1)和(2)分别代表第一个和第二个块，前一步的陈旧激活被加深了颜色.\",\"在每一步t，首先将输入xt​分割成N个块xt(1)​, xt(2)​, ..., xt(N)​，对于每一层l和每个设备i，在获取输入激活块Atl,(i)​之后，将执行两个异步操作：\",\"在设备 i 上，Atl,(i)​被重新分散回前一步的陈旧激活Atl​.这个Scatter操作的输出随后被输入到稀疏操作Fl​中（线性、卷积或注意力层），它仅在新鲜区域上执行计算并产生相应的输出.\",\"与此同时，对A_{t}^{l,(i)}执行AllGather操作，以准备下一步的完整激活Atl​.\",\"最后，将最终输出聚合在一起，以近似ϵθ​(xt​)\"]},\"280\":{\"h\":\"总结\",\"t\":[\"DistriFusion是一种利用多GPU并行加速扩散模型的新方法.将图像分割成多个区块，并将每个区块分配给独立的GPU进行处理.同时复用前一步骤的预计算激活值，以保持区块间的相互作用.\",\"局限:\",\"对于低分辨率图像，DistriFusion的加速效果有限.\",\"对于步数极少的方法，由于去噪状态迅速变化，该方法可能不适用.\"]},\"281\":{\"h\":\"PipeFusion\",\"t\":[\"https://arxiv.org/html/2405.14430v2\",\"PipeFusion可以看作是在DistriFusion上的改进与推广, 重点是把DistriFusion方法移植到了DiT模型上, 证明了DistriFusion方法的泛用性.\"]},\"282\":{\"h\":\"摘要\",\"t\":[\"PipeFusion通过将图像分割成多个patch并跨多个GPU分布网络层，采用流水线并行方式来协调通信和计算.\",\"利用相邻扩散步骤输入之间的高相似性，通过复用一步旧的特征图来为当前步骤提供上下文，从而消除流水线中的等待时间\"]},\"283\":{\"h\":\"问题\",\"t\":[\"DiT模型的推理延迟显著，随着序列长度的增加，计算时间呈二次方增长.单GPU无法满足实际应用的延迟要求，需要跨多个计算设备并行化DiT推理.\",\"DistriFusion需要为每个GPU维护所有层的KV数据，导致GPU增加时内存开销增加.\",\"DistriFusion基于U-NET模型，在每个layer上做集合通信，通信开销较大.\"]},\"284\":{\"h\":\"本文方法\",\"t\":[\"和DistriFusion相同操作如patch分割, 异步修正, warmup预热不再赘述\",\"PipeFusion与DistriFusion不同的是:\",\"采用流水线并行方式来协调不同设备上的计算和通信, 但保存原本激活值的方式和DistriFusion保持一致\",\"采用P2P通信, 考虑DiT模型特点, 不再像DistriFusion每个时间步的通信都是Scatter给每个设备, 而是点对点通信, 减少通信成本\",\"提升存储效率, 与DistriFusion相比，PipeFusion每个设备只存储与其特定阶段相关的参数的1/N.\"]},\"285\":{\"h\":\"xDIT\",\"t\":[\"DiT模型在文生图和文生视频等表现出了杰出的性能, 但是与此同时DiTs 的输入序列长度日益增长，序列增长导致Attention 机制的计算量也随之呈平方级膨胀, 推理延迟极为严重, 单卡推理必然无法满足需求, 多GPU乃至多机DiT部署是必然要求.\",\"但是在之前都是基于hf diffusers库进行改造, 正如在vLLM前都是基于hf transformer进行改造, 都是临时的不成体系的方案, 迫切需求一个性能好且易用性高的DiT推理框架.\",\"于是, 在PipeFusion基础上升级的xDit诞生了.\",\"https://arxiv.org/abs/2405.14430\",\"https://github.com/xdit-project/xDiT\",\"DiT 和 LLM 推理任务特点不同, 改进思路也不同:\",\"LLM 有 Prefill 和 Decode 两阶段，分别是compute boundary和memory boundary的; 而DiT 的计算和 Prefill 相似是compute boundary的.\",\"LLM 模型很大，而序列长度有限(虽然现在长文本需求也在增加);而 DiT 正好反过来，模型不大，推理时序列长度很长, TP的适用性较低.\",\"LLM 模型大多收敛到微调 Llama 架构, 但DiT架构则呈现出较大差异性, 框架统一难度较大.\",\"考虑到 DiT 的特点，本文提出一系列混合并行方式, 提供了一套优雅的开发接口，针对性解决了 DiT 模型更改难度高的问题，这套开发接口尽可能可能复用 diffusers 的现有逻辑，开发者通过一些 wrapper 实现复杂的混合并行，实现高效地扩展方法.\"]},\"286\":{\"h\":\"Overview\"},\"287\":{\"h\":\"DiT 主干网络混合并行\",\"t\":[\"xDiT支持四种基础并行策略以任何形式混合, 达到近似线性效果:\",\"Pipefusion Parallel\",\"Sequence Parallel\",\"Data Parallel\",\"CFG Parallel https://arxiv.org/abs/2207.12598\"]},\"288\":{\"h\":\"Parallel VAE\",\"t\":[\"针对扩散模型后处理的解码 VAE 模块 (解码器模块)在高分辨率图像生成时 OOM (Out Of Memory)问题，xDiT 实现了 Patch Parallel 版本的 VAE.\"]},\"289\":{\"h\":\"简单灵活的开发接口\"},\"290\":{\"h\":\"PipeFusion\",\"t\":[\"见前文\"]},\"291\":{\"h\":\"USP：混合序列并行\",\"t\":[\"见MLSys_分布式开发（选读） DeepSpeed-Ulysses部分\"]},\"292\":{\"h\":\"CFG Parallel\",\"t\":[\"Classifier-Free Guidance（CFG）是扩散模型领域的一个重要的技巧，可以提供更广泛的条件控制、减少训练负担、增强生成内容的质量和细节，以及提高模型的实用性和适应性.\",\"一个输入 prompt，使用 CFG 需要同时进行 unconditional guide 和 text guide 的生成, 就是DistriFusion 中的 split batch.\"]},\"293\":{\"h\":\"Hybrid Parallel\",\"t\":[\"xDiT 设计目标是扩展 DiT 推理过程到超大规模, 实现多机多卡, 不同并行方式混合在一起变得尤为重要.\",\"PipeFusion 和 Sequence 在图像内部的不同 Patch 间并行则较为复杂, 两种并行方式的混合使用，是 xDiT 核心创新点之一.\",\"PipeFusion利用过时的KV进行Attention计算，这使得PipeFusion无法像大型语言模型（LLM）那样轻松地实现并行策略的混合, 以pipe_degree=4，sp_degree=2的混合并行方法为例:\",\"标准 SP Attention实现，输入Q，K，V和输出O都是沿着序列维度切分，且切分方式一致。如果不同rank的输入patch没有重叠，每个micro step计算出fresh KV更新的位置在不同rank间也没有重叠.\",\"如下图所示，standard SP的KV Buffer中黄色部分是SP0 rank=0拥有的fresh KV，绿色部分是SP1 rank=1拥有的fresh KV，二者并不相同.\",\"在这个diffusion step内，device=0无法拿到P1,3,5,7的fresh KV进行计算，但是PipeFusion则需要在下一个diffusion step中，拥有上一个diffusion step全部的KV(保留旧有激活值避免大量通讯).\",\"standard SP只拥有1/sp_degree的fresh kv buffer，因此无法获得混合并行推理正确的结果.\",\"xDiT专门定制了序列并行的实现方式，以适应这种混合并行的需求.\",\"xDiT使用xFuserLongContextAttention把SP的中间结果存在KV Buffer内.\",\"效果如下图，每个micro-step SP执行完毕后，SP Group内不同rank设备的fresh KV是相互补充的.\",\"这样一个diffusion step后，SP Group所有设备的KV Buffer都更新成最新，供下一个Diffusion Step使用.\"]},\"294\":{\"h\":\"Parallel VAE\",\"t\":[\"VAE 模块在高清图片生成时, 会导致OOM(见https://github.com/huggingface/diffusers/issues/5924).\",\"在xDiT开发了 DistVAE解决了这个问题, 使用了两种关键策略:\",\"SP: 特征图分割成多个 Patch，并在不同设备上进行序列并行 VAE 解码, 中间激活所需的峰值内存减少到 1/N.\",\"分块输入处理: 输入特征图分割成块，并依次送入卷积运算符\"]},\"295\":{\"c\":[\"SOSD\"]},\"296\":{\"c\":[\"分布式系统\",\"分布式推理\",\"文生图\"]},\"297\":{\"h\":\"\",\"t\":[\"404 Not Found\"]},\"298\":{\"h\":\"\"},\"299\":{\"h\":\"Posts\"}},\"dirtCount\":0,\"index\":[[\"特征图分割成多个\",{\"1\":{\"294\":1}}],[\"特点\",{\"0\":{\"274\":1}}],[\"供下一个diffusion\",{\"1\":{\"293\":1}}],[\"效果如下图\",{\"1\":{\"293\":1}}],[\"效率问题\",{\"1\":{\"243\":1}}],[\"效率更高\",{\"1\":{\"60\":1}}],[\"拥有上一个diffusion\",{\"1\":{\"293\":1}}],[\"二者并不相同\",{\"1\":{\"293\":1}}],[\"二维统计与排序\",{\"1\":{\"236\":1}}],[\"绿色部分是sp1\",{\"1\":{\"293\":1}}],[\"标准\",{\"1\":{\"293\":1}}],[\"间并行则较为复杂\",{\"1\":{\"293\":1}}],[\"见https\",{\"1\":{\"294\":1}}],[\"见mlsys\",{\"1\":{\"291\":1}}],[\"见前文\",{\"1\":{\"290\":1}}],[\"见前文tp内容\",{\"1\":{\"94\":1}}],[\"混合序列并行\",{\"0\":{\"291\":1}}],[\"版本的\",{\"1\":{\"288\":1}}],[\"达到近似线性效果\",{\"1\":{\"287\":1}}],[\"框架统一难度较大\",{\"1\":{\"285\":1}}],[\"正好反过来\",{\"1\":{\"285\":1}}],[\"正如在vllm前都是基于hf\",{\"1\":{\"285\":1}}],[\"于是\",{\"1\":{\"285\":1}}],[\"于是通信开销不得不成为主要矛盾\",{\"1\":{\"276\":1}}],[\"迫切需求一个性能好且易用性高的dit推理框架\",{\"1\":{\"285\":1}}],[\"序列增长导致attention\",{\"1\":{\"285\":1}}],[\"序列并行以增加通信量为代价\",{\"1\":{\"267\":1}}],[\"序列并行将沿序列维度\",{\"1\":{\"267\":1}}],[\"序列并行旨在解决此问题\",{\"1\":{\"261\":1}}],[\"序列并行\",{\"0\":{\"143\":1},\"1\":{\"145\":1,\"254\":1}}],[\"异步修正\",{\"1\":{\"284\":1}}],[\"摘要\",{\"0\":{\"282\":1}}],[\"证明了distrifusion方法的泛用性\",{\"1\":{\"281\":1}}],[\"局限\",{\"1\":{\"280\":1}}],[\"局部计算\",{\"1\":{\"262\":1,\"267\":1}}],[\"局部统计\",{\"1\":{\"261\":1}}],[\"^\",{\"1\":{\"279\":1}}],[\"重点是把distrifusion方法移植到了dit模型上\",{\"1\":{\"281\":1}}],[\"重计算\",{\"1\":{\"278\":1}}],[\"重新计算\",{\"1\":{\"95\":1}}],[\"暂存\",{\"1\":{\"278\":1}}],[\"确实有相当一部分激活值完全可以\",{\"1\":{\"278\":1}}],[\"确定重要权重\",{\"1\":{\"268\":1}}],[\"确定开始行\",{\"1\":{\"108\":1}}],[\"确定要计算的行数\",{\"1\":{\"108\":1}}],[\"确定\",{\"1\":{\"99\":1}}],[\"陈旧\",{\"1\":{\"278\":1}}],[\"位移补丁并行\",{\"0\":{\"279\":1}}],[\"位移区块并行\",{\"1\":{\"278\":1}}],[\"位于网格坐标\",{\"1\":{\"262\":1}}],[\"甚至超过计算时间\",{\"1\":{\"278\":1}}],[\"极高的通信开销\",{\"1\":{\"278\":1}}],[\"极大地提升了并行效率\",{\"1\":{\"259\":1}}],[\"极大地减少了内存浪费\",{\"1\":{\"248\":1}}],[\"极大优化了效率\",{\"1\":{\"245\":1}}],[\"区块间同步传递中间激活信息\",{\"1\":{\"278\":1}}],[\"区块间引入交互又会带来过高的同步成本\",{\"1\":{\"275\":1}}],[\"独立计算各个区块并随后拼接\",{\"1\":{\"278\":1}}],[\"独立计算其局部乘积\",{\"1\":{\"262\":1}}],[\"独立计算其注意力输出\",{\"1\":{\"260\":1}}],[\"独立计算其分配到的注意力头的q\",{\"1\":{\"260\":1}}],[\"平均加速在2倍左右\",{\"1\":{\"277\":1}}],[\"难以并行加速\",{\"1\":{\"277\":1}}],[\"难度\",{\"1\":{\"268\":1}}],[\"步数较大的去噪方式\",{\"1\":{\"277\":1}}],[\"步即可使用\",{\"1\":{\"42\":1}}],[\"说利用picard迭代以数据并行的方式并行化去噪步骤\",{\"1\":{\"277\":1}}],[\"十亿次乘加运算\",{\"1\":{\"277\":1}}],[\"处理一张1024x1024的图像时\",{\"1\":{\"277\":1}}],[\"处理多模态任务的价值也不断被挖掘\",{\"1\":{\"272\":1}}],[\"ϵt​=ϵθ​\",{\"1\":{\"277\":1}}],[\"ϵt​\",{\"1\":{\"277\":1}}],[\"ϵθ​\",{\"1\":{\"277\":1}}],[\"例如文本\",{\"1\":{\"277\":1}}],[\"例如从fp16到int8或int4\",{\"1\":{\"250\":1}}],[\"扩散模型通常会训练一个噪声预测神经网络模型\",{\"1\":{\"277\":1}}],[\"扩展代码使其能够使用2\",{\"1\":{\"107\":1}}],[\"倾向于使用allgather而非allreduce进行数据交互\",{\"1\":{\"276\":1}}],[\"预训练的扩散模型中提炼出生成模型\",{\"1\":{\"276\":1}}],[\"预测模型的执行时间\",{\"1\":{\"100\":1}}],[\"相似是compute\",{\"1\":{\"285\":1}}],[\"相关工作\",{\"0\":{\"276\":1}}],[\"相邻gpu对应位置进行通讯\",{\"1\":{\"41\":1}}],[\"边界处会出现明显的接缝和强烈的撕裂感\",{\"1\":{\"278\":1}}],[\"边界处会出现明显的接缝\",{\"1\":{\"275\":1}}],[\"边界情况\",{\"1\":{\"209\":1}}],[\"会导致oom\",{\"1\":{\"294\":1}}],[\"会导致单块gpu利用率降低\",{\"1\":{\"92\":1}}],[\"会生成n个小图拼接而成的大图\",{\"1\":{\"275\":1}}],[\"放在n个device上进行推理\",{\"1\":{\"275\":1}}],[\"常规方法\",{\"1\":{\"275\":1}}],[\"常常超出单张加速卡的承载能力\",{\"1\":{\"241\":1}}],[\"加速效果较好\",{\"1\":{\"274\":1}}],[\"工作属于是diffusion\",{\"1\":{\"273\":1}}],[\"像算子并行\",{\"1\":{\"272\":1}}],[\"像素与像素之间的高度关联性也难以像文本推理一样通过简单的切分矩阵实现并发\",{\"1\":{\"272\":1}}],[\"利用相邻扩散步骤输入之间的高相似性\",{\"1\":{\"282\":1}}],[\"利用前一步扩散过程中稍有过时或\",{\"1\":{\"278\":1}}],[\"利用多个设备上的神经网络并行性来加速扩散过程\",{\"1\":{\"276\":1}}],[\"利用了diffusion过程相邻步之间feature\",{\"1\":{\"274\":1}}],[\"利用diffusion相邻step冗余考虑适当保存activate值\",{\"1\":{\"272\":1}}],[\"利用pipeline\",{\"1\":{\"82\":1}}],[\"避免不必要的重计算\",{\"1\":{\"272\":1}}],[\"避免多轮采样带来的内存开销\",{\"1\":{\"272\":1}}],[\"避免连锁反应\",{\"1\":{\"226\":1}}],[\"除以sj​得到wj\",{\"1\":{\"268\":1}}],[\"除去batchnormalization\",{\"1\":{\"99\":1}}],[\"根据其激活统计量计算出最佳尺度因子sj​\",{\"1\":{\"268\":1}}],[\"根据激活的统计量\",{\"1\":{\"268\":1}}],[\"识别出那些持续接收到高幅度激活值的权重通道\",{\"1\":{\"268\":1}}],[\"α或更优地sj∗​=\",{\"1\":{\"268\":1}}],[\"他们给出了一个启发式的最佳尺度因子计算公式\",{\"1\":{\"268\":1}}],[\"尺度等效变换\",{\"1\":{\"268\":1}}],[\"任何微小的权重误差都会被巨大的激活值放大\",{\"1\":{\"268\":1}}],[\"任务实现\",{\"0\":{\"108\":1}}],[\"任务分析\",{\"0\":{\"107\":1}}],[\"约0\",{\"1\":{\"268\":1}}],[\"约束下\",{\"1\":{\"241\":1}}],[\"约束条件\",{\"0\":{\"193\":1,\"200\":1,\"207\":1}}],[\"∥f2​\",{\"1\":{\"268\":1}}],[\"∥w\",{\"1\":{\"268\":1}}],[\"∥wx−quant\",{\"1\":{\"268\":1}}],[\"∥w−quant\",{\"1\":{\"268\":1}}],[\"∥\",{\"1\":{\"268\":1}}],[\"朴素量化\",{\"1\":{\"268\":1}}],[\"量化的目标变成了最小化\",{\"1\":{\"268\":1}}],[\"量化后的计算为\",{\"1\":{\"268\":1}}],[\"量化误差的显著性\",{\"1\":{\"268\":1}}],[\"σ2=e\",{\"1\":{\"267\":2}}],[\"σ2\",{\"1\":{\"267\":1}}],[\"σi2​\",{\"1\":{\"261\":1}}],[\"∑xi2​\",{\"1\":{\"267\":1}}],[\"∑xi​\",{\"1\":{\"267\":1}}],[\"∑x2=∑i=1p​\",{\"1\":{\"267\":1}}],[\"∑x=∑i=1p​\",{\"1\":{\"267\":1}}],[\"μ=n∑x​\",{\"1\":{\"267\":1}}],[\"μ\",{\"1\":{\"267\":1}}],[\"μi​\",{\"1\":{\"261\":1}}],[\"挑战在于\",{\"1\":{\"267\":1}}],[\"公式简化为\",{\"1\":{\"265\":1}}],[\"公开课\",{\"2\":{\"110\":1,\"141\":1}}],[\"排空延迟\",{\"1\":{\"265\":1}}],[\"排序简化判断\",{\"1\":{\"236\":1}}],[\"排序后的判断\",{\"1\":{\"221\":1}}],[\"启动延迟\",{\"1\":{\"265\":1}}],[\"整个流水线视角\",{\"1\":{\"265\":1}}],[\"整体框架已经在源码中基本完成了\",{\"1\":{\"107\":1}}],[\"填满\",{\"1\":{\"265\":1}}],[\"才统一开始反向传播\",{\"1\":{\"265\":1}}],[\"才能达成最小通信量策略\",{\"1\":{\"76\":1}}],[\"刷新式\",{\"1\":{\"265\":1}}],[\"微批次\",{\"1\":{\"265\":1}}],[\"替代了\",{\"1\":{\"263\":1}}],[\"替换顺序\",{\"1\":{\"227\":1}}],[\"次昂贵的目标模型前向传播\",{\"1\":{\"263\":1}}],[\"次廉价的草稿模型前向传播和一次昂贵的目标模型前向传播\",{\"1\":{\"263\":1}}],[\"次计算\",{\"1\":{\"75\":2}}],[\"−quant\",{\"1\":{\"268\":1}}],[\"−μ2\",{\"1\":{\"267\":1}}],[\"−\",{\"1\":{\"267\":1}}],[\"−pd​\",{\"1\":{\"263\":1}}],[\"−f\",{\"1\":{\"175\":2}}],[\"∝max\",{\"1\":{\"263\":1}}],[\"修正后\",{\"1\":{\"263\":1}}],[\"修正与补全\",{\"1\":{\"263\":1}}],[\"修改一开始的线程分配方式\",{\"1\":{\"107\":1}}],[\"拒绝条件\",{\"1\":{\"263\":1}}],[\"拒绝决策\",{\"1\":{\"263\":1}}],[\"获取它在草稿模型和目标模型中的概率\",{\"1\":{\"263\":1}}],[\"获取嵌入向量\",{\"1\":{\"89\":1}}],[\"逐个进行判断\",{\"1\":{\"263\":1}}],[\"逐步从简单处理二分类或者多分类的简单架构逐步发展到现在拥有对话能力的llm\",{\"1\":{\"173\":1}}],[\"γ\",{\"1\":{\"263\":2}}],[\"γ=\",{\"1\":{\"263\":1}}],[\"⋅∣xprefix​\",{\"1\":{\"263\":3}}],[\"记录每个线性层输入激活x的统计量\",{\"1\":{\"268\":1}}],[\"记录下草稿模型在每一步的输出概率分布\",{\"1\":{\"263\":1}}],[\"记录哪些参数参与计算\",{\"1\":{\"48\":1}}],[\"起草阶段\",{\"1\":{\"263\":1}}],[\"详细流程\",{\"1\":{\"263\":1}}],[\"详细说明拆分步骤\",{\"1\":{\"148\":1}}],[\"速度快得多的模型\",{\"1\":{\"263\":1}}],[\"强大的大模型\",{\"1\":{\"263\":1}}],[\"参与者\",{\"1\":{\"263\":1}}],[\"参数内存占用减少为\",{\"1\":{\"262\":1}}],[\"参数的选取\",{\"1\":{\"91\":1}}],[\"参数都按块切分\",{\"1\":{\"84\":1}}],[\"参数更新逻辑\",{\"1\":{\"78\":1}}],[\"参数从\",{\"1\":{\"78\":1}}],[\"参数\",{\"1\":{\"35\":2,\"55\":1,\"76\":1,\"79\":1}}],[\"机制的计算量也随之呈平方级膨胀\",{\"1\":{\"285\":1}}],[\"机制来打破自回归解码的序列化瓶颈\",{\"1\":{\"263\":1}}],[\"机器学习的基础\",{\"1\":{\"172\":1}}],[\"验证阶段\",{\"1\":{\"263\":1}}],[\"验证\",{\"1\":{\"263\":1}}],[\"验证并解释task2中提出的猜想\",{\"1\":{\"107\":1}}],[\"草稿\",{\"1\":{\"263\":1}}],[\"草稿模型\",{\"1\":{\"251\":1,\"263\":1}}],[\"推测解码通过\",{\"1\":{\"263\":1}}],[\"推理过程到超大规模\",{\"1\":{\"293\":1}}],[\"推理时序列长度很长\",{\"1\":{\"285\":1}}],[\"推理任务特点不同\",{\"1\":{\"285\":1}}],[\"推理延迟极为严重\",{\"1\":{\"285\":1}}],[\"推理服务\",{\"0\":{\"271\":1}}],[\"推理阶段\",{\"1\":{\"268\":1}}],[\"推理阶段的三大核心挑战\",{\"1\":{\"241\":1}}],[\"推理取代训练\",{\"1\":{\"241\":1}}],[\"推理成为新的瓶颈\",{\"1\":{\"241\":1}}],[\"内进行\",{\"1\":{\"262\":1}}],[\"内存复杂度成为根本瓶颈\",{\"1\":{\"254\":1}}],[\"内存复制到\",{\"1\":{\"78\":1}}],[\"内存革命\",{\"1\":{\"248\":1}}],[\"内存墙\",{\"1\":{\"241\":1}}],[\"内存都有重要的影响\",{\"1\":{\"91\":1}}],[\"内存中的fp16参数中\",{\"1\":{\"78\":1}}],[\"内存中\",{\"1\":{\"78\":1,\"79\":2}}],[\"内存碎片通过对不同寿命的内存进行整理\",{\"1\":{\"64\":1}}],[\"内存其实根本无法使用\",{\"1\":{\"58\":1}}],[\"显著\",{\"1\":{\"268\":1}}],[\"显著减少了gpu的空闲时间\",{\"1\":{\"266\":1}}],[\"显著减少模型的内存占用和计算量\",{\"1\":{\"250\":1}}],[\"显存\",{\"1\":{\"262\":1}}],[\"行或列\",{\"1\":{\"262\":1}}],[\"行内广播\",{\"1\":{\"262\":1}}],[\"行并行\",{\"1\":{\"245\":2,\"259\":3}}],[\"沿其列维度被切分为\",{\"1\":{\"262\":1}}],[\"沿其行维度\",{\"1\":{\"262\":1}}],[\"沿序列维度s被切分为p块\",{\"1\":{\"261\":1}}],[\"沿序列维度\",{\"1\":{\"261\":1}}],[\"沿序列\",{\"1\":{\"254\":1}}],[\"且切分方式一致\",{\"1\":{\"293\":1}}],[\"且激活内存并未完全切分\",{\"1\":{\"262\":1}}],[\"且其内置的分布式后端ray与hpc集群的作业管理系统\",{\"1\":{\"256\":1}}],[\"代价是增加了额外的通信开销\",{\"1\":{\"261\":1}}],[\"代码实现没什么好说的\",{\"1\":{\"186\":1}}],[\"代码实现\",{\"0\":{\"186\":1,\"196\":1,\"203\":1,\"210\":1,\"216\":1,\"222\":1,\"228\":1,\"234\":1}}],[\"能正常工作\",{\"1\":{\"261\":1}}],[\"能否在大规模训练时取得想象中的线性效果\",{\"1\":{\"157\":1}}],[\"形成一个新的输入\",{\"1\":{\"263\":1}}],[\"形成一条\",{\"1\":{\"244\":1}}],[\"形状为\",{\"1\":{\"261\":2}}],[\"作者还在这里cue了一下shih等人在2023提出的paradigms\",{\"1\":{\"277\":1}}],[\"作用域\",{\"1\":{\"261\":1}}],[\"作为输入\",{\"1\":{\"100\":1,\"101\":1,\"277\":1}}],[\"激活感知的重要性\",{\"1\":{\"268\":1}}],[\"激活内存和参数内存在2d\",{\"1\":{\"262\":1}}],[\"激活值\",{\"1\":{\"261\":1}}],[\"激活函数\",{\"1\":{\"35\":2,\"89\":1,\"99\":1}}],[\"变化i\",{\"1\":{\"262\":1}}],[\"变得极长时\",{\"1\":{\"261\":1}}],[\"变量保存\",{\"1\":{\"221\":1}}],[\"拼接后的结果再通过输出投影矩阵\",{\"1\":{\"260\":1}}],[\"列内归约\",{\"1\":{\"262\":1}}],[\"列维度\",{\"1\":{\"260\":1}}],[\"列并行\",{\"1\":{\"245\":2,\"259\":3}}],[\"均值计算\",{\"1\":{\"267\":1}}],[\"均按其输出维度\",{\"1\":{\"260\":1}}],[\"均采用fp16半精度运算\",{\"1\":{\"55\":1}}],[\"关键洞察\",{\"1\":{\"259\":1}}],[\"关键的问题是\",{\"1\":{\"98\":1}}],[\"权重矩阵\",{\"1\":{\"262\":1}}],[\"权重切分\",{\"1\":{\"259\":2,\"260\":1}}],[\"权重更新等\",{\"1\":{\"76\":1}}],[\"巧妙地设计计算与通信的顺序\",{\"1\":{\"258\":1}}],[\"结论\",{\"1\":{\"265\":1}}],[\"结果聚合\",{\"1\":{\"259\":1,\"260\":1}}],[\"结果收敛不明显\",{\"1\":{\"85\":1}}],[\"结合awq量化和推测解码等算法\",{\"1\":{\"256\":1}}],[\"服务层并行化与解耦编排\",{\"1\":{\"256\":1}}],[\"架构\",{\"1\":{\"285\":1}}],[\"架构解析\",{\"1\":{\"256\":1}}],[\"架构设计\",{\"1\":{\"256\":1}}],[\"架构模式\",{\"1\":{\"243\":1}}],[\"面临的约束\",{\"1\":{\"256\":1}}],[\"面向特定架构的组件级并行\",{\"1\":{\"240\":1}}],[\"规模化部署基于vllm的deepseek模型\",{\"1\":{\"256\":1}}],[\"规则符合民意的条件是\",{\"1\":{\"219\":1}}],[\"集群上\",{\"1\":{\"256\":1}}],[\"集合通讯库\",{\"0\":{\"49\":1}}],[\"案例背景\",{\"1\":{\"256\":1}}],[\"降低到o\",{\"1\":{\"255\":1}}],[\"超级样本\",{\"1\":{\"255\":1}}],[\"往往处理的是batch\",{\"1\":{\"255\":1}}],[\"维度对张量及计算进行切分\",{\"1\":{\"254\":1}}],[\"维度的拆分\",{\"1\":{\"99\":1}}],[\"宏观流水线\",{\"1\":{\"253\":1}}],[\"构建一个由统一入口\",{\"1\":{\"256\":1}}],[\"构建\",{\"1\":{\"253\":1}}],[\"构建模型\",{\"1\":{\"43\":1}}],[\"文生视频等多模态任务中的分布式推理服务就是在这种背景下被关注的\",{\"1\":{\"272\":1}}],[\"文生视频和文生图任务由于图形任务的整体性对大块内存的使用提出了更为严苛的要求\",{\"1\":{\"272\":1}}],[\"文生视频\",{\"0\":{\"271\":1}}],[\"文生图文生视频等潜力巨大的任务类型开始提上研究日程\",{\"1\":{\"272\":1}}],[\"文生图\",{\"0\":{\"271\":1,\"272\":1},\"1\":{\"253\":1,\"272\":1},\"2\":{\"296\":1}}],[\"文章首先对数据并行\",{\"1\":{\"240\":1}}],[\"文章针对三个challenge提出了三个解决方法\",{\"1\":{\"160\":1}}],[\"文章只对attention和fwd操作做了优化\",{\"1\":{\"157\":1}}],[\"文章的训练测试规模比较小\",{\"1\":{\"157\":1}}],[\"文章将自己的sp流程和pp\",{\"1\":{\"153\":1}}],[\"若草稿频繁被拒\",{\"1\":{\"251\":1}}],[\"若草稿模型预测准确率高且自身速度快\",{\"1\":{\"251\":1}}],[\"反之\",{\"1\":{\"251\":1}}],[\"反向是all\",{\"1\":{\"148\":1}}],[\"反向时不做操作\",{\"1\":{\"147\":1}}],[\"反向时执行all\",{\"1\":{\"147\":1}}],[\"反向传播逻辑\",{\"1\":{\"78\":1}}],[\"反向传播梯度\",{\"1\":{\"78\":1}}],[\"则拒绝\",{\"1\":{\"263\":1}}],[\"则接受\",{\"1\":{\"263\":1}}],[\"则可能因引入额外计算而降低整体速度\",{\"1\":{\"251\":1}}],[\"则能带来显著的性能提升\",{\"1\":{\"251\":1}}],[\"则直接标记为ready\",{\"1\":{\"48\":1}}],[\"性能增益来源\",{\"1\":{\"263\":1}}],[\"性能模型分析\",{\"1\":{\"251\":1}}],[\"性能分析\",{\"0\":{\"95\":1}}],[\"精度较高\",{\"1\":{\"250\":1}}],[\"优势分析\",{\"1\":{\"262\":1}}],[\"优势\",{\"1\":{\"248\":1}}],[\"优化扩散模型的神经推理\",{\"1\":{\"276\":1}}],[\"优化器计算要求cpu进行o\",{\"1\":{\"75\":1}}],[\"优化器状态和梯度\",{\"1\":{\"78\":1}}],[\"优化器状态\",{\"1\":{\"35\":1,\"79\":1}}],[\"优化\",{\"1\":{\"74\":1}}],[\"立刻将其从批次中移除并返回结果\",{\"1\":{\"248\":1}}],[\"连续批处理则将请求处理看作一个连续的流\",{\"1\":{\"248\":1}}],[\"连续批处理\",{\"1\":{\"248\":1}}],[\"调度策略\",{\"1\":{\"266\":1}}],[\"调度革命\",{\"1\":{\"248\":1}}],[\"调用mandelbrotserial\",{\"1\":{\"108\":1}}],[\"调用自定义函数\",{\"1\":{\"48\":1}}],[\"节点间\",{\"1\":{\"247\":1}}],[\"节点内\",{\"1\":{\"247\":1}}],[\"充分利用节点内gpu之间的高速nvlink总线\",{\"1\":{\"247\":1}}],[\"策略\",{\"1\":{\"247\":1}}],[\"设计目标是扩展\",{\"1\":{\"293\":1}}],[\"设计无需训练的高效采样算法\",{\"1\":{\"276\":1}}],[\"设计哲学\",{\"1\":{\"247\":1}}],[\"设计由n个输入序列在p个可用设备上分区组成\",{\"1\":{\"151\":1}}],[\"存在扩展性边界\",{\"1\":{\"245\":1}}],[\"存储在物理上非连续的内存块\",{\"1\":{\"248\":1}}],[\"存储持续整个训练过程\",{\"1\":{\"79\":1}}],[\"存储momentum和variance\",{\"1\":{\"55\":1}}],[\"存储模型的参数和输入\",{\"1\":{\"55\":1}}],[\"理论局限\",{\"1\":{\"245\":1}}],[\"理论上gpu数量等于batch\",{\"1\":{\"92\":1}}],[\"头\",{\"1\":{\"245\":1}}],[\"基于扩散模型与微分方程之间的联系\",{\"1\":{\"276\":1}}],[\"基于矩阵乘法的结合律\",{\"1\":{\"245\":1}}],[\"基础操作还有进一步优化的空间\",{\"1\":{\"157\":1}}],[\"水平切分至多个计算单元\",{\"1\":{\"245\":1}}],[\"负载均衡\",{\"1\":{\"244\":1}}],[\"负责会话初始化\",{\"1\":{\"43\":1}}],[\"延迟\",{\"1\":{\"244\":1}}],[\"延迟一步的参数更新\",{\"1\":{\"75\":1}}],[\"开发者通过一些\",{\"1\":{\"285\":1}}],[\"开始\",{\"1\":{\"277\":1}}],[\"开始行和总计算行数\",{\"1\":{\"108\":1}}],[\"开销\",{\"1\":{\"244\":1}}],[\"气泡大小可以被有效减小\",{\"1\":{\"266\":1}}],[\"气泡大小的理论占比为\",{\"1\":{\"244\":1}}],[\"气泡开销与阶段数p成正比\",{\"1\":{\"265\":1}}],[\"气泡时间\",{\"1\":{\"265\":1}}],[\"气泡\",{\"1\":{\"244\":1}}],[\"产生\",{\"1\":{\"244\":1}}],[\"产生部分输出\",{\"1\":{\"35\":1}}],[\"流水线\",{\"1\":{\"265\":1}}],[\"流水线阶段\",{\"1\":{\"265\":1}}],[\"流水线气泡占比\",{\"1\":{\"265\":1}}],[\"流水线气泡\",{\"1\":{\"244\":1,\"264\":1}}],[\"流水线并行的核心挑战在于最小化\",{\"1\":{\"264\":1}}],[\"流水线并行技术将一个模型\",{\"1\":{\"93\":1}}],[\"流水线并行中的schedule\",{\"1\":{\"91\":1}}],[\"流水线并行\",{\"0\":{\"80\":1},\"1\":{\"53\":1,\"240\":1}}],[\"让多个微批次\",{\"1\":{\"244\":1}}],[\"让每一个卡分批成环去跑一小批token\",{\"1\":{\"157\":1}}],[\"让每一个gpu仅保留几个隐藏层的模型\",{\"1\":{\"35\":1}}],[\"让每一张卡只保留必须的参数和算子\",{\"1\":{\"35\":1}}],[\"它仅在新鲜区域上执行计算并产生相应的输出\",{\"1\":{\"279\":1}}],[\"它将变换等效地应用于激活和权重\",{\"1\":{\"268\":1}}],[\"它将m个微批次划分为更小的块\",{\"1\":{\"266\":1}}],[\"它将模型的计算图\",{\"1\":{\"244\":1}}],[\"它可以直接在每个gpu上本地执行\",{\"1\":{\"259\":1}}],[\"它利用一个轻量级的\",{\"1\":{\"251\":1}}],[\"它借鉴了操作系统中虚拟内存和分页的思想\",{\"1\":{\"248\":1}}],[\"它对已分割的查询\",{\"1\":{\"150\":1}}],[\"造成gpu计算资源闲置\",{\"1\":{\"243\":1}}],[\"部分设备会处于空闲状态\",{\"1\":{\"244\":1}}],[\"部分sp方法\",{\"1\":{\"159\":1}}],[\"部署成本随副本数量线性增长\",{\"1\":{\"243\":1}}],[\"成效不佳\",{\"1\":{\"276\":1}}],[\"成功地将巨大的激活内存分散到了多个设备上\",{\"1\":{\"267\":1}}],[\"成正比\",{\"1\":{\"261\":1,\"262\":1}}],[\"成本\",{\"1\":{\"243\":1}}],[\"成为ai应用的主要成本中心和性能瓶颈\",{\"1\":{\"241\":1}}],[\"应用尺度并量化\",{\"1\":{\"268\":1}}],[\"应用与理论局限\",{\"1\":{\"243\":1,\"244\":1}}],[\"应该与该通道激活值的幅度成正比\",{\"1\":{\"268\":1}}],[\"应对超长上下文挑战的序列并行\",{\"1\":{\"240\":1}}],[\"定期聚合梯度以确保模型权重的一致性\",{\"1\":{\"243\":1}}],[\"定义网络拓扑关系\",{\"1\":{\"40\":1}}],[\"各设备独立计算\",{\"1\":{\"245\":2}}],[\"各阶段计算量的不均衡是实践中的关键挑战\",{\"1\":{\"244\":1}}],[\"各副本独立完成计算后\",{\"1\":{\"243\":1}}],[\"各个节点之间通过网络进行通讯的计算机系统\",{\"1\":{\"32\":1}}],[\"核心创新点之一\",{\"1\":{\"293\":1}}],[\"核心\",{\"1\":{\"278\":1}}],[\"核心收益在于用\",{\"1\":{\"263\":1}}],[\"核心思路\",{\"1\":{\"256\":1}}],[\"核心思想\",{\"1\":{\"243\":1,\"244\":1,\"245\":1,\"260\":1,\"261\":1,\"262\":1}}],[\"核心问题一直是优化问题\",{\"1\":{\"173\":1}}],[\"展示多维并行策略的综合应用\",{\"1\":{\"241\":1}}],[\"展示了如何在现实约束下进行系统架构设计\",{\"1\":{\"240\":1}}],[\"吞吐墙\",{\"1\":{\"241\":1}}],[\"吞吐量提升显著\",{\"1\":{\"248\":1}}],[\"吞吐量\",{\"1\":{\"95\":1}}],[\"带来的序列化计算\",{\"1\":{\"241\":1}}],[\"后训练时代\",{\"1\":{\"241\":1}}],[\"后向传播的顺序与梯度更新的顺序大致可认为是相同的\",{\"1\":{\"48\":1}}],[\"深入分析关键系统优化与算法优化的内在机理\",{\"1\":{\"241\":1}}],[\"深入探讨了以连续批处理\",{\"1\":{\"240\":1}}],[\"深度学习模型在训练万亿级别参数时存在根本性限制\",{\"1\":{\"52\":1}}],[\"等于并行的设备数\",{\"1\":{\"265\":1}}],[\"等前沿算法优化\",{\"1\":{\"240\":1}}],[\"等基础概念进行界定\",{\"1\":{\"240\":1}}],[\"等多重挑战\",{\"1\":{\"240\":1}}],[\"等等需要保存的东西\",{\"1\":{\"35\":1}}],[\"阶段的部署面临着内存墙\",{\"1\":{\"240\":1}}],[\"阶乘的位数估算\",{\"0\":{\"182\":1}}],[\"状态映射代替直接修改\",{\"1\":{\"236\":1}}],[\"善用数学性质简化问题\",{\"1\":{\"236\":1}}],[\"哈希统计\",{\"1\":{\"236\":1}}],[\"哈希数组大小\",{\"1\":{\"215\":1}}],[\"映射数组初始化\",{\"1\":{\"233\":1}}],[\"遍历整个字符串进行替换\",{\"1\":{\"226\":1}}],[\"读入初始字符串\",{\"1\":{\"226\":1}}],[\"读取输入整数n\",{\"1\":{\"194\":1}}],[\"过小的局部批量可能导致通信开销占比过大\",{\"1\":{\"243\":1}}],[\"过半数计算\",{\"1\":{\"221\":1}}],[\"过程中\",{\"1\":{\"78\":1,\"79\":1}}],[\"统计符合条件的天数是否过半\",{\"1\":{\"220\":1}}],[\"投票统计\",{\"0\":{\"218\":1}}],[\"测试用例处理\",{\"1\":{\"215\":1}}],[\"补充计算\",{\"1\":{\"215\":1}}],[\"累加所有需要补充的字符数\",{\"1\":{\"214\":1}}],[\"没有考虑输入为单个数字的情况\",{\"1\":{\"209\":1}}],[\"减去\",{\"1\":{\"209\":1}}],[\"减少训练负担\",{\"1\":{\"292\":1}}],[\"减少通信成本\",{\"1\":{\"284\":1}}],[\"减少采样步数\",{\"1\":{\"272\":1}}],[\"减少计算时内存占用\",{\"1\":{\"86\":1}}],[\"减少内存释放和分配的时间\",{\"1\":{\"64\":1}}],[\"减少内存使用\",{\"1\":{\"62\":1}}],[\"忘记处理多组测试用例的情况\",{\"1\":{\"215\":1}}],[\"忘记将字符转换为数字\",{\"1\":{\"209\":1}}],[\"忘记了估计数据规模\",{\"1\":{\"184\":1}}],[\"字符串处理\",{\"1\":{\"236\":1}}],[\"字符串修改\",{\"1\":{\"227\":1}}],[\"字符串比较前的转换\",{\"1\":{\"202\":1}}],[\"字符替换\",{\"0\":{\"224\":1}}],[\"字符映射\",{\"1\":{\"215\":1}}],[\"字符统计问题\",{\"0\":{\"212\":1}}],[\"字符转数字\",{\"1\":{\"209\":1}}],[\"试图将输入转换为整数类型会导致溢出\",{\"1\":{\"209\":1}}],[\"无需训练\",{\"1\":{\"274\":1}}],[\"无需通信\",{\"1\":{\"245\":1}}],[\"无需进行完整的大数运算\",{\"1\":{\"208\":1}}],[\"无法解决单模型瓶颈\",{\"1\":{\"243\":1}}],[\"无论哪一种模型\",{\"1\":{\"35\":1}}],[\"无论使用哪一种模型\",{\"1\":{\"33\":1}}],[\"无论在cv方向还是nlp等其他方向\",{\"1\":{\"33\":1}}],[\"奇偶判断\",{\"0\":{\"205\":1}}],[\"返回值处理\",{\"1\":{\"202\":1}}],[\"时\",{\"1\":{\"279\":1}}],[\"时必须全部小写\",{\"1\":{\"209\":1}}],[\"时必须全部大写\",{\"1\":{\"202\":1}}],[\"时间复杂度是梦寐以求的o\",{\"1\":{\"185\":1}}],[\"时间复杂度\",{\"1\":{\"184\":1,\"197\":1,\"204\":1,\"211\":1,\"217\":1,\"223\":1,\"229\":1,\"235\":1}}],[\"或者\",{\"1\":{\"278\":1}}],[\"或者考虑让cpu也存储一部分模型数据\",{\"1\":{\"33\":1}}],[\"或近似为l2范数\",{\"1\":{\"268\":1}}],[\"或激活值的数值精度\",{\"1\":{\"250\":1}}],[\"或\",{\"1\":{\"202\":1,\"209\":1,\"262\":1}}],[\"虽然现在长文本需求也在增加\",{\"1\":{\"285\":1}}],[\"虽然需要两次通信\",{\"1\":{\"262\":1}}],[\"虽然题目保证输入合法\",{\"1\":{\"202\":1}}],[\"虽然亚伯拉罕\",{\"1\":{\"185\":1}}],[\"比较\",{\"1\":{\"201\":1}}],[\"比如\",{\"1\":{\"99\":1}}],[\"判断是否过半\",{\"1\":{\"220\":1}}],[\"判断规则k是否符合民意\",{\"1\":{\"219\":1}}],[\"判断每个数的奇偶性\",{\"1\":{\"206\":1}}],[\"判断该字符串是否等于\",{\"1\":{\"199\":1}}],[\"判断当前进程是否拥有第\",{\"1\":{\"78\":1}}],[\"复杂度分析\",{\"0\":{\"197\":1,\"204\":1,\"211\":1,\"217\":1,\"223\":1,\"229\":1,\"235\":1}}],[\"复平面左上和右下两个点坐标\",{\"1\":{\"108\":1}}],[\"容易多输出或少输出一个\",{\"1\":{\"195\":1}}],[\"循环次数控制\",{\"1\":{\"195\":1}}],[\"循环n次输出字符\",{\"1\":{\"194\":1}}],[\"易错点\",{\"0\":{\"195\":1,\"202\":1,\"209\":1,\"215\":1,\"221\":1,\"227\":1,\"233\":1}}],[\"≤\",{\"1\":{\"193\":2,\"200\":2,\"207\":2}}],[\"按其行维度切分为\",{\"1\":{\"259\":1}}],[\"按其列维度切分为\",{\"1\":{\"259\":1}}],[\"按层\",{\"1\":{\"244\":1}}],[\"按顺序组成\",{\"1\":{\"192\":1}}],[\"按照ring拓扑结构进行计算也会有约一半的cpu处于等待状态\",{\"1\":{\"161\":1}}],[\"该模型需要经过数十到数百次的迭代去噪步骤\",{\"1\":{\"277\":1}}],[\"该比例近似为\",{\"1\":{\"265\":1}}],[\"该修正分布为\",{\"1\":{\"263\":1}}],[\"该架构中\",{\"1\":{\"256\":1}}],[\"该技术旨在打破自回归解码\",{\"1\":{\"251\":1}}],[\"该技术是vllm等现代推理框架的核心\",{\"1\":{\"248\":1}}],[\"该方法可能不适用\",{\"1\":{\"280\":1}}],[\"该方法观察到\",{\"1\":{\"250\":1}}],[\"该方法在主机设备之间构建self\",{\"1\":{\"157\":1}}],[\"该函数可独立在各设备上计算\",{\"1\":{\"245\":1}}],[\"该字符串由一个\",{\"1\":{\"192\":1}}],[\"给定当前已生成的序列\",{\"1\":{\"263\":1}}],[\"给定n个人在m天内对规则k的投票情况\",{\"1\":{\"219\":1}}],[\"给定n个大整数\",{\"1\":{\"206\":1}}],[\"给定一个n×n的矩阵和m次操作\",{\"1\":{\"231\":1}}],[\"给定一个字符串s和q次操作\",{\"1\":{\"225\":1}}],[\"给定一个字符串和目标次数m\",{\"1\":{\"213\":1}}],[\"给定一个正整数n\",{\"1\":{\"192\":1}}],[\"给定t个测试用例\",{\"1\":{\"199\":1}}],[\"给不同的gpu做计算的方式\",{\"1\":{\"35\":1}}],[\"龙字符串\",{\"0\":{\"191\":1}}],[\"套公式罢了\",{\"1\":{\"186\":1}}],[\"外层中括号意为高斯取整\",{\"1\":{\"185\":1}}],[\"棣美弗早于斯特林提出了一个类似的公式\",{\"1\":{\"185\":1}}],[\"斯特林的名字命名\",{\"1\":{\"185\":1}}],[\"斯特林公式估算的误差已经降到了十万分之一以下\",{\"1\":{\"185\":1}}],[\"斯特林公式的取值已经十分准确\",{\"1\":{\"185\":1}}],[\"斯特林公式\",{\"0\":{\"185\":1}}],[\"≈2πn​\",{\"1\":{\"185\":1}}],[\"所有设备都在忙碌的时间\",{\"1\":{\"265\":1}}],[\"所有m个微批次完成前向传播后\",{\"1\":{\"265\":1}}],[\"所有gpu的注意力头输出\",{\"1\":{\"260\":1}}],[\"所要存储的数据规模\",{\"1\":{\"184\":1}}],[\"所以ans也是为了省事儿用的int偷懒是可耻的\",{\"1\":{\"186\":1}}],[\"所以斯特林公式十分好用\",{\"1\":{\"185\":1}}],[\"所以我们一般需要人为设置训练轮次来恰当的结束训练\",{\"1\":{\"174\":1}}],[\"所以可以在sequence维度上进行拆分\",{\"1\":{\"148\":1}}],[\"所以可以给出函数workerthreadstart\",{\"1\":{\"108\":1}}],[\"所以难度并不算大~~\",{\"1\":{\"107\":1}}],[\"所以一种可取的方法是将一部中间过程结果丢弃\",{\"1\":{\"95\":1}}],[\"所以此时应采用fp32的全精度计算\",{\"1\":{\"55\":1}}],[\"所以在训练ai时我们希望能使用尽可能大的数据集\",{\"1\":{\"33\":1}}],[\"程序代码的复杂度和\",{\"1\":{\"184\":1}}],[\"随着序列长度的增加\",{\"1\":{\"283\":1}}],[\"随着分辨率的增加\",{\"1\":{\"277\":1}}],[\"随着transformer在文本生成方面逐步展现出的巨大潜力和chatgpt等生成式对话ai的逐步商业化\",{\"1\":{\"272\":1}}],[\"随着模型即服务\",{\"1\":{\"241\":1}}],[\"随着模型参数规模迈入万亿级别\",{\"1\":{\"240\":1}}],[\"随着n的增大\",{\"1\":{\"185\":1}}],[\"随便算算就存爆了\",{\"1\":{\"184\":1}}],[\"随机选择\",{\"1\":{\"101\":1}}],[\"完全无法满足题目要求\",{\"1\":{\"184\":1}}],[\"完成后可以通过make\",{\"1\":{\"107\":1}}],[\"完成一次周期内所有micro\",{\"1\":{\"93\":1}}],[\"举个栗子\",{\"1\":{\"184\":1}}],[\"更鲁棒的做法是使用校准数据集\",{\"1\":{\"268\":1}}],[\"更适合量化\",{\"1\":{\"268\":1}}],[\"更取决于与之相乘的激活值的大小\",{\"1\":{\"268\":1}}],[\"更强大的\",{\"1\":{\"251\":1}}],[\"更不可能过测试了\",{\"1\":{\"184\":1}}],[\"更新完成后\",{\"1\":{\"79\":1}}],[\"更新参数\",{\"1\":{\"78\":1}}],[\"更新模型参数\",{\"1\":{\"37\":1}}],[\"题目7\",{\"1\":{\"236\":2}}],[\"题目5\",{\"1\":{\"236\":2}}],[\"题目4\",{\"1\":{\"236\":2}}],[\"题目3\",{\"1\":{\"236\":2}}],[\"题目1\",{\"1\":{\"236\":1}}],[\"题目描述\",{\"0\":{\"192\":1,\"199\":1,\"206\":1,\"213\":1,\"219\":1,\"225\":1,\"231\":1}}],[\"题目分析\",{\"0\":{\"184\":1}}],[\"题目引入\",{\"0\":{\"183\":1}}],[\"快写个程序帮助他\",{\"1\":{\"183\":1}}],[\"快速\",{\"1\":{\"98\":1}}],[\"持有\",{\"1\":{\"262\":1}}],[\"持有其对应注意力头的权重分片\",{\"1\":{\"260\":1}}],[\"持\",{\"1\":{\"183\":1}}],[\"矜\",{\"1\":{\"183\":1}}],[\"男孩离开前告诉弯通\",{\"1\":{\"183\":1}}],[\"弯通又做梦了\",{\"1\":{\"183\":1}}],[\"自回归解码\",{\"1\":{\"241\":1}}],[\"自身替换\",{\"1\":{\"227\":1}}],[\"自己实现的算子\",{\"0\":{\"178\":1}}],[\"自动微分\",{\"1\":{\"172\":1}}],[\"自动查找高效的\",{\"1\":{\"101\":1}}],[\"自动并行\",{\"0\":{\"96\":1}}],[\"课程已实现的算子\",{\"0\":{\"177\":1}}],[\"尊重课程要求\",{\"1\":{\"176\":1}}],[\"θ\",{\"1\":{\"175\":1}}],[\"θ+h⋅ei​\",{\"1\":{\"175\":1}}],[\"∂θi​∂f​≈hf\",{\"1\":{\"175\":1}}],[\"经过此步\",{\"1\":{\"262\":1}}],[\"经过gelu激活函数\",{\"1\":{\"245\":1}}],[\"经过sp优化后可以在单a100上运行了\",{\"1\":{\"145\":1}}],[\"经典的极限定义可以帮我们很好的理解这一点\",{\"1\":{\"175\":1}}],[\"数组初始化\",{\"1\":{\"221\":1}}],[\"数学原理\",{\"1\":{\"245\":1}}],[\"数学\",{\"2\":{\"189\":1}}],[\"数学在计算机算法研究中的作用\",{\"0\":{\"182\":1}}],[\"数即一切\",{\"1\":{\"187\":1}}],[\"数值微分是一种极为朴实无华的实现\",{\"1\":{\"175\":1}}],[\"数值微分\",{\"0\":{\"175\":1}}],[\"数据布局\",{\"1\":{\"262\":1}}],[\"数据结构与算法\",{\"2\":{\"188\":1}}],[\"数据并行失效\",{\"1\":{\"255\":1}}],[\"数据并行d\",{\"1\":{\"95\":1}}],[\"数据并行内存使用并不高效\",{\"1\":{\"60\":1}}],[\"数据并行比模型并行更好\",{\"1\":{\"60\":1}}],[\"数据并行\",{\"0\":{\"36\":1,\"92\":1},\"1\":{\"52\":1,\"98\":1}}],[\"数据分布式的一些基本想法\",{\"0\":{\"35\":1}}],[\"数据集越大\",{\"1\":{\"33\":1}}],[\"直到模型达到最佳性能\",{\"1\":{\"174\":1}}],[\"直接处理最后部分\",{\"1\":{\"108\":1}}],[\"清除所有梯度\",{\"1\":{\"174\":1}}],[\"朝着loss最小化的方向调整模型参数\",{\"1\":{\"174\":1}}],[\"求梯度\",{\"1\":{\"174\":1}}],[\"神经网络运用了广泛的数学理论\",{\"1\":{\"173\":1}}],[\"线性\",{\"1\":{\"279\":1}}],[\"线性代数\",{\"1\":{\"172\":1}}],[\"线性映射层\",{\"1\":{\"146\":1}}],[\"同步全局的∑x和∑x2\",{\"1\":{\"267\":1}}],[\"同一列内的所有gpu拥有相同的\",{\"1\":{\"262\":1}}],[\"同一行内的所有gpu拥有相同的\",{\"1\":{\"262\":1}}],[\"同reference\",{\"1\":{\"172\":1}}],[\"同时复用前一步骤的预计算激活值\",{\"1\":{\"280\":1}}],[\"同时本文还指出\",{\"1\":{\"277\":1}}],[\"同时动态地将新请求加入批次\",{\"1\":{\"248\":1}}],[\"同时从前一个设备接收key\",{\"1\":{\"157\":1}}],[\"同时从前一个主机接收key\",{\"1\":{\"157\":1}}],[\"同时重新在序列维度上进行分区\",{\"1\":{\"150\":1}}],[\"同时\",{\"1\":{\"101\":1}}],[\"同时输入\",{\"1\":{\"99\":1}}],[\"同时确保准确性\",{\"1\":{\"75\":1}}],[\"同时最大限度地节省\",{\"1\":{\"74\":1}}],[\"同时又有12ψ个byte\",{\"1\":{\"55\":1}}],[\"同时保持高效率\",{\"1\":{\"52\":1}}],[\"同时保持了低通信量和高计算粒度\",{\"1\":{\"52\":1}}],[\"同时对分布式开发\",{\"1\":{\"0\":1}}],[\"希望读者能自行实现automatic\",{\"1\":{\"172\":1}}],[\"伪代码的形式实现\",{\"1\":{\"172\":1}}],[\"涉及到课程具体代码实现的部分将以注释\",{\"1\":{\"172\":1}}],[\"目前主要只使用数据并行\",{\"1\":{\"276\":1}}],[\"目前优化主要集中在以下几点\",{\"1\":{\"276\":1}}],[\"目前考虑到的针对diffusion的优化主要集中于以下几个方面\",{\"1\":{\"272\":1}}],[\"目标模型\",{\"1\":{\"251\":1,\"263\":1}}],[\"目标是把红色块的数据广播到其余gpu对应的位置上\",{\"1\":{\"40\":1}}],[\"目的是通过一步步迭代来提升模型的性能\",{\"1\":{\"173\":1}}],[\"目的主要是回顾machine\",{\"1\":{\"172\":1}}],[\"写在开始之前\",{\"0\":{\"172\":1}}],[\"系统性构建推理并行技术的分类体系\",{\"1\":{\"241\":1}}],[\"系统\",{\"2\":{\"170\":1}}],[\"导致gpu增加时内存开销增加\",{\"1\":{\"283\":1}}],[\"导致gpu在等待最慢请求时空闲\",{\"1\":{\"248\":1}}],[\"导致生成单张高分辨率图像的延迟在实际应用中变得过高\",{\"1\":{\"277\":1}}],[\"导致部分设备处于闲置状态的时间\",{\"1\":{\"264\":1}}],[\"导致相对高的还原计算代价\",{\"1\":{\"163\":1}}],[\"导致模型训练失效\",{\"1\":{\"55\":1}}],[\"针对扩散模型后处理的解码\",{\"1\":{\"288\":1}}],[\"针对性解决了\",{\"1\":{\"285\":1}}],[\"针对llm的并行方法特点是\",{\"1\":{\"276\":1}}],[\"针对这个问题\",{\"1\":{\"161\":1}}],[\"针对mlp\",{\"1\":{\"89\":1}}],[\"解码\",{\"1\":{\"294\":1}}],[\"解码器模块\",{\"1\":{\"288\":1}}],[\"解码过程就向前跳跃了多个步骤\",{\"1\":{\"251\":1}}],[\"解决方案\",{\"1\":{\"253\":1,\"254\":1}}],[\"解决了kv缓存的内存碎片问题\",{\"1\":{\"248\":1}}],[\"解决了高校attention实现\",{\"1\":{\"160\":1}}],[\"解题要点\",{\"1\":{\"201\":1}}],[\"解题步骤如下\",{\"1\":{\"194\":1}}],[\"解题思路\",{\"0\":{\"194\":1,\"201\":1,\"208\":1,\"214\":1,\"220\":1,\"226\":1,\"232\":1}}],[\"解压包\",{\"1\":{\"106\":1}}],[\"好吧也需要堆卡出奇迹\",{\"1\":{\"157\":1}}],[\"天才般的想法\",{\"1\":{\"157\":1}}],[\"环注意\",{\"1\":{\"157\":1}}],[\"环境配置完成后就可以clone\",{\"1\":{\"106\":1}}],[\"环境配置\",{\"0\":{\"106\":1}}],[\"老生常谈的transformer长序列要求\",{\"1\":{\"156\":1}}],[\"感觉想法来源于ring\",{\"1\":{\"154\":1}}],[\"主干网络混合并行\",{\"0\":{\"287\":1}}],[\"主流方法评述\",{\"1\":{\"250\":1}}],[\"主要作用于layernorm\",{\"1\":{\"261\":1}}],[\"主要是矩阵乘法\",{\"1\":{\"258\":1}}],[\"主要思路如下\",{\"1\":{\"214\":1}}],[\"主要考察字符串的遍历和替换操作\",{\"1\":{\"226\":1}}],[\"主要考察字符串处理和大小写转换\",{\"1\":{\"201\":1}}],[\"主要考察基础的字符串操作和循环控制\",{\"1\":{\"194\":1}}],[\"主要工作\",{\"0\":{\"154\":1}}],[\"主攻cv\",{\"1\":{\"0\":1}}],[\"用巨量的计算换取极高的生成能力\",{\"1\":{\"276\":1}}],[\"用精度换效率\",{\"1\":{\"275\":1}}],[\"用于将巨大的模型本身进行切分\",{\"1\":{\"255\":1}}],[\"用于实现多变量函数f中参数向量θ中单个参数θi​的偏导数\",{\"1\":{\"175\":1}}],[\"用于transformer模型层的剩余模块中的后续操作\",{\"1\":{\"151\":1}}],[\"用commnication换取序列长度\",{\"1\":{\"154\":1}}],[\"用pytorch等使用虚拟内存分配方式的库时\",{\"1\":{\"58\":1}}],[\"注意力机制中与序列长度n相关的o\",{\"1\":{\"254\":1}}],[\"注意力计算后\",{\"1\":{\"151\":1}}],[\"注意替换操作要同时进行\",{\"1\":{\"226\":1}}],[\"注意处理大小写不敏感的比较\",{\"1\":{\"201\":1}}],[\"注意与上张图不同\",{\"1\":{\"84\":1}}],[\"收集\",{\"1\":{\"151\":1}}],[\"接下来\",{\"1\":{\"151\":1}}],[\"接受条件\",{\"1\":{\"263\":1}}],[\"接受\",{\"1\":{\"100\":1,\"101\":1,\"263\":1}}],[\"键\",{\"1\":{\"150\":1,\"151\":1}}],[\"需求大token和长sequence的输入\",{\"1\":{\"150\":1}}],[\"需要同时进行\",{\"1\":{\"292\":1}}],[\"需要跨多个计算设备并行化dit推理\",{\"1\":{\"283\":1}}],[\"需要从一个\",{\"1\":{\"263\":1}}],[\"需要在设备网格的每一列内进行一次\",{\"1\":{\"262\":1}}],[\"需要被广播给该行的所有gpu\",{\"1\":{\"262\":1}}],[\"需要将沿序列维度切分的激活重新拼接成完整的张量\",{\"1\":{\"261\":1}}],[\"需要一次跨所有gpu的\",{\"1\":{\"261\":1}}],[\"需要换行\",{\"1\":{\"233\":1}}],[\"需要减1后再使用\",{\"1\":{\"233\":1}}],[\"需要保存原始的m值用于最后判断\",{\"1\":{\"221\":1}}],[\"需要判断right\",{\"1\":{\"221\":1}}],[\"需要考虑奇数情况\",{\"1\":{\"221\":1}}],[\"需要两层判断\",{\"1\":{\"220\":1}}],[\"需要计算使每个字符出现次数达到m次所需添加的最少字符数\",{\"1\":{\"213\":1}}],[\"需要通信汇总不同worker上的attention结果\",{\"1\":{\"162\":1}}],[\"需要通过同步来保障语义对等\",{\"1\":{\"89\":1}}],[\"需要缓存下来\",{\"1\":{\"146\":1}}],[\"需要分别创建保存q和k的矩阵\",{\"1\":{\"146\":1}}],[\"需要综合流水线并行p\",{\"1\":{\"95\":1}}],[\"需要额外的12ψ\",{\"1\":{\"55\":1}}],[\"长上下文的保存有助于llm推理\",{\"1\":{\"150\":1}}],[\"长序列在llm应用中非常重要\",{\"1\":{\"150\":1}}],[\"链接在这儿\",{\"1\":{\"149\":1}}],[\"官号本身讲的也非常不错\",{\"1\":{\"149\":1}}],[\"先是一个reduce\",{\"1\":{\"148\":1}}],[\"具体实现\",{\"1\":{\"157\":1}}],[\"具体实现图例见下\",{\"1\":{\"146\":1}}],[\"具体过程见下\",{\"1\":{\"148\":1}}],[\"得到x\",{\"1\":{\"268\":1}}],[\"得到quant\",{\"1\":{\"268\":1}}],[\"得到全局平方和\",{\"1\":{\"267\":1}}],[\"得到全局总和\",{\"1\":{\"267\":1}}],[\"得到最终的mha层输出\",{\"1\":{\"260\":1}}],[\"得到最终结果\",{\"1\":{\"245\":1}}],[\"得到最终输出\",{\"1\":{\"245\":1}}],[\"得到w1​和w2​后进行reduce\",{\"1\":{\"148\":1}}],[\"得到结果z1h​b1r​和z2h​b2r​\",{\"1\":{\"148\":1}}],[\"得到结果ya1c​​和ya2c​\",{\"1\":{\"148\":1}}],[\"得到mandelbrotserial\",{\"1\":{\"108\":1}}],[\"​被重新分散回前一步的陈旧激活atl​\",{\"1\":{\"279\":1}}],[\"​之后\",{\"1\":{\"279\":1}}],[\"​=wj​\",{\"1\":{\"268\":1}}],[\"​=g\",{\"1\":{\"148\":1}}],[\"​∣2\",{\"1\":{\"268\":1}}],[\"​⋅γ+β\",{\"1\":{\"267\":1}}],[\"​+o\",{\"1\":{\"175\":1}}],[\"​​​=layernorm\",{\"1\":{\"148\":1}}],[\"​w1​\",{\"1\":{\"148\":1}}],[\"​\",{\"1\":{\"148\":4,\"175\":1,\"265\":2,\"279\":6}}],[\"拆分后如下图\",{\"1\":{\"148\":1}}],[\"拆分到多个\",{\"1\":{\"98\":1}}],[\"拆分到\",{\"1\":{\"98\":1}}],[\"模块在高清图片生成时\",{\"1\":{\"294\":1}}],[\"模块\",{\"1\":{\"288\":1}}],[\"模块总的大小为\",{\"1\":{\"146\":1}}],[\"模型更改难度高的问题\",{\"1\":{\"285\":1}}],[\"模型大多收敛到微调\",{\"1\":{\"285\":1}}],[\"模型不大\",{\"1\":{\"285\":1}}],[\"模型很大\",{\"1\":{\"285\":1}}],[\"模型ϵθ​会将xt​\",{\"1\":{\"277\":1}}],[\"模型尺寸较小\",{\"1\":{\"276\":1}}],[\"模型压缩等等\",{\"1\":{\"272\":1}}],[\"模型中只有一小部分\",{\"1\":{\"268\":1}}],[\"模型权重和激活张量同时沿两个维度进行切分\",{\"1\":{\"262\":1}}],[\"模型权重与动态kv缓存的巨大内存占用\",{\"1\":{\"241\":1}}],[\"模型内部并行\",{\"1\":{\"255\":1}}],[\"模型如stable\",{\"1\":{\"253\":1}}],[\"模型进行复制\",{\"1\":{\"247\":1}}],[\"模型有\",{\"1\":{\"84\":1}}],[\"模型状态\",{\"1\":{\"76\":1}}],[\"模型并行\",{\"1\":{\"52\":1,\"98\":1}}],[\"模型绕过mp方法按层切分时可能带来的通讯瓶颈\",{\"1\":{\"35\":1}}],[\"模型\",{\"1\":{\"35\":1}}],[\"元素个数都是as2b个\",{\"1\":{\"146\":1}}],[\"元素个数为sbh个\",{\"1\":{\"146\":1}}],[\"原始的\",{\"1\":{\"263\":1}}],[\"原始self\",{\"1\":{\"146\":1}}],[\"原理\",{\"1\":{\"248\":1,\"250\":1,\"251\":1}}],[\"原本单a100跑不了的模型\",{\"1\":{\"145\":1}}],[\"总执行时间\",{\"1\":{\"265\":1}}],[\"总结\",{\"0\":{\"187\":1,\"236\":1,\"280\":1}}],[\"总大小为4sbh\",{\"1\":{\"146\":1}}],[\"总大小为19sbh\",{\"1\":{\"146\":1}}],[\"总大小是\",{\"1\":{\"146\":1}}],[\"总的大小为\",{\"1\":{\"146\":1}}],[\"总共大小为4sbh\",{\"1\":{\"146\":1}}],[\"总是绕不开训练集和测试集这两部分\",{\"1\":{\"33\":1}}],[\"qi​\",{\"1\":{\"260\":2}}],[\"q为设备数\",{\"1\":{\"255\":1}}],[\"q2\",{\"1\":{\"255\":1}}],[\"q×n\",{\"1\":{\"229\":1}}],[\"qp​\",{\"1\":{\"162\":1}}],[\"qkv\",{\"1\":{\"151\":2}}],[\"qkt矩阵相乘\",{\"1\":{\"146\":1}}],[\"q\",{\"1\":{\"146\":2,\"150\":1,\"151\":1,\"168\":2,\"228\":3,\"255\":1}}],[\"quant\",{\"1\":{\"268\":1}}],[\"quantization\",{\"1\":{\"240\":1,\"250\":2,\"268\":1}}],[\"query\",{\"1\":{\"146\":1,\"260\":1}}],[\"question\",{\"0\":{\"137\":1}}],[\"quit\",{\"1\":{\"128\":1,\"168\":2}}],[\"忽略输入输出的memory\",{\"1\":{\"146\":1}}],[\"认为2sb远小于sbh\",{\"1\":{\"146\":1}}],[\"红色线表示baseline\",{\"1\":{\"145\":1}}],[\"蓝色部分表示不同参数级别模型中需要用于保存parameter和optimizer\",{\"1\":{\"145\":1}}],[\"选读\",{\"0\":{\"142\":1},\"1\":{\"291\":1}}],[\"选择收集到一定量的梯度\",{\"1\":{\"48\":1}}],[\"\\u0013��x���2\\u0013�������������\",{\"1\":{\"134\":1}}],[\"\\u0013�p�����\\u0014��\\u0001����\\u0013�ƞ���h\\u0014�\",{\"1\":{\"134\":1}}],[\"\\u0002\\u000e\\u001d�\",{\"1\":{\"134\":1}}],[\"�\\u0005��������������������������\",{\"1\":{\"134\":1}}],[\"�8\\u0001�����������\",{\"1\":{\"134\":1}}],[\"�t���������������������\",{\"1\":{\"134\":1}}],[\"��\",{\"1\":{\"134\":1}}],[\"��`\",{\"1\":{\"134\":1}}],[\"��\\u0004��������������\",{\"1\":{\"134\":1}}],[\"��\\u0010���\",{\"1\":{\"134\":1}}],[\"��z\\b�����0x\\b��\\u0010������\\u0001����\\u0010���\\u0002��\\u0006�������\\u0006���������\\u0014��\\u0004��μ\\u0014�\\u0003�\\u0001��\\u0010������\\u0010��������\\u0010������\\u0010����������\\u0010����\\u000f�ܩ\\u0003�<\",{\"1\":{\"134\":1}}],[\"��z����z���ӌ�z�����z����z�\",{\"1\":{\"134\":1}}],[\"���\\u0007��2\",{\"1\":{\"134\":1}}],[\"���\\u0010�\",{\"1\":{\"134\":1}}],[\"�������������\",{\"1\":{\"134\":1}}],[\"�����������������������������������\",{\"1\":{\"134\":1}}],[\"����\\u0014�d\\u0016����\",{\"1\":{\"134\":1}}],[\"���z����rich��������������������������pe��d�\\u0007��1�e����������\",{\"1\":{\"134\":1}}],[\"���\\u000f��������������������������\",{\"1\":{\"134\":1}}],[\"�\\u0001l�\",{\"1\":{\"134\":1}}],[\"�\",{\"1\":{\"134\":3}}],[\"\\u0001��\\u000e\\u001f�\\u000e��\",{\"1\":{\"134\":1}}],[\"zp​\",{\"1\":{\"259\":1}}],[\"z2​\",{\"1\":{\"259\":1}}],[\"z2h​\",{\"1\":{\"148\":1}}],[\"z1​\",{\"1\":{\"259\":1}}],[\"z1h​\",{\"1\":{\"148\":1}}],[\"zi​\",{\"1\":{\"259\":1}}],[\"zi​=xai​\",{\"1\":{\"259\":1}}],[\"zip\",{\"1\":{\"124\":1,\"177\":1}}],[\"zhihu\",{\"1\":{\"149\":1}}],[\"zhuanlan\",{\"1\":{\"149\":1}}],[\"zeroslike\",{\"1\":{\"177\":1}}],[\"zeroslikeop\",{\"1\":{\"177\":1}}],[\"zeros\",{\"1\":{\"177\":8}}],[\"zero3\",{\"0\":{\"68\":1}}],[\"zero2\",{\"0\":{\"67\":1}}],[\"zero1\",{\"0\":{\"66\":1},\"1\":{\"68\":1}}],[\"zero考虑可以使用将模型内存切分成多分\",{\"1\":{\"62\":1}}],[\"zero使用不同的方式优化这两种内存使用\",{\"1\":{\"54\":1}}],[\"zero\",{\"0\":{\"51\":1,\"59\":1,\"60\":1,\"61\":1,\"65\":1,\"69\":1,\"73\":1,\"77\":1},\"1\":{\"51\":1,\"52\":1,\"73\":1,\"74\":1,\"76\":1,\"78\":3,\"79\":2,\"117\":1,\"177\":2}}],[\"图优化\",{\"1\":{\"272\":1}}],[\"图块与图块\",{\"1\":{\"272\":1}}],[\"图像宽度和高度\",{\"1\":{\"108\":1}}],[\"图中f和f​互为共轭\",{\"1\":{\"147\":1}}],[\"图中红色\",{\"1\":{\"40\":1}}],[\"图中0\",{\"1\":{\"37\":1}}],[\"函数的各个参数\",{\"1\":{\"108\":1}}],[\"函数事实上是用来计算mandelbrot图像的\",{\"1\":{\"108\":1}}],[\"首先将输入xt​分割成n个块xt\",{\"1\":{\"279\":1}}],[\"首先将xt​分割成多个补丁xt\",{\"1\":{\"279\":1}}],[\"首先我们可以根据阅读mandelbrotserial\",{\"1\":{\"108\":1}}],[\"首先通过\",{\"1\":{\"78\":1}}],[\"回答性能是否明显高于8线程并解释原因\",{\"1\":{\"107\":1}}],[\"报告最后得出的8线程加速比\",{\"1\":{\"107\":1}}],[\"找到适应任何线程数的泛型分配方式\",{\"1\":{\"107\":1}}],[\"生成一个\",{\"1\":{\"263\":1}}],[\"生成加速图\",{\"1\":{\"107\":1}}],[\"生成随机矩阵\",{\"1\":{\"24\":1}}],[\"创建线程0和线程1\",{\"1\":{\"107\":1}}],[\"创建训练操作\",{\"1\":{\"43\":1}}],[\"配置环境路径\",{\"1\":{\"106\":1}}],[\"很多环境配置会方便一些\",{\"1\":{\"106\":1}}],[\"很难做到即时通讯\",{\"1\":{\"47\":1}}],[\"还有一种可能\",{\"1\":{\"184\":1}}],[\"还是未知数\",{\"1\":{\"157\":1}}],[\"还是建议使用linux系统做lab\",{\"1\":{\"106\":1}}],[\"还需要最小化\",{\"1\":{\"76\":1}}],[\"本轮解码结束\",{\"1\":{\"263\":2}}],[\"本地归一化\",{\"1\":{\"261\":1,\"267\":1}}],[\"本地激活\",{\"1\":{\"259\":2}}],[\"本人使用os为ubuntu\",{\"1\":{\"106\":1}}],[\"本文方法\",{\"0\":{\"278\":1,\"284\":1}}],[\"本文方法基于patch并行性\",{\"1\":{\"276\":1}}],[\"本文又一次强调了模型分片到多个设备上\",{\"1\":{\"277\":1}}],[\"本文结构与贡献\",{\"1\":{\"241\":1}}],[\"本文选取了三个典型案例进行深度研究\",{\"1\":{\"240\":1}}],[\"本文进一步剖析了模型量化\",{\"1\":{\"240\":1}}],[\"本文系统性地梳理和剖析大规模模型推理所采用的高性能并行计算范式\",{\"1\":{\"240\":1}}],[\"本文希望从应用和实现的角度对autodiff\",{\"1\":{\"172\":1}}],[\"本文也建议读者有一些微积分\",{\"1\":{\"172\":1}}],[\"本文提出一系列混合并行方式\",{\"1\":{\"285\":1}}],[\"本文提出了一种新的并行范式\",{\"1\":{\"278\":1}}],[\"本文提出通过计算attn\",{\"1\":{\"162\":1}}],[\"本文提出以分块方式执行self\",{\"1\":{\"157\":1}}],[\"本文提出sequece\",{\"1\":{\"145\":1}}],[\"本文设计出ring\",{\"1\":{\"154\":1}}],[\"本文认为sp最主要的问题是跨设备如何计算sub\",{\"1\":{\"154\":1}}],[\"本文假设h极大\",{\"1\":{\"146\":1}}],[\"本文推导与假设中用到了以下几个参量\",{\"1\":{\"146\":1}}],[\"本文以transformer结构为例估算activation\",{\"1\":{\"146\":1}}],[\"本文引入了\",{\"1\":{\"97\":1}}],[\"本文探讨了以下几种因素对效率的影响\",{\"1\":{\"91\":1}}],[\"本文中研究人员提出了一种名为ptd\",{\"1\":{\"91\":1}}],[\"本文将其记作2ψ+2ψ+kψ\",{\"1\":{\"55\":1}}],[\"本文将其统称为残差状态\",{\"1\":{\"53\":1}}],[\"本文指出了ring\",{\"1\":{\"38\":1}}],[\"本文通过在不同数量卡上训练结果进行说明\",{\"1\":{\"38\":1}}],[\"本文还指出dp代码重构成本较大\",{\"1\":{\"38\":1}}],[\"依据如下规则进行更新\",{\"1\":{\"101\":1}}],[\"依据是\",{\"1\":{\"48\":1}}],[\"执行矩阵乘法\",{\"1\":{\"268\":1}}],[\"执行流程\",{\"1\":{\"266\":1}}],[\"执行对通信带宽要求极高的张量并行\",{\"1\":{\"247\":1}}],[\"执行allgather操作\",{\"1\":{\"279\":1}}],[\"执行all\",{\"1\":{\"150\":1}}],[\"执行过程中有两步\",{\"1\":{\"148\":1}}],[\"执行优化器\",{\"0\":{\"101\":1},\"1\":{\"101\":1}}],[\"执行模拟器\",{\"0\":{\"100\":1}}],[\"执行同步训练\",{\"1\":{\"43\":1}}],[\"当微批次数量m远大于阶段数p时\",{\"1\":{\"265\":1}}],[\"当tf​≈tb​时\",{\"1\":{\"265\":1}}],[\"当序列长度\",{\"1\":{\"261\":1}}],[\"当处理多个图像生成任务时\",{\"1\":{\"253\":1}}],[\"当并行度过高\",{\"1\":{\"245\":1}}],[\"当单个模型的体积超出单设备显存时\",{\"1\":{\"243\":1}}],[\"当c和d相同时也需要正确处理\",{\"1\":{\"227\":1}}],[\"当字符出现次数已经超过m时\",{\"1\":{\"215\":1}}],[\"当n很大的时候\",{\"1\":{\"185\":1}}],[\"当计算self\",{\"1\":{\"157\":1}}],[\"当然如何找到这样一个自动优化过程是复杂且艰巨的\",{\"1\":{\"174\":1}}],[\"当然我们也可以通过编写调度器实现学习率的动态调整\",{\"1\":{\"174\":1}}],[\"当然\",{\"1\":{\"99\":1}}],[\"当在同一个bucket中的梯度的hook都被调用后\",{\"1\":{\"48\":1}}],[\"种\",{\"1\":{\"99\":1}}],[\"至少有\",{\"1\":{\"99\":1}}],[\"假设第i个gpu持有数据块xi​\",{\"1\":{\"267\":1}}],[\"假设在第\",{\"1\":{\"263\":1}}],[\"假设我们有\",{\"1\":{\"259\":1,\"265\":1}}],[\"假设加速是否与线程数线性相关并加以验证\",{\"1\":{\"107\":1}}],[\"假设模型包含\",{\"1\":{\"99\":1}}],[\"假设有ψ个参数\",{\"1\":{\"55\":1}}],[\"假设有4块gpu\",{\"1\":{\"39\":1}}],[\"都是临时的不成体系的方案\",{\"1\":{\"285\":1}}],[\"都持有一个完整的模型副本\",{\"1\":{\"243\":1}}],[\"都可以对其取$\",{\"1\":{\"185\":1}}],[\"都能支持2种或者更多的拆分方式\",{\"1\":{\"99\":1}}],[\"都支持\",{\"1\":{\"99\":1}}],[\"都会有大量的空闲时间\",{\"1\":{\"84\":1}}],[\"损失函数\",{\"1\":{\"99\":1}}],[\"矩阵按行切分\",{\"1\":{\"260\":1}}],[\"矩阵访问\",{\"1\":{\"233\":1}}],[\"矩阵操作与状态维护\",{\"1\":{\"236\":1}}],[\"矩阵操作\",{\"0\":{\"230\":1}}],[\"矩阵乘法\",{\"1\":{\"99\":1}}],[\"矩阵相乘\",{\"1\":{\"24\":1}}],[\"嵌入通过参与计算设备之间的高度优化的全对全集合\",{\"1\":{\"151\":1}}],[\"嵌入中\",{\"1\":{\"151\":1}}],[\"嵌入\",{\"1\":{\"99\":1}}],[\"归一化\",{\"1\":{\"99\":1}}],[\"全局方差为\",{\"1\":{\"267\":1}}],[\"全局均值为\",{\"1\":{\"267\":1}}],[\"全局统计同步\",{\"1\":{\"261\":1}}],[\"全局batchsize\",{\"1\":{\"95\":1}}],[\"全连接层\",{\"1\":{\"99\":1}}],[\"池化\",{\"1\":{\"99\":1}}],[\"卷积或注意力层\",{\"1\":{\"279\":1}}],[\"卷积\",{\"1\":{\"99\":1}}],[\"搜索空间对于不同的\",{\"1\":{\"99\":1}}],[\"搜索算法\",{\"1\":{\"98\":1}}],[\"概述\",{\"0\":{\"99\":1}}],[\"马尔可夫链蒙特卡洛\",{\"1\":{\"98\":1}}],[\"增强生成内容的质量和细节\",{\"1\":{\"292\":1}}],[\"增加设备数会减小每个设备上的局部批量\",{\"1\":{\"243\":1}}],[\"增量的执行模拟器\",{\"1\":{\"98\":1}}],[\"增大模型规模可以提升模型效果\",{\"1\":{\"82\":1}}],[\"空间复杂度\",{\"1\":{\"184\":1,\"197\":1,\"204\":1,\"211\":1,\"217\":1,\"223\":1,\"229\":1,\"235\":1}}],[\"空间\",{\"1\":{\"98\":1}}],[\"描述单个\",{\"1\":{\"98\":1}}],[\"描述参数如何并行\",{\"1\":{\"98\":1}}],[\"描述训练数据如何并行\",{\"1\":{\"98\":1}}],[\"描述不同的\",{\"1\":{\"98\":1}}],[\"引入了一个\",{\"1\":{\"97\":1}}],[\"取决于最终的throughput\",{\"1\":{\"95\":1}}],[\"$来得到这个n进制数的位数\",{\"1\":{\"185\":1}}],[\"$eax\",{\"1\":{\"168\":1}}],[\"$esp\",{\"1\":{\"168\":1}}],[\"$��������������������������������\",{\"1\":{\"134\":1}}],[\"$\",{\"1\":{\"95\":1,\"106\":1,\"245\":2,\"262\":3}}],[\"$mpipathshare\",{\"1\":{\"24\":1}}],[\"$mpipathlib\",{\"1\":{\"24\":1}}],[\"$mpipathinclude\",{\"1\":{\"24\":1}}],[\"$mpipathbin\",{\"1\":{\"24\":1}}],[\"有\",{\"1\":{\"285\":1}}],[\"有效计算时间\",{\"1\":{\"265\":1}}],[\"有效绕开了底层框架对特定分布式后端\",{\"1\":{\"256\":1}}],[\"有机结合了dp\",{\"1\":{\"247\":1}}],[\"有强依赖性\",{\"1\":{\"245\":1}}],[\"有过半数的人支持该规则\",{\"1\":{\"219\":1}}],[\"有一些导数\",{\"1\":{\"172\":1}}],[\"有两个layernorm层\",{\"1\":{\"146\":1}}],[\"有$\",{\"1\":{\"95\":1}}],[\"有了对模型算子切分的方法\",{\"1\":{\"35\":1}}],[\"令\",{\"1\":{\"95\":1}}],[\"越大\",{\"1\":{\"93\":1}}],[\"中间激活所需的峰值内存减少到\",{\"1\":{\"294\":1}}],[\"中间结果全部丢掉\",{\"1\":{\"86\":1}}],[\"中采样一个额外的token\",{\"1\":{\"263\":1}}],[\"中采样一个token\",{\"1\":{\"263\":1}}],[\"中\",{\"1\":{\"248\":1,\"265\":1}}],[\"中的\",{\"1\":{\"292\":1}}],[\"中的算子做再实现\",{\"1\":{\"172\":1}}],[\"中的不同层发放到多块gpu上\",{\"1\":{\"93\":1}}],[\"考虑dit模型特点\",{\"1\":{\"284\":1}}],[\"考虑到\",{\"1\":{\"285\":1}}],[\"考虑到激活值规模庞大\",{\"1\":{\"275\":1}}],[\"考虑到cse234课程要求\",{\"1\":{\"172\":1}}],[\"考虑gelu非线性\",{\"1\":{\"148\":1}}],[\"考虑tp并行\",{\"1\":{\"147\":1}}],[\"考虑使用模型并行\",{\"1\":{\"95\":1}}],[\"考虑张量并行\",{\"0\":{\"94\":1}}],[\"考虑流水线并行\",{\"0\":{\"93\":1}}],[\"考虑一些综合利用并行方式的策略\",{\"1\":{\"35\":1}}],[\"问题定义\",{\"1\":{\"268\":1}}],[\"问题域\",{\"1\":{\"253\":1,\"254\":1,\"255\":1}}],[\"问题\",{\"0\":{\"283\":1},\"1\":{\"92\":1,\"288\":1}}],[\"问题是\",{\"1\":{\"84\":1}}],[\"贡献\",{\"0\":{\"91\":1}}],[\"|\",{\"1\":{\"89\":1}}],[\"||\",{\"1\":{\"12\":1,\"14\":1}}],[\"掉的\",{\"1\":{\"89\":1}}],[\"掉的输入\",{\"1\":{\"89\":1}}],[\"掉\",{\"1\":{\"89\":1}}],[\"编码\",{\"1\":{\"89\":1}}],[\"实际情况种\",{\"1\":{\"99\":1}}],[\"实际上就是要求阶乘的位数倒也没有拐弯抹角\",{\"1\":{\"184\":1}}],[\"实际上是按行切分的\",{\"1\":{\"146\":1}}],[\"实际上一般也确实极大\",{\"1\":{\"146\":1}}],[\"实际上\",{\"1\":{\"89\":1}}],[\"实现多机多卡\",{\"1\":{\"293\":1}}],[\"实现高效地扩展方法\",{\"1\":{\"285\":1}}],[\"实现复杂的混合并行\",{\"1\":{\"285\":1}}],[\"实现了\",{\"1\":{\"288\":1}}],[\"实现了优异的性能\",{\"1\":{\"250\":1}}],[\"实现了高效扩展\",{\"1\":{\"247\":1}}],[\"实现请求级并行\",{\"1\":{\"243\":1}}],[\"实现依赖于p2p的通信模式\",{\"1\":{\"162\":1}}],[\"实现将两个图片都拉到8线程时7\",{\"1\":{\"107\":1}}],[\"实现mlp的整体的tensor\",{\"1\":{\"89\":1}}],[\"实现对超大规模模型的支持\",{\"1\":{\"82\":1}}],[\"实现超线性优化\",{\"1\":{\"52\":1}}],[\"即由于流水线启动和排空阶段的依赖关系\",{\"1\":{\"264\":1}}],[\"即\",{\"1\":{\"262\":2,\"268\":1}}],[\"即被复制\",{\"1\":{\"261\":1}}],[\"即使经过tp切分\",{\"1\":{\"261\":1}}],[\"即使在n很小的时候\",{\"1\":{\"185\":1}}],[\"即常数时间复杂度\",{\"1\":{\"185\":1}}],[\"即可轻松求得n\",{\"1\":{\"185\":1}}],[\"即向下取整\",{\"1\":{\"185\":1}}],[\"即如何让模型进行学习\",{\"1\":{\"173\":1}}],[\"即sequence\",{\"1\":{\"148\":1}}],[\"即只考虑中间过程的memory\",{\"1\":{\"146\":1}}],[\"即将图像的不同空间交给不同线程计算\",{\"1\":{\"107\":1}}],[\"即在权重矩阵上执行\",{\"1\":{\"89\":1}}],[\"即输入与参数矩阵相乘\",{\"1\":{\"89\":1}}],[\"另一个全对全集合将注意力计算的输出上下文张量转换为序列\",{\"1\":{\"151\":1}}],[\"另一种方法是沿着它的列\",{\"1\":{\"89\":1}}],[\"另一部分动态存储临时数据\",{\"1\":{\"72\":1}}],[\"另一部分用来测试模型效果\",{\"1\":{\"33\":1}}],[\"来促进区块间的交互\",{\"1\":{\"278\":1}}],[\"来管理逻辑块到物理块的映射\",{\"1\":{\"248\":1}}],[\"来找出模型对损失的影响\",{\"1\":{\"174\":1}}],[\"来进行存储的话\",{\"1\":{\"146\":1}}],[\"来确定当前卡要\",{\"1\":{\"89\":1}}],[\"来实现tensor\",{\"1\":{\"89\":1}}],[\"来对尽可能多的标签进行刻画\",{\"1\":{\"33\":1}}],[\"张量并行等并行技术并不适合diffusion模型\",{\"1\":{\"275\":1}}],[\"张量并行\",{\"0\":{\"87\":1},\"1\":{\"255\":1,\"258\":1}}],[\"类似cpu指令执行\",{\"1\":{\"85\":1}}],[\"切成更小的micro\",{\"1\":{\"85\":1}}],[\"切分小patch分到不同设备处理\",{\"1\":{\"276\":1}}],[\"切分后的矩阵过小\",{\"1\":{\"245\":1}}],[\"切分后的任务如何再将结果重新合在一起\",{\"1\":{\"32\":1}}],[\"切分成若干份给不同的gpu\",{\"1\":{\"37\":1}}],[\"切分模型让不同gpu延后计算开始时间\",{\"1\":{\"35\":1}}],[\"块表\",{\"1\":{\"248\":1}}],[\"块\",{\"1\":{\"84\":1,\"259\":2}}],[\"个位置发生了拒绝\",{\"1\":{\"263\":1}}],[\"个草稿token都被接受了\",{\"1\":{\"263\":1}}],[\"个草稿token\",{\"1\":{\"263\":1}}],[\"个token的草稿序列\",{\"1\":{\"263\":1}}],[\"个gpu组织成一个\",{\"1\":{\"262\":1}}],[\"个gpu的子矩阵\",{\"1\":{\"259\":1}}],[\"个gpu\",{\"1\":{\"259\":1}}],[\"个批次的forward和backward运算\",{\"1\":{\"84\":1}}],[\"个\",{\"1\":{\"84\":2,\"99\":1}}],[\"个人介绍\",{\"0\":{\"0\":1}}],[\"第二步通信\",{\"1\":{\"262\":1}}],[\"第二次矩阵乘法\",{\"1\":{\"259\":1}}],[\"第二个线性层按行并行\",{\"1\":{\"245\":1}}],[\"第一步通信\",{\"1\":{\"262\":1}}],[\"第一次矩阵乘法\",{\"1\":{\"259\":1}}],[\"第一个线性层按列并行\",{\"1\":{\"245\":1}}],[\"第i个元素为\",{\"1\":{\"175\":1}}],[\"第\",{\"1\":{\"84\":1}}],[\"然后将n个device的结果合成为一个全分辨率的结果\",{\"1\":{\"275\":1}}],[\"然后将请求分发给由slurm动态启动和管理的多个vllm推理服务进程\",{\"1\":{\"256\":1}}],[\"然后取其四次方根\",{\"1\":{\"268\":1}}],[\"然后由原始的\",{\"1\":{\"251\":1}}],[\"然后就会发现\",{\"1\":{\"184\":1}}],[\"然后重复1\",{\"1\":{\"174\":1}}],[\"然后通过数据并行技术扩展集群上进行加速\",{\"1\":{\"95\":1}}],[\"然后与权重矩阵进行矩阵乘法\",{\"1\":{\"89\":1}}],[\"然后流水线并行\",{\"1\":{\"85\":1}}],[\"然后每份放到一块\",{\"1\":{\"84\":1}}],[\"然后给不同的gpu去算每一个单独的部分\",{\"1\":{\"35\":1}}],[\"份\",{\"1\":{\"84\":1,\"262\":2}}],[\"事实上task中给的提示还是比较明显的\",{\"1\":{\"107\":1}}],[\"事实上是tp\",{\"1\":{\"84\":1}}],[\"事实上对于采用哪种并行模式\",{\"1\":{\"35\":1}}],[\"最佳的尺度因子s\",{\"1\":{\"268\":1}}],[\"最佳尺度因子s的求解\",{\"1\":{\"268\":1}}],[\"最终生成的序列是\",{\"1\":{\"263\":1}}],[\"最终的结果\",{\"1\":{\"262\":1}}],[\"最终的输出\",{\"1\":{\"259\":1}}],[\"最终gpipe通过更大规模的模型和更大的batch\",{\"1\":{\"82\":1}}],[\"最后通过一次\",{\"1\":{\"260\":1}}],[\"最后通过一次all\",{\"1\":{\"245\":1}}],[\"最后一个数字后不能有空格\",{\"1\":{\"233\":1}}],[\"最后输出时根据映射关系重建矩阵\",{\"1\":{\"232\":1}}],[\"最后输出字符\",{\"1\":{\"194\":1}}],[\"最后需要输出换行符\",{\"1\":{\"227\":1}}],[\"最后\",{\"1\":{\"150\":1,\"240\":1,\"245\":2,\"279\":1}}],[\"最大迭代次数\",{\"1\":{\"108\":1}}],[\"最优\",{\"1\":{\"101\":1}}],[\"最小通信量为\",{\"1\":{\"76\":1}}],[\"最小化cpu和gpu之间的通信量\",{\"1\":{\"75\":1}}],[\"提供了一套优雅的开发接口\",{\"1\":{\"285\":1}}],[\"提供一个从理论到工程实践的完整审视视角\",{\"1\":{\"241\":1}}],[\"提升存储效率\",{\"1\":{\"284\":1}}],[\"提升训练精度\",{\"1\":{\"35\":1}}],[\"提高了模型并行模式下的设备利用率\",{\"1\":{\"82\":1}}],[\"方差的标准公式是\",{\"1\":{\"267\":1}}],[\"方差计算\",{\"1\":{\"267\":1}}],[\"方法及其变体\",{\"1\":{\"173\":1}}],[\"方法无法满足日益增长的参数量需求\",{\"1\":{\"52\":1}}],[\"方案\",{\"1\":{\"82\":1}}],[\"必须等待批次中所有请求都完成后才能返回结果\",{\"1\":{\"248\":1}}],[\"必须正确初始化row和col数组为0到n\",{\"1\":{\"233\":1}}],[\"必须先将输入字符串转换为小写\",{\"1\":{\"202\":1}}],[\"必须严格按照l\",{\"1\":{\"195\":1}}],[\"必须是小写\",{\"1\":{\"195\":1}}],[\"必须是大写\",{\"1\":{\"195\":1}}],[\"必须引入分布式系统\",{\"1\":{\"82\":1}}],[\"必须在gpu上进行\",{\"1\":{\"76\":1}}],[\"仅在第二次矩阵乘法后需要一次all\",{\"1\":{\"245\":1}}],[\"仅提供注释供参考~\",{\"1\":{\"176\":1}}],[\"仅对原课程代码已经实现的算子分享全部代码\",{\"1\":{\"176\":1}}],[\"仅将属于其\",{\"1\":{\"79\":1}}],[\"仅需\",{\"1\":{\"42\":1}}],[\"到1\",{\"1\":{\"268\":1}}],[\"到\",{\"1\":{\"79\":2}}],[\"广播至所有gpu\",{\"1\":{\"260\":1}}],[\"广播更新后的参数\",{\"1\":{\"78\":1}}],[\"广播到所有其他进程\",{\"1\":{\"43\":1}}],[\"删除\",{\"1\":{\"78\":1}}],[\"上标\",{\"1\":{\"279\":1}}],[\"上下文\",{\"1\":{\"263\":1}}],[\"上计算局部的均值\",{\"1\":{\"261\":1}}],[\"上式是\",{\"1\":{\"175\":1}}],[\"上进行\",{\"1\":{\"89\":1}}],[\"上对参数进行类似\",{\"1\":{\"79\":1}}],[\"上并行更新对应的参数\",{\"1\":{\"79\":1}}],[\"上可用\",{\"1\":{\"79\":1}}],[\"上\",{\"1\":{\"79\":1,\"82\":1,\"84\":2,\"98\":2,\"279\":1}}],[\"上更新参数\",{\"1\":{\"78\":1}}],[\"上分配全精度参数\",{\"1\":{\"78\":1}}],[\"上分配半精度参数\",{\"1\":{\"78\":1}}],[\"上的开销\",{\"1\":{\"184\":1}}],[\"上的梯度\",{\"1\":{\"78\":1}}],[\"上的\",{\"1\":{\"76\":1}}],[\"上的内存\",{\"1\":{\"74\":1}}],[\"层视为线性层来处理\",{\"1\":{\"89\":1}}],[\"层时\",{\"1\":{\"89\":1}}],[\"层\",{\"1\":{\"78\":1,\"84\":2,\"245\":1}}],[\"xdit使用xfuserlongcontextattention把sp的中间结果存在kv\",{\"1\":{\"293\":1}}],[\"xdit专门定制了序列并行的实现方式\",{\"1\":{\"293\":1}}],[\"xdit支持四种基础并行策略以任何形式混合\",{\"1\":{\"287\":1}}],[\"xdit\",{\"0\":{\"285\":1},\"1\":{\"285\":2,\"288\":1,\"293\":2}}],[\"xt\",{\"1\":{\"279\":4}}],[\"xt​\",{\"1\":{\"277\":2,\"279\":2}}],[\"xt−1​=update\",{\"1\":{\"277\":1}}],[\"xij​是输入张量x的第i个token在第j个通道的值\",{\"1\":{\"268\":1}}],[\"xi​\",{\"1\":{\"261\":1,\"262\":3}}],[\"x∥\",{\"1\":{\"268\":1}}],[\"xk+1​\",{\"1\":{\"263\":1}}],[\"xnew​\",{\"1\":{\"263\":2}}],[\"x∣\",{\"1\":{\"263\":2}}],[\"x~i−1​\",{\"1\":{\"263\":2}}],[\"x~i+1​\",{\"1\":{\"263\":1}}],[\"x~i​∣\",{\"1\":{\"263\":2}}],[\"x~i​\",{\"1\":{\"263\":3}}],[\"x~1\",{\"1\":{\"263\":2}}],[\"x~1​\",{\"1\":{\"263\":3}}],[\"x~k​\",{\"1\":{\"263\":1}}],[\"x~2​\",{\"1\":{\"263\":1}}],[\"xprefix​\",{\"1\":{\"263\":2}}],[\"xwiv​\",{\"1\":{\"260\":1}}],[\"xwik​\",{\"1\":{\"260\":1}}],[\"x是输入\",{\"1\":{\"259\":1}}],[\"x2\",{\"1\":{\"267\":3}}],[\"x2​a2​\",{\"1\":{\"245\":1}}],[\"x2​\",{\"1\":{\"245\":1}}],[\"x2s​\",{\"1\":{\"148\":2}}],[\"xa\",{\"1\":{\"259\":1}}],[\"xa2​\",{\"1\":{\"245\":1}}],[\"xa1​\",{\"1\":{\"245\":1}}],[\"xavier\",{\"1\":{\"89\":1}}],[\"x+h\",{\"1\":{\"175\":1}}],[\"x=\",{\"1\":{\"148\":1,\"245\":1}}],[\"x1​a1​\",{\"1\":{\"245\":1}}],[\"x1​\",{\"1\":{\"245\":1}}],[\"x1s​\",{\"1\":{\"148\":2}}],[\"x1\",{\"1\":{\"108\":2}}],[\"x0\",{\"1\":{\"108\":2}}],[\"x\",{\"1\":{\"78\":7,\"89\":2,\"119\":3,\"128\":4,\"148\":1,\"168\":2,\"175\":1,\"185\":1,\"222\":2,\"234\":5,\"259\":1,\"260\":1,\"261\":2,\"262\":2,\"263\":1,\"267\":5,\"268\":5}}],[\"xvf\",{\"1\":{\"19\":1,\"106\":1}}],[\"下标转换\",{\"1\":{\"233\":1}}],[\"下文将介绍一种用数学方法巧妙估算阶乘结果规模的方式\",{\"1\":{\"184\":1}}],[\"下面是对这个算法的详细解释\",{\"1\":{\"173\":1}}],[\"下面是两种重计算策略的对比\",{\"1\":{\"163\":1}}],[\"下图中绿色部分表示不同参数级别模型中需要用于保存activation需要的显存大小\",{\"1\":{\"145\":1}}],[\"下图gpu与cpu二次通信应该是从cpu到gpu\",{\"1\":{\"78\":1}}],[\"下载包\",{\"1\":{\"106\":1}}],[\"下载解压一下编译环境就好啦\",{\"1\":{\"106\":1}}],[\"下也能保持效率\",{\"1\":{\"75\":1}}],[\"之间的随机数\",{\"1\":{\"263\":1}}],[\"之间的移动\",{\"1\":{\"74\":1}}],[\"之间天然的独立性\",{\"1\":{\"260\":1}}],[\"之间进行\",{\"1\":{\"79\":1}}],[\"之后\",{\"1\":{\"78\":1}}],[\"消除大部分通信成本\",{\"1\":{\"78\":1}}],[\"消除了数据和模型并行训练中的内存冗余\",{\"1\":{\"52\":1}}],[\"此处省略了时间步t和条件c的输入\",{\"1\":{\"279\":1}}],[\"此处x切分仅作示意\",{\"1\":{\"146\":1}}],[\"此过程不会增加开销\",{\"1\":{\"157\":1}}],[\"此外\",{\"1\":{\"78\":1}}],[\"此时不再提供任何其他新的输入\",{\"1\":{\"93\":1}}],[\"此时\",{\"1\":{\"40\":1,\"255\":1,\"263\":1}}],[\"只要接受率足够高\",{\"1\":{\"263\":1}}],[\"只需要检查最后一位数字即可判断奇偶性\",{\"1\":{\"210\":1}}],[\"只需要在gpu内存中临时保存少量的梯度\",{\"1\":{\"78\":1}}],[\"只需判断最后一个字符对应的数字的奇偶性\",{\"1\":{\"208\":1}}],[\"只保留最开始的输入\",{\"1\":{\"86\":1}}],[\"只有将\",{\"1\":{\"76\":1}}],[\"只有那些计算复杂度低于o\",{\"1\":{\"76\":1}}],[\"只有很少一部分被过程中产生的冗余量使用了\",{\"1\":{\"54\":1}}],[\"由于去噪状态迅速变化\",{\"1\":{\"280\":1}}],[\"由于激活的缩放是无损的浮点运算\",{\"1\":{\"268\":1}}],[\"由于gelu是一个逐元素\",{\"1\":{\"259\":1}}],[\"由于我之前已经测试过最大数据规模\",{\"1\":{\"186\":1}}],[\"由于块计算比块传输需要更长的时间\",{\"1\":{\"157\":1}}],[\"由于该方法将环中主机设备之间的key\",{\"1\":{\"157\":1}}],[\"由于fp16参数已经位于gpu上\",{\"1\":{\"78\":1}}],[\"由于每个机器\",{\"1\":{\"48\":1}}],[\"单卡推理必然无法满足需求\",{\"1\":{\"285\":1}}],[\"单卡策略\",{\"0\":{\"78\":1}}],[\"单gpu无法满足实际应用的延迟要求\",{\"1\":{\"283\":1}}],[\"单个gpu视角\",{\"1\":{\"265\":1}}],[\"单个gpu的激活内存占用减少为\",{\"1\":{\"262\":1}}],[\"单个微批次在一个阶段上的反向传播时间\",{\"1\":{\"265\":1}}],[\"单个微批次在一个阶段上的前向传播时间\",{\"1\":{\"265\":1}}],[\"单节点优化\",{\"1\":{\"256\":1}}],[\"单一的并行策略往往不足\",{\"1\":{\"247\":1}}],[\"单身狗进化\",{\"1\":{\"183\":1}}],[\"单batch\",{\"1\":{\"85\":1}}],[\"单批量以这种顺序进行计算\",{\"1\":{\"84\":1}}],[\"单机gpu内存有限\",{\"1\":{\"82\":1}}],[\"单就这一块部分内存而言\",{\"1\":{\"60\":1}}],[\"发送到\",{\"1\":{\"76\":2,\"79\":1}}],[\"如下图所示\",{\"1\":{\"293\":1}}],[\"如u\",{\"1\":{\"277\":1}}],[\"如将高分辨率图像压缩为低分辨率的潜在表示\",{\"1\":{\"276\":1}}],[\"如int4\",{\"1\":{\"268\":1}}],[\"如每个通道的最大绝对值或l2范数\",{\"1\":{\"268\":1}}],[\"如gpipe的\",{\"1\":{\"265\":1}}],[\"如gelu\",{\"1\":{\"259\":1}}],[\"如sglang\",{\"1\":{\"256\":1}}],[\"如slurm\",{\"1\":{\"256\":1}}],[\"如tp\",{\"1\":{\"256\":1}}],[\"如text\",{\"1\":{\"253\":1}}],[\"如round\",{\"1\":{\"268\":1}}],[\"如reduce\",{\"1\":{\"254\":1}}],[\"如ring\",{\"1\":{\"159\":1}}],[\"如layernorm和dropout\",{\"1\":{\"254\":1}}],[\"如nvlink\",{\"1\":{\"245\":1}}],[\"如矩阵乘法\",{\"1\":{\"245\":1,\"261\":1}}],[\"如何高效地对单个复杂样本进行并行计算成为核心挑战\",{\"1\":{\"255\":1}}],[\"如何最大化并发处理能力\",{\"1\":{\"241\":1}}],[\"如何快速搜索\",{\"1\":{\"98\":1}}],[\"如何将一个\",{\"1\":{\"98\":2}}],[\"如何合理的使用上面的多种并行技术一直是一个困难的命题\",{\"1\":{\"91\":1}}],[\"如进行backward更新\",{\"1\":{\"95\":1}}],[\"如microbatch的大小\",{\"1\":{\"91\":1}}],[\"如图中所示\",{\"1\":{\"76\":1}}],[\"如范数计算\",{\"1\":{\"76\":1}}],[\"如果不同rank的输入patch没有重叠\",{\"1\":{\"293\":1}}],[\"如果所有\",{\"1\":{\"263\":1}}],[\"如果所有草稿都接受\",{\"1\":{\"263\":1}}],[\"如果发生拒绝\",{\"1\":{\"263\":1}}],[\"如果\",{\"1\":{\"263\":2}}],[\"如果先将\",{\"1\":{\"259\":1}}],[\"如果草稿被接受\",{\"1\":{\"251\":1}}],[\"如果简单分块计算约会有1\",{\"1\":{\"161\":1}}],[\"如果已经到最后部分不够切分\",{\"1\":{\"108\":1}}],[\"如果该遇到整除要加一行避免遗漏\",{\"1\":{\"108\":1}}],[\"如果还是使用半精度进行运算\",{\"1\":{\"55\":1}}],[\"如果我们希望分布式系统在ai训练中发挥他的力量\",{\"1\":{\"35\":1}}],[\"如果我们一台机器存储不开我们的数据集\",{\"1\":{\"33\":1}}],[\"如果我们一台机器\",{\"1\":{\"33\":1}}],[\"和distrifusion相同操作如patch分割\",{\"1\":{\"284\":1}}],[\"和方差\",{\"1\":{\"261\":1}}],[\"和权重分块\",{\"1\":{\"259\":1}}],[\"和由slurm管理的多个独立vllm实例组成的解耦式架构\",{\"1\":{\"256\":1}}],[\"和流水线并行\",{\"1\":{\"255\":1}}],[\"和草稿模型的延迟密切相关\",{\"1\":{\"251\":1}}],[\"和多个独立的推理副本构成\",{\"1\":{\"243\":1}}],[\"和分页注意力\",{\"1\":{\"240\":1}}],[\"和张量并行\",{\"1\":{\"240\":1}}],[\"和right\",{\"1\":{\"221\":1}}],[\"和一个\",{\"1\":{\"192\":1}}],[\"和阶乘计算结果的误差程度\",{\"1\":{\"185\":1}}],[\"和ulysses一样\",{\"1\":{\"153\":1}}],[\"和值\",{\"1\":{\"150\":1,\"151\":1}}],[\"和q\",{\"1\":{\"146\":1}}],[\"和attention的qkv部分\",{\"1\":{\"147\":1}}],[\"和attention\",{\"1\":{\"146\":1}}],[\"和selective\",{\"1\":{\"145\":1}}],[\"和memory\",{\"1\":{\"95\":1}}],[\"和数据并行时\",{\"1\":{\"95\":1}}],[\"和\",{\"1\":{\"76\":2,\"84\":1,\"89\":1,\"97\":1,\"99\":1,\"100\":1,\"101\":1,\"194\":1,\"245\":2,\"261\":1,\"262\":2,\"263\":1,\"265\":1,\"267\":1,\"279\":1,\"285\":2,\"292\":1,\"293\":1}}],[\"的生成\",{\"1\":{\"292\":1}}],[\"的现有逻辑\",{\"1\":{\"285\":1}}],[\"的特点\",{\"1\":{\"285\":1}}],[\"的输入序列长度日益增长\",{\"1\":{\"285\":1}}],[\"的激活信息\",{\"1\":{\"278\":1}}],[\"的确没有本文出色\",{\"1\":{\"277\":1}}],[\"的通道\",{\"1\":{\"268\":1}}],[\"的通信带宽\",{\"1\":{\"76\":1}}],[\"的数值范围更小\",{\"1\":{\"268\":1}}],[\"的数值幅度始终很大\",{\"1\":{\"268\":1}}],[\"的数量\",{\"1\":{\"265\":1}}],[\"的数量进行切分\",{\"1\":{\"245\":1}}],[\"的权重对模型性能至关重要\",{\"1\":{\"268\":1}}],[\"的目标是最小化\",{\"1\":{\"268\":1}}],[\"的核心洞察是\",{\"1\":{\"268\":1}}],[\"的核心是将神经网络中计算密集的算子\",{\"1\":{\"258\":1}}],[\"的子张量\",{\"1\":{\"267\":1}}],[\"的时间\",{\"1\":{\"265\":1}}],[\"的概率分布中采样一个新的token来补上\",{\"1\":{\"263\":1}}],[\"的结果聚合起来\",{\"1\":{\"262\":1}}],[\"的gpu拥有数据\",{\"1\":{\"262\":1}}],[\"的二维网格\",{\"1\":{\"262\":1}}],[\"的张量切分为p份\",{\"1\":{\"267\":1}}],[\"的张量\",{\"1\":{\"261\":1}}],[\"的内存占用\",{\"1\":{\"261\":1}}],[\"的流程\",{\"1\":{\"259\":1}}],[\"的总数量\",{\"1\":{\"265\":1}}],[\"的总和\",{\"1\":{\"259\":1,\"265\":1}}],[\"的总结\",{\"1\":{\"172\":1}}],[\"的操作\",{\"1\":{\"259\":1}}],[\"的工程范例\",{\"1\":{\"256\":1}}],[\"的依赖和功能限制\",{\"1\":{\"256\":1}}],[\"的序列化限制\",{\"1\":{\"251\":1}}],[\"的关键权重\",{\"1\":{\"250\":1}}],[\"的策略\",{\"1\":{\"245\":1,\"259\":1}}],[\"的兴起\",{\"1\":{\"241\":1}}],[\"的来临\",{\"1\":{\"241\":1}}],[\"的位数\",{\"1\":{\"185\":1}}],[\"的位数就是距离弯通脱单的天数\",{\"1\":{\"183\":1}}],[\"的弯通想知道自己还有多久能脱单\",{\"1\":{\"183\":1}}],[\"的阶乘\",{\"1\":{\"183\":1}}],[\"的方式\",{\"1\":{\"174\":1}}],[\"的方案\",{\"1\":{\"82\":1}}],[\"的代码\",{\"1\":{\"108\":1}}],[\"的开头和结尾插入计时代码\",{\"1\":{\"107\":1}}],[\"的并行化\",{\"1\":{\"244\":1,\"245\":1}}],[\"的并行方法\",{\"1\":{\"98\":1}}],[\"的并发事实\",{\"1\":{\"107\":1}}],[\"的执行顺序\",{\"1\":{\"99\":1}}],[\"的效能\",{\"1\":{\"91\":1}}],[\"的模型参数\",{\"1\":{\"84\":1}}],[\"的平均梯度\",{\"1\":{\"79\":1}}],[\"的\",{\"1\":{\"76\":2,\"79\":1,\"84\":1,\"89\":2,\"101\":1}}],[\"的计算和\",{\"1\":{\"285\":1}}],[\"的计算复杂度都是o\",{\"1\":{\"76\":1}}],[\"的计算才能转移到cpu上\",{\"1\":{\"76\":1}}],[\"的额外通讯开销\",{\"1\":{\"68\":1}}],[\"训练的计算复杂度通常为o\",{\"1\":{\"76\":1}}],[\"训练效果越好\",{\"1\":{\"33\":1}}],[\"允许将cpu优化器步骤与gpu计算重叠\",{\"1\":{\"75\":1}}],[\"其每个通道j乘以对应的sj​\",{\"1\":{\"268\":1}}],[\"其元素数量为ni​\",{\"1\":{\"267\":1}}],[\"其计算可以表示为\",{\"1\":{\"259\":1}}],[\"其精髓在于利用矩阵乘法的线性代数性质\",{\"1\":{\"258\":1}}],[\"其系统设计融合了多维度并行策略\",{\"1\":{\"255\":1}}],[\"其加速效果与草稿模型的接受率\",{\"1\":{\"251\":1}}],[\"其次\",{\"1\":{\"240\":1}}],[\"其在推理\",{\"1\":{\"240\":1}}],[\"其余算子均为个人实现\",{\"1\":{\"176\":1}}],[\"其余保持图像默认参数\",{\"1\":{\"108\":1}}],[\"其他元素均为\",{\"1\":{\"175\":1}}],[\"其复杂度为o\",{\"1\":{\"76\":1}}],[\"其速度比现有技术快6倍\",{\"1\":{\"75\":1}}],[\"其中n表示设备的数量\",{\"1\":{\"279\":1}}],[\"其中t表示总步数\",{\"1\":{\"277\":1}}],[\"其中t为测试用例数\",{\"1\":{\"217\":1}}],[\"其中∥⋅∥f​是frobenius范数\",{\"1\":{\"268\":1}}],[\"其中e\",{\"1\":{\"267\":1}}],[\"其中\",{\"1\":{\"244\":1,\"259\":2,\"262\":1,\"267\":1,\"268\":1}}],[\"其中l为字符串长度\",{\"1\":{\"204\":1}}],[\"其中内层中括号标记运算顺序\",{\"1\":{\"185\":1}}],[\"其中x的大小为s×b×h\",{\"1\":{\"148\":1}}],[\"其中m为模型大小\",{\"1\":{\"76\":1}}],[\"其中m和b分别为模型规模和\",{\"1\":{\"75\":1}}],[\"其中k取决于模型\",{\"1\":{\"55\":1}}],[\"高效去噪\",{\"1\":{\"276\":1}}],[\"高效的cpu优化器\",{\"1\":{\"75\":1}}],[\"高估了gpu的存储性能\",{\"1\":{\"74\":1}}],[\"采用p2p通信\",{\"1\":{\"284\":1}}],[\"采用流水线并行方式来协调不同设备上的计算和通信\",{\"1\":{\"284\":1}}],[\"采用流水线并行方式来协调通信和计算\",{\"1\":{\"282\":1}}],[\"采用\",{\"1\":{\"245\":1}}],[\"采用megatron的模型并行方式\",{\"1\":{\"70\":1}}],[\"采取了两种优化措施\",{\"1\":{\"75\":1}}],[\"较大\",{\"1\":{\"75\":1}}],[\"防止通信瓶颈\",{\"1\":{\"75\":1}}],[\"防止cpu性能瓶颈\",{\"1\":{\"75\":1}}],[\"以适应这种混合并行的需求\",{\"1\":{\"293\":1}}],[\"以pipe\",{\"1\":{\"293\":1}}],[\"以保持区块间的相互作用\",{\"1\":{\"280\":1}}],[\"以近似ϵθ​\",{\"1\":{\"279\":1}}],[\"以准备下一步的完整激活atl​\",{\"1\":{\"279\":1}}],[\"以预测xt​中的相应噪声ϵt​\",{\"1\":{\"277\":1}}],[\"以获得最终的清晰图像x0​\",{\"1\":{\"277\":1}}],[\"以往方法\",{\"0\":{\"275\":1}}],[\"以供后续层使用\",{\"1\":{\"267\":1}}],[\"以自回归的方式快速生成一个包含\",{\"1\":{\"263\":1}}],[\"以矩阵乘法\",{\"1\":{\"262\":1}}],[\"以layernorm为例\",{\"1\":{\"261\":1}}],[\"以实现通信优化\",{\"1\":{\"259\":1}}],[\"以最小化开销\",{\"1\":{\"258\":1}}],[\"以最小化通信开销\",{\"1\":{\"245\":1}}],[\"以系统架构设计弥补框架能力不足\",{\"1\":{\"256\":1}}],[\"以数据并行的方式扩展整个集群的吞吐能力\",{\"1\":{\"247\":1}}],[\"以及提高模型的实用性和适应性\",{\"1\":{\"292\":1}}],[\"以及其后的所有草稿token\",{\"1\":{\"263\":1}}],[\"以及单次前向传播巨大的浮点运算量\",{\"1\":{\"241\":1}}],[\"以及在复杂单一样本场景下的混合并行策略\",{\"1\":{\"240\":1}}],[\"以alphafold3为例\",{\"1\":{\"240\":1}}],[\"以stable\",{\"1\":{\"240\":1}}],[\"以逐块的方式进行注意力和前馈网络计算\",{\"1\":{\"157\":1}}],[\"以使每个gpu接收完整的序列\",{\"1\":{\"150\":1}}],[\"以mlp为例\",{\"1\":{\"148\":1}}],[\"以节省内存\",{\"1\":{\"95\":1}}],[\"以在三个关键方面达到最优化\",{\"1\":{\"75\":1}}],[\"以处理本地\",{\"1\":{\"43\":1}}],[\"通信开销较大\",{\"1\":{\"283\":1}}],[\"通信开销随并行度增加而上升\",{\"1\":{\"245\":1}}],[\"通信成本会超过分布式计算带来的节省\",{\"1\":{\"275\":1}}],[\"通信\",{\"1\":{\"262\":1}}],[\"通信对部分结果求和\",{\"1\":{\"245\":1}}],[\"通信将结果拼接为完整的输出\",{\"1\":{\"245\":1}}],[\"通常只涉及一个gpu\",{\"1\":{\"275\":1}}],[\"通常取0\",{\"1\":{\"268\":1}}],[\"通常是batch\",{\"1\":{\"262\":1}}],[\"通常与\",{\"1\":{\"261\":1}}],[\"通常由一个负载均衡器\",{\"1\":{\"243\":1}}],[\"通常还会加一些约束条件\",{\"1\":{\"99\":1}}],[\"通常建议在gpu内存满足的情况下最大化模型并行\",{\"1\":{\"95\":1}}],[\"通常建议在集群上采用流水线并行\",{\"1\":{\"95\":1}}],[\"通常在一个节点内部采用不超过g张卡做张量并行\",{\"1\":{\"95\":1}}],[\"通常pipeline\",{\"1\":{\"93\":1}}],[\"通常张量并行只适用于一个multi\",{\"1\":{\"91\":1}}],[\"通过复用一步旧的特征图来为当前步骤提供上下文\",{\"1\":{\"282\":1}}],[\"通过分析\",{\"1\":{\"268\":1}}],[\"通过分析确定了cpu和gpu设备之间的最佳计算和数据划分策略\",{\"1\":{\"75\":1}}],[\"通过这个公式我们可以轻易发现xt−1​和xt​的强依赖关系\",{\"1\":{\"277\":1}}],[\"通过这两次通信\",{\"1\":{\"267\":1}}],[\"通过这种方式\",{\"1\":{\"266\":1,\"268\":1}}],[\"通过这种\",{\"1\":{\"259\":1}}],[\"通过在tp通信的间隙插入沿序列维度的\",{\"1\":{\"261\":1}}],[\"通过all\",{\"1\":{\"259\":1}}],[\"通过与张量并行协同的通信策略\",{\"1\":{\"254\":1}}],[\"通过降低权重和\",{\"1\":{\"250\":1}}],[\"通过一个\",{\"1\":{\"248\":1}}],[\"通过一个在hpc环境下部署cpu推理集群的工程案例\",{\"1\":{\"240\":1}}],[\"通过infiniband等高带宽网络传递激活值\",{\"1\":{\"247\":1}}],[\"通过\",{\"1\":{\"245\":2}}],[\"通过微批处理\",{\"1\":{\"244\":1}}],[\"通过模型副本\",{\"1\":{\"243\":1}}],[\"通过前沿案例研究\",{\"1\":{\"241\":1}}],[\"通过代入n\",{\"1\":{\"185\":1}}],[\"通过斯特林公式我们可以简单估算阶乘的位数\",{\"1\":{\"185\":1}}],[\"通过将每个参数减去各自的梯度\",{\"1\":{\"174\":1}}],[\"通过将序列整体切分成小的chunk\",{\"1\":{\"154\":1}}],[\"通过求损失关于模型每个参数的偏导\",{\"1\":{\"174\":1}}],[\"通过给空闲的worker来fetch部分key\",{\"1\":{\"161\":1}}],[\"通过对比可以发现\",{\"1\":{\"145\":1}}],[\"通过编排forward\",{\"1\":{\"93\":1}}],[\"通过获取当前\",{\"1\":{\"89\":1}}],[\"通过结合上述两种方法\",{\"1\":{\"89\":1}}],[\"通讯\",{\"1\":{\"278\":1}}],[\"通讯效率低\",{\"1\":{\"47\":1}}],[\"通讯成本会比较高昂\",{\"1\":{\"35\":1}}],[\"与distrifusion相比\",{\"1\":{\"284\":1}}],[\"与此同时\",{\"1\":{\"279\":1}}],[\"与微批次数量m成反比\",{\"1\":{\"265\":1}}],[\"与mlp的第一层类似\",{\"1\":{\"260\":1}}],[\"与1d\",{\"1\":{\"255\":1,\"262\":1}}],[\"与流水线调度\",{\"1\":{\"244\":1}}],[\"与推测解码\",{\"1\":{\"240\":1}}],[\"与i\",{\"1\":{\"240\":1}}],[\"与标准字符串\",{\"1\":{\"201\":1}}],[\"与标准transformer相比\",{\"1\":{\"157\":1}}],[\"与已知的transformer架构一样\",{\"1\":{\"151\":1}}],[\"与\",{\"1\":{\"74\":1,\"76\":1}}],[\"与zero\",{\"1\":{\"70\":1}}],[\"尽量减少数据在\",{\"1\":{\"74\":1}}],[\"背景知识\",{\"0\":{\"277\":1}}],[\"背景\",{\"0\":{\"74\":1,\"156\":1,\"159\":1}}],[\"将最终输出聚合在一起\",{\"1\":{\"279\":1}}],[\"将执行两个异步操作\",{\"1\":{\"279\":1}}],[\"将图像分割成多个区块\",{\"1\":{\"278\":1,\"280\":1}}],[\"将图像split成n个patch\",{\"1\":{\"275\":1}}],[\"将归一化后的数据块yi​重新拼接成完整的输出张量y\",{\"1\":{\"267\":1}}],[\"将此输入一次性传入目标模型\",{\"1\":{\"263\":1}}],[\"将前缀和整个草稿序列拼接起来\",{\"1\":{\"263\":1}}],[\"将同一列的所有\",{\"1\":{\"262\":1}}],[\"将所有部分结果相加\",{\"1\":{\"260\":1}}],[\"将总共h个注意力头均分到p个gpu上\",{\"1\":{\"260\":1}}],[\"将第二个权重矩阵\",{\"1\":{\"259\":1}}],[\"将第一个权重矩阵\",{\"1\":{\"259\":1}}],[\"将会引入一次代价高昂的通信\",{\"1\":{\"259\":1}}],[\"将激活内存分散到多个设备上\",{\"1\":{\"254\":1}}],[\"将这些独立的组件视为流水线的不同阶段\",{\"1\":{\"253\":1}}],[\"将逻辑上连续的kv缓存序列\",{\"1\":{\"248\":1}}],[\"将模型的不同层\",{\"1\":{\"247\":1}}],[\"将模型切分成一连串stage\",{\"1\":{\"82\":1}}],[\"将q\",{\"1\":{\"245\":1}}],[\"将权重矩阵a按行切分\",{\"1\":{\"245\":1}}],[\"将权重矩阵a按列切分\",{\"1\":{\"245\":1}}],[\"将单个计算密集型算子\",{\"1\":{\"245\":1}}],[\"将字符转换为数组下标时的计算可能越界\",{\"1\":{\"215\":1}}],[\"将输入数字以字符串形式读入\",{\"1\":{\"208\":1}}],[\"将输入字符串转换为小写\",{\"1\":{\"201\":1}}],[\"将整个字符串转换为小写\",{\"1\":{\"203\":1}}],[\"将阶乘结果用表存储\",{\"1\":{\"184\":1}}],[\"将成为任何一台机器的噩梦\",{\"1\":{\"184\":1}}],[\"将原本一个batch的数据分解为多个更小的micro\",{\"1\":{\"93\":1}}],[\"将\",{\"1\":{\"89\":1,\"262\":1}}],[\"将weight矩阵a分割\",{\"1\":{\"89\":1}}],[\"将自己的\",{\"1\":{\"79\":1}}],[\"将更新后的参数复制回\",{\"1\":{\"78\":1}}],[\"将梯度和优化器状态在不同的\",{\"1\":{\"79\":1}}],[\"将梯度复制到\",{\"1\":{\"78\":1}}],[\"将梯度减少到拥有该层的进程\",{\"1\":{\"78\":1}}],[\"将fp16参数存储在gpu上\",{\"1\":{\"78\":1}}],[\"将该图分割为cpu和gpu设备之间的部分\",{\"1\":{\"76\":1}}],[\"将数据池分为两部分\",{\"1\":{\"72\":1}}],[\"将百度的\",{\"1\":{\"42\":1}}],[\"多gpu乃至多机dit部署是必然要求\",{\"1\":{\"285\":1}}],[\"多gpu通常仅用于批量推理\",{\"1\":{\"275\":1}}],[\"多维张量并行\",{\"1\":{\"255\":1}}],[\"多维度并行策略剖析\",{\"1\":{\"255\":1}}],[\"多机分布序列维度\",{\"1\":{\"157\":1}}],[\"多机集群\",{\"1\":{\"91\":1}}],[\"多线程不一定高效率\",{\"1\":{\"107\":1}}],[\"多个\",{\"1\":{\"98\":1}}],[\"多batch训练时\",{\"1\":{\"85\":1}}],[\"多batch可以提速\",{\"1\":{\"85\":1}}],[\"多卡策略\",{\"0\":{\"79\":1}}],[\"多次通讯的方式收集数据\",{\"1\":{\"62\":1}}],[\"多台机器之间通讯耗费的时间会不会比原先计算的时间更长\",{\"1\":{\"32\":1}}],[\"再进行比较\",{\"1\":{\"202\":1}}],[\"再进行通讯获取参数\",{\"1\":{\"60\":1}}],[\"再重新计算这些中间结果\",{\"1\":{\"86\":1}}],[\"再做聚合和同步\",{\"1\":{\"48\":1}}],[\"而\",{\"1\":{\"285\":1}}],[\"而序列长度有限\",{\"1\":{\"285\":1}}],[\"而dit\",{\"1\":{\"285\":1}}],[\"而不用\",{\"1\":{\"278\":1}}],[\"而不是直接访问\",{\"1\":{\"233\":1}}],[\"而不是简单统计1的个数\",{\"1\":{\"221\":1}}],[\"而不是\",{\"1\":{\"221\":1}}],[\"而不是像megtron\",{\"1\":{\"153\":1}}],[\"而权重在缩放后其动态范围被压缩\",{\"1\":{\"268\":1}}],[\"而无需任何通信\",{\"1\":{\"259\":1}}],[\"而且\",{\"1\":{\"185\":1}}],[\"而且只优化了自注意力部分\",{\"1\":{\"154\":1}}],[\"而非模型整体来说\",{\"1\":{\"157\":1}}],[\"而非对所有输入\",{\"1\":{\"84\":1}}],[\"而通信成本大大增长\",{\"1\":{\"92\":1}}],[\"而流水线并行则几乎只用于更大的模型\",{\"1\":{\"91\":1}}],[\"而其余的计算\",{\"1\":{\"76\":1}}],[\"而gpu需进行o\",{\"1\":{\"75\":1}}],[\"而是点对点通信\",{\"1\":{\"284\":1}}],[\"而是因为它们对应的输入通道\",{\"1\":{\"268\":1}}],[\"而是优先执行已就绪的反向计算任务\",{\"1\":{\"266\":1}}],[\"而是维护行列的映射关系\",{\"1\":{\"232\":1}}],[\"而是切分后再流水线并行\",{\"1\":{\"85\":1}}],[\"而是对输出进行通讯和更新\",{\"1\":{\"70\":1}}],[\"而是直接替换\",{\"1\":{\"41\":1}}],[\"而在其他gpu需要使用其进行计算时\",{\"1\":{\"60\":1}}],[\"两种并行方式的混合使用\",{\"1\":{\"293\":1}}],[\"两种并行都需要存储模型状态变量\",{\"1\":{\"60\":1}}],[\"两阶段\",{\"1\":{\"285\":1}}],[\"两次扩散时间步的图相似度是很高的\",{\"1\":{\"278\":1}}],[\"两者使用均不高效\",{\"1\":{\"60\":1}}],[\"因为前向和反向传播的间隔变长了\",{\"1\":{\"266\":1}}],[\"因为这些操作在纯tp中需要在每个gpu上保留完整的激活副本\",{\"1\":{\"261\":1}}],[\"因为m在循环中会减少\",{\"1\":{\"221\":1}}],[\"因为哈希数组大小固定\",{\"1\":{\"217\":1}}],[\"因为一般causal\",{\"1\":{\"161\":1}}],[\"因为最小训练单位还是一个token\",{\"1\":{\"157\":1}}],[\"因为我们在attention之前矩阵乘就已经切分了squence\",{\"1\":{\"157\":1}}],[\"因为每一个分发计算都需要完全copy所有数据\",{\"1\":{\"60\":1}}],[\"因为通讯更少\",{\"1\":{\"60\":1}}],[\"因此无法获得混合并行推理正确的结果\",{\"1\":{\"293\":1}}],[\"因此量化误差大大减小\",{\"1\":{\"268\":1}}],[\"因此保存fa的结果便可计算三者的矩阵\",{\"1\":{\"163\":1}}],[\"因此将其命名\",{\"1\":{\"157\":1}}],[\"因此这部分计算不需要与cpu进行通信\",{\"1\":{\"78\":1}}],[\"因此\",{\"1\":{\"48\":1,\"78\":1,\"208\":1,\"250\":1,\"262\":1}}],[\"因此不同机器之间处理的bucket的顺序将会不一致\",{\"1\":{\"48\":1}}],[\"也能保持很高的模型精度\",{\"1\":{\"268\":1}}],[\"也会成为瓶颈\",{\"1\":{\"261\":1}}],[\"也会是比较大的开销\",{\"1\":{\"184\":1}}],[\"也是被广泛认为最好的数学方法\",{\"1\":{\"173\":1}}],[\"也是cse234的课程要求\",{\"1\":{\"172\":1}}],[\"也就是端到端的计算能够正确的完成\",{\"1\":{\"93\":1}}],[\"也就是单计算仅需4ψ\",{\"1\":{\"55\":1}}],[\"也可以考虑每个gpu都各自保存和重算部分数据\",{\"1\":{\"62\":1}}],[\"也可以是一个batch\",{\"1\":{\"37\":1}}],[\"一次性快速生成多个候选token\",{\"1\":{\"251\":1}}],[\"一次生成一个token\",{\"1\":{\"251\":1}}],[\"一种基于近似二阶信息的单次权重压缩方法\",{\"1\":{\"250\":1}}],[\"一种方法是沿着其行\",{\"1\":{\"89\":1}}],[\"一旦某个请求完成\",{\"1\":{\"248\":1}}],[\"一旦梯度在\",{\"1\":{\"79\":1}}],[\"一层transformer的memory总的大小为\",{\"1\":{\"146\":1}}],[\"一般会通过重计算的方式降低显存占用\",{\"1\":{\"145\":1}}],[\"一般来说\",{\"1\":{\"55\":1}}],[\"一个输入\",{\"1\":{\"292\":1}}],[\"一个阶段在完成某个微批次的前向计算后\",{\"1\":{\"266\":1}}],[\"一个规模小得多\",{\"1\":{\"263\":1}}],[\"一个完整的mlp层只在最后需要一次all\",{\"1\":{\"259\":1}}],[\"一个标准的transformer\",{\"1\":{\"259\":1}}],[\"一个数的奇偶性只取决于其最后一位数字\",{\"1\":{\"208\":1}}],[\"一个\",{\"1\":{\"192\":1}}],[\"一个linear\",{\"1\":{\"146\":1}}],[\"一个高效的并行策略搜索引擎\",{\"1\":{\"97\":1}}],[\"一个更加全面的并行策略搜索空间\",{\"1\":{\"97\":1}}],[\"一部分存储大批量的计算数据\",{\"1\":{\"72\":1}}],[\"一张gpu\",{\"1\":{\"33\":1}}],[\"要求输出一个特定的字符串\",{\"1\":{\"192\":1}}],[\"要是用递归或者循环写阶乘\",{\"1\":{\"184\":1}}],[\"要注意分割的时候会不会漏行\",{\"1\":{\"107\":1}}],[\"要在合理时间内找到最优解依然比较困难\",{\"1\":{\"99\":1}}],[\"要服务于计算速度内存变化就会发生膨胀\",{\"1\":{\"55\":1}}],[\"要用多少张卡进行并行\",{\"1\":{\"35\":1}}],[\"看上去这是仅仅由1变2的内存保留变化\",{\"1\":{\"55\":1}}],[\"论文方法\",{\"0\":{\"160\":1}}],[\"论文背景\",{\"0\":{\"153\":1}}],[\"论文做如下符号说明\",{\"1\":{\"148\":1}}],[\"论文基于当前的\",{\"1\":{\"101\":1}}],[\"论文提出从\",{\"1\":{\"98\":1}}],[\"论文提出了一种名为efficiency的offload策略\",{\"1\":{\"75\":1}}],[\"论文中提出token\",{\"1\":{\"161\":1}}],[\"论文中提到的mp就是传统的mp方案\",{\"1\":{\"84\":1}}],[\"论文中所提到的dp背景本篇blog前文均有提及\",{\"1\":{\"46\":1}}],[\"论文假设有30\",{\"1\":{\"58\":1}}],[\"论文以adam优化器为例说明了模型造成的内存浪费是一件难以接受的事情\",{\"1\":{\"55\":1}}],[\"临时缓冲区和不可用的碎片内存所消耗\",{\"1\":{\"53\":1}}],[\"剩余内存被激活\",{\"1\":{\"53\":1}}],[\"大幅提升吞吐量\",{\"1\":{\"248\":1}}],[\"大规模ai模型的infer并行计算范式\",{\"0\":{\"239\":1}}],[\"大数处理技巧\",{\"1\":{\"236\":1}}],[\"大数处理方式\",{\"1\":{\"209\":1}}],[\"大家可以简单跑一下这个程序\",{\"1\":{\"184\":1}}],[\"大家都对ml有一些基本的认识了\",{\"1\":{\"35\":1}}],[\"大小为h×4h和4h×h\",{\"1\":{\"148\":1}}],[\"大小为8sbh\",{\"1\":{\"146\":1}}],[\"大小也是2sbh\",{\"1\":{\"146\":1}}],[\"大多数\",{\"1\":{\"99\":1}}],[\"大op\",{\"1\":{\"98\":1}}],[\"大模型即使的带宽小的情况下\",{\"1\":{\"62\":1}}],[\"大部分内存都被用于模型本身数据的存储和运算\",{\"1\":{\"54\":1}}],[\"大部分内存\",{\"1\":{\"53\":1}}],[\"简单灵活的开发接口\",{\"0\":{\"289\":1}}],[\"简单对某个x取极限并没有意义\",{\"1\":{\"175\":1}}],[\"简单增加机器数量也用处不大\",{\"1\":{\"53\":1}}],[\"简介\",{\"0\":{\"98\":1,\"150\":1}}],[\"简称sp\",{\"1\":{\"145\":1}}],[\"简称pp\",{\"1\":{\"35\":1}}],[\"简称fsdp\",{\"1\":{\"35\":1}}],[\"简称tp\",{\"1\":{\"35\":1}}],[\"简称mp\",{\"1\":{\"35\":1}}],[\"简称dp\",{\"1\":{\"35\":1}}],[\"巨量参数可以大幅提升nlp处理能力\",{\"1\":{\"53\":1}}],[\"零冗余优化器\",{\"1\":{\"52\":1}}],[\"现代框架如megatron\",{\"1\":{\"247\":1}}],[\"现在使用的最多的\",{\"1\":{\"173\":1}}],[\"现在假定我们要做一个训练任务\",{\"1\":{\"35\":1}}],[\"现有的序列并行方法依托内存通讯\",{\"1\":{\"150\":1}}],[\"现有的dp\",{\"1\":{\"150\":1}}],[\"现有的pp\",{\"1\":{\"53\":1}}],[\"现有系统在模型训练中内存消耗的全部范围\",{\"1\":{\"53\":1}}],[\"现有dp\",{\"1\":{\"52\":1}}],[\"对a\",{\"1\":{\"279\":1}}],[\"对a进行列切分的tensor并行\",{\"1\":{\"148\":1}}],[\"对缩放后的w\",{\"1\":{\"268\":1}}],[\"对权重矩阵w的每一列j\",{\"1\":{\"268\":1}}],[\"对权重矩阵进行行或列切分\",{\"1\":{\"245\":1}}],[\"对x的每个输入通道进行缩放\",{\"1\":{\"268\":1}}],[\"对x按sequence维度切分\",{\"1\":{\"148\":1}}],[\"对模型性能影响最大的是那些接收到大幅度激活值的权重\",{\"1\":{\"250\":1}}],[\"对配置好的\",{\"1\":{\"247\":1}}],[\"对计算单元构成严峻考验\",{\"1\":{\"241\":1}}],[\"对每个通道j\",{\"1\":{\"268\":1}}],[\"对每次操作\",{\"1\":{\"226\":1}}],[\"对每一天统计支持规则k的人数\",{\"1\":{\"220\":1}}],[\"对某个参数求数值偏导的定义\",{\"1\":{\"175\":1}}],[\"对squence做切分时的原则\",{\"1\":{\"157\":1}}],[\"对b进行行切分的tensor并行\",{\"1\":{\"148\":1}}],[\"对通信\",{\"1\":{\"91\":1}}],[\"对数据按层切分\",{\"1\":{\"84\":1}}],[\"对数据并行中存储的fp16参数进行切分\",{\"1\":{\"68\":1}}],[\"对数据并行中存储的fp16梯度进行切分\",{\"1\":{\"67\":1}}],[\"对应输入通道j\",{\"1\":{\"268\":1}}],[\"对应输入的元素大小为2sbh\",{\"1\":{\"146\":1}}],[\"对应大小\",{\"1\":{\"146\":1}}],[\"对应的计算公式如下\",{\"1\":{\"148\":1}}],[\"对应的嵌入向量处理成\",{\"1\":{\"89\":1}}],[\"对应的每个\",{\"1\":{\"79\":1}}],[\"对应位置数据不再做相加\",{\"1\":{\"41\":1}}],[\"对切分后部分输入分开运算\",{\"1\":{\"70\":1}}],[\"对优化器本身存储的fp32数据进行切分\",{\"1\":{\"66\":1}}],[\"对于步数极少的方法\",{\"1\":{\"280\":1}}],[\"对于低分辨率图像\",{\"1\":{\"280\":1}}],[\"对于每一层l和每个设备i\",{\"1\":{\"279\":1}}],[\"对于每个字符\",{\"1\":{\"214\":1}}],[\"对于输入x\",{\"1\":{\"268\":1}}],[\"对于输入张量x中的每一个特征向量x\",{\"1\":{\"267\":1}}],[\"对于这些\",{\"1\":{\"268\":1}}],[\"对于一个线性层\",{\"1\":{\"268\":1}}],[\"对于一个元素的mask只用1个bytes\",{\"1\":{\"146\":1}}],[\"对于第j$个输入通道\",{\"1\":{\"268\":1}}],[\"对于第\",{\"1\":{\"263\":1}}],[\"对于模型中特别宽的层\",{\"1\":{\"255\":1}}],[\"对于那些在tp中无法拆分的操作\",{\"1\":{\"254\":1}}],[\"对于上式求导的含义我们不再赘述\",{\"1\":{\"175\":1}}],[\"对于单卡内存需求来说\",{\"1\":{\"157\":1}}],[\"对于单个\",{\"1\":{\"98\":3}}],[\"对于内循环\",{\"1\":{\"157\":1}}],[\"对于内存使用和kernel计算效率\",{\"1\":{\"91\":1}}],[\"对于非tp并行的部分在sequence维度都是相互独立的\",{\"1\":{\"148\":1}}],[\"对于两块的输入并没有并行操作\",{\"1\":{\"147\":1}}],[\"对于self\",{\"1\":{\"146\":1}}],[\"对于linear需要保存输入的activation大小为2sbh\",{\"1\":{\"146\":1}}],[\"对于attention\",{\"1\":{\"146\":1}}],[\"对于attention块来说\",{\"1\":{\"146\":1}}],[\"对于attention模块\",{\"1\":{\"146\":1}}],[\"对于多个gpu集群\",{\"1\":{\"95\":1}}],[\"对于临时缓存采用开一段固定大小内存的方式进行反复存储\",{\"1\":{\"63\":1}}],[\"对于大型模型\",{\"1\":{\"53\":1}}],[\"对于没有参与计算的参数\",{\"1\":{\"48\":1}}],[\"哪些参数没有参与计算\",{\"1\":{\"48\":1}}],[\"前一步的陈旧激活被加深了颜色\",{\"1\":{\"279\":1}}],[\"前\",{\"1\":{\"265\":1}}],[\"前文提及的基础操作还有进一步优化的空间\",{\"1\":{\"159\":1}}],[\"前向传播逻辑\",{\"1\":{\"78\":1}}],[\"前向传播\",{\"1\":{\"78\":1}}],[\"前向传播结束后从输出开始遍历计算图\",{\"1\":{\"48\":1}}],[\"前面所提及的方法\",{\"1\":{\"35\":1}}],[\"改进思路也不同\",{\"1\":{\"285\":1}}],[\"改进做法\",{\"0\":{\"48\":1}}],[\"改进了\",{\"1\":{\"42\":1}}],[\"传统分批方法\",{\"1\":{\"275\":1}}],[\"传统推理采用静态批处理\",{\"1\":{\"248\":1}}],[\"传统做法\",{\"0\":{\"47\":1}}],[\"传统的dp在带来使用大数据集可能的同时\",{\"1\":{\"38\":1}}],[\"不再像distrifusion每个时间步的通信都是scatter给每个设备\",{\"1\":{\"284\":1}}],[\"不再赘述\",{\"1\":{\"46\":1}}],[\"不仅取决于权重本身\",{\"1\":{\"268\":1}}],[\"不是立即开始下一个微批次的前向\",{\"1\":{\"266\":1}}],[\"不是magatron\",{\"1\":{\"84\":1}}],[\"不兼容\",{\"1\":{\"256\":1}}],[\"不直接修改矩阵\",{\"1\":{\"232\":1}}],[\"不能边替换边比较\",{\"1\":{\"227\":1}}],[\"不区分大小写\",{\"1\":{\"199\":1}}],[\"不幸的是过度训练可能导致参数在某一个方向上下降过多\",{\"1\":{\"174\":1}}],[\"不做赘述\",{\"1\":{\"156\":1}}],[\"不够高效\",{\"1\":{\"150\":1}}],[\"不包含模型参数大小和优化器中状态大小\",{\"1\":{\"146\":1}}],[\"不难发现只要我们给出startrow\",{\"1\":{\"108\":1}}],[\"不需要引入太多通讯开销\",{\"1\":{\"276\":1}}],[\"不需要再补充\",{\"1\":{\"215\":1}}],[\"不需要线程之间进行响应和同步\",{\"1\":{\"107\":1}}],[\"不需要进行额外通讯\",{\"1\":{\"89\":1}}],[\"不过由于我们使用的a是同一个\",{\"1\":{\"89\":1}}],[\"不同并行方式混合在一起变得尤为重要\",{\"1\":{\"293\":1}}],[\"不同阶段部署在不同设备上\",{\"1\":{\"244\":1}}],[\"不同属性如何并行\",{\"1\":{\"98\":1}}],[\"不同gpu之间需要进行定期同步\",{\"1\":{\"93\":1}}],[\"不同的并行策略\",{\"1\":{\"91\":1}}],[\"不同参数的梯度在后向调度的不同位置计算\",{\"1\":{\"78\":1}}],[\"不同迭代中\",{\"1\":{\"48\":1}}],[\"不得不保存的模型内容\",{\"1\":{\"53\":1}}],[\"不切分数据的方式\",{\"1\":{\"35\":1}}],[\"就能实现显著加速\",{\"1\":{\"263\":1}}],[\"就直接判断\",{\"1\":{\"209\":1}}],[\"就是distrifusion\",{\"1\":{\"292\":1}}],[\"就是采用高精度的算法\",{\"1\":{\"184\":1}}],[\"就是将单一计算机节点要做的任务分布在多个计算机节点上完成的\",{\"1\":{\"32\":1}}],[\"就已经溢出了\",{\"1\":{\"184\":1}}],[\"就可以完成计算了\",{\"1\":{\"108\":1}}],[\"就可以解决这个问题\",{\"1\":{\"107\":1}}],[\"就可以实现从本机训练到分布式训练的部署\",{\"1\":{\"45\":1}}],[\"就放在第\",{\"1\":{\"84\":1}}],[\"就调用allreduce对该bucket进行通信\",{\"1\":{\"48\":1}}],[\"9\",{\"1\":{\"45\":1}}],[\"80g\",{\"1\":{\"145\":1}}],[\"8080\",{\"1\":{\"26\":2}}],[\"83185307179586\",{\"1\":{\"118\":1}}],[\"8倍加速比的效果\",{\"1\":{\"107\":1}}],[\"8个线程\",{\"1\":{\"107\":1}}],[\"8位指数位\",{\"1\":{\"55\":2}}],[\"8\",{\"1\":{\"45\":1}}],[\"7的fresh\",{\"1\":{\"293\":1}}],[\"763\",{\"1\":{\"277\":1}}],[\"71828182846\",{\"1\":{\"186\":1}}],[\"718613\",{\"1\":{\"8\":1}}],[\"7位尾数位\",{\"1\":{\"55\":1}}],[\"7\",{\"0\":{\"230\":1},\"1\":{\"45\":1,\"107\":1,\"216\":1}}],[\"652206513\",{\"1\":{\"149\":1}}],[\"6sbh\",{\"1\":{\"147\":1}}],[\"6370614359173\",{\"1\":{\"119\":1}}],[\"62\",{\"1\":{\"118\":1}}],[\"6\",{\"0\":{\"224\":1},\"1\":{\"45\":1,\"107\":1,\"236\":1}}],[\"保留旧有激活值避免大量通讯\",{\"1\":{\"293\":1}}],[\"保证带宽不浪费\",{\"1\":{\"71\":1}}],[\"保证各个计算gpu模型同步\",{\"1\":{\"37\":1}}],[\"保存到检查点以及在完成或发生错误时关闭\",{\"1\":{\"43\":1}}],[\"从纯高斯噪声xt​∼n\",{\"1\":{\"277\":1}}],[\"从权重转移到了激活上\",{\"1\":{\"268\":1}}],[\"从\",{\"1\":{\"263\":1}}],[\"从草稿的第一个token开始\",{\"1\":{\"263\":1}}],[\"从下图中我们可以看出\",{\"1\":{\"89\":1}}],[\"从而消除流水线中的等待时间\",{\"1\":{\"282\":1}}],[\"从而消除gpu空闲\",{\"1\":{\"248\":1}}],[\"从而使得通信可以被隐藏在后续层的计算过程中\",{\"1\":{\"278\":1}}],[\"从而在极低的位宽下\",{\"1\":{\"268\":1}}],[\"从而导致显著的性能下降\",{\"1\":{\"268\":1}}],[\"从而让前向和反向计算重叠\",{\"1\":{\"266\":1}}],[\"从而容纳更大的模型\",{\"1\":{\"255\":1}}],[\"从而支持更长的序列训练和推理\",{\"1\":{\"254\":1}}],[\"从而最大化图像生成吞吐量\",{\"1\":{\"253\":1}}],[\"从而实现加速\",{\"1\":{\"251\":1}}],[\"从而实现并发计算和通信\",{\"1\":{\"157\":1}}],[\"从而提升硬件利用率\",{\"1\":{\"244\":1}}],[\"从而降低内存占用\",{\"1\":{\"60\":1}}],[\"从而保证通讯无boundary\",{\"1\":{\"35\":1}}],[\"从检查点恢复\",{\"1\":{\"43\":1}}],[\"固定j\",{\"1\":{\"262\":1}}],[\"固定的情况下\",{\"1\":{\"243\":1}}],[\"固定\",{\"1\":{\"43\":1}}],[\"初始化权重\",{\"1\":{\"89\":1}}],[\"初始化每个进程的层\",{\"1\":{\"78\":1}}],[\"初始化\",{\"1\":{\"43\":1}}],[\"操作将所有gpu的局部平方和相加\",{\"1\":{\"267\":1}}],[\"操作将所有gpu的局部总和相加\",{\"1\":{\"267\":1}}],[\"操作来聚合局部统计量\",{\"1\":{\"261\":1}}],[\"操作完成\",{\"1\":{\"259\":1,\"261\":1}}],[\"操作符的共置\",{\"1\":{\"99\":1}}],[\"操作视为对输入进行索引操作\",{\"1\":{\"89\":1}}],[\"操作\",{\"1\":{\"42\":1,\"79\":1,\"89\":3,\"261\":2,\"262\":1}}],[\"添加hook\",{\"1\":{\"43\":1}}],[\"添加\",{\"1\":{\"42\":1,\"43\":1}}],[\"添加了对单机多卡的支持\",{\"1\":{\"42\":1}}],[\"并依次送入卷积运算符\",{\"1\":{\"294\":1}}],[\"并不依赖于其他区块的最新激活信息\",{\"1\":{\"278\":1}}],[\"并使用张量并行进行推理的大通讯量的不可行性\",{\"1\":{\"277\":1}}],[\"并使用模型并行操作来优化\",{\"1\":{\"89\":1}}],[\"并利用成熟的指数积分器来减少采样步骤\",{\"1\":{\"276\":1}}],[\"并在不同设备上进行序列并行\",{\"1\":{\"294\":1}}],[\"并在潜在空间中学习扩散模型\",{\"1\":{\"276\":1}}],[\"并在后续针对数据结果决定是否要进行优化\",{\"1\":{\"108\":1}}],[\"并尽早开始反向传播\",{\"1\":{\"266\":1}}],[\"并继续验证下一个token\",{\"1\":{\"263\":1}}],[\"并分发给设备网格的各列\",{\"1\":{\"262\":1}}],[\"并分发给设备网格的各行\",{\"1\":{\"262\":1}}],[\"并独立计算其部分结果\",{\"1\":{\"259\":1}}],[\"并应用了\",{\"1\":{\"259\":1}}],[\"并非所有权重都同等重要\",{\"1\":{\"250\":1}}],[\"并探讨其在推理场景下的应用边界与理论局限\",{\"1\":{\"240\":1}}],[\"并且梦到了一个帅气的男孩纸\",{\"1\":{\"183\":1}}],[\"并且每个\",{\"1\":{\"79\":1}}],[\"并通过key\",{\"1\":{\"157\":1}}],[\"并随机替换被选择的\",{\"1\":{\"101\":1}}],[\"并沿着其列\",{\"1\":{\"89\":1}}],[\"并将每个区块分配给独立的gpu进行处理\",{\"1\":{\"280\":1}}],[\"并将独立且并行地处理一个单独的补丁\",{\"1\":{\"279\":1}}],[\"并将输入数据集切分为多个片段\",{\"1\":{\"243\":1}}],[\"并将更新后的\",{\"1\":{\"78\":1}}],[\"并将其分为两部分\",{\"1\":{\"53\":1}}],[\"并减少\",{\"1\":{\"74\":1}}],[\"并优化了性能\",{\"1\":{\"42\":1}}],[\"并行处理\",{\"1\":{\"243\":1}}],[\"并行\",{\"1\":{\"151\":1}}],[\"并行部分有mlp的linear部分\",{\"1\":{\"147\":1}}],[\"并行度为t\",{\"1\":{\"147\":1}}],[\"并行计算\",{\"1\":{\"260\":1},\"2\":{\"110\":1}}],[\"并行运算\",{\"2\":{\"103\":1,\"165\":1,\"270\":1}}],[\"并行运算感兴趣\",{\"1\":{\"0\":1}}],[\"并行使用的gpu的数量\",{\"1\":{\"93\":1}}],[\"并行中使用的参数如何调整\",{\"1\":{\"35\":1}}],[\"库实现了\",{\"1\":{\"42\":1}}],[\"命名为\",{\"1\":{\"42\":1}}],[\"包括流水线并行和张量并行\",{\"1\":{\"95\":1}}],[\"包括优化器状态\",{\"1\":{\"53\":1}}],[\"包\",{\"1\":{\"42\":1}}],[\"进一步将那些在tp中未被切分\",{\"1\":{\"261\":1}}],[\"进行量化\",{\"1\":{\"268\":1}}],[\"进行前向计算\",{\"1\":{\"263\":1}}],[\"进行线性变换\",{\"1\":{\"260\":1}}],[\"进行切分\",{\"1\":{\"260\":1,\"261\":1}}],[\"进行一次并行的验证\",{\"1\":{\"251\":1}}],[\"进行垂直切分\",{\"1\":{\"244\":1}}],[\"进行全局的\",{\"1\":{\"151\":1}}],[\"进行all\",{\"1\":{\"148\":1}}],[\"进行空间分解\",{\"1\":{\"107\":1}}],[\"进行数据计算\",{\"1\":{\"35\":1}}],[\"进而生成\",{\"1\":{\"101\":1}}],[\"进程直接在\",{\"1\":{\"79\":1}}],[\"进程\",{\"1\":{\"48\":1}}],[\"进入all\",{\"1\":{\"40\":1}}],[\"使每个gpu仅留1\",{\"1\":{\"66\":1,\"67\":1,\"68\":1}}],[\"使我们能够根据设备数量按比例扩展模型大小\",{\"1\":{\"52\":1}}],[\"使得w\",{\"1\":{\"268\":1}}],[\"使得单个设备的显存占用从o\",{\"1\":{\"255\":1}}],[\"使得系统能够容纳更多的并发请求\",{\"1\":{\"248\":1}}],[\"使得每一个chunk都尽可能大\",{\"1\":{\"154\":1}}],[\"使得每个gpu只和其相邻的两块gpu通讯\",{\"1\":{\"40\":1}}],[\"使得参数额外开销尽可能减少\",{\"1\":{\"68\":1}}],[\"使得某些参数的梯度不需要用到\",{\"1\":{\"48\":1}}],[\"使用了两种关键策略\",{\"1\":{\"294\":1}}],[\"使用diffusion\",{\"1\":{\"272\":1}}],[\"使用更好的solver\",{\"1\":{\"272\":1}}],[\"使用一些通用优化手段\",{\"1\":{\"272\":1}}],[\"使用一小部分校准数据对模型进行前向传播\",{\"1\":{\"268\":1}}],[\"使用一个目标函数\",{\"1\":{\"174\":1}}],[\"使用草稿模型\",{\"1\":{\"263\":1}}],[\"使用其本地的激活结果\",{\"1\":{\"259\":1}}],[\"使用pp\",{\"1\":{\"247\":1}}],[\"使用tp\",{\"1\":{\"247\":1}}],[\"使用哈希表优化统计\",{\"1\":{\"236\":1}}],[\"使用哈希数组统计每个字符的出现次数\",{\"1\":{\"214\":1}}],[\"使用引用或指针修改字符串时的正确性\",{\"1\":{\"227\":1}}],[\"使用排序来简化判断过半的操作\",{\"1\":{\"220\":1}}],[\"使用16个线程运行改进后代码\",{\"1\":{\"107\":1}}],[\"使用固定大小buffer指定数据的单批发送量\",{\"1\":{\"71\":1}}],[\"使用reduce\",{\"1\":{\"70\":1}}],[\"使用时通讯获取完整参数\",{\"1\":{\"68\":1}}],[\"使用的参数可能不相同\",{\"1\":{\"48\":1}}],[\"使用参数的反序作为梯度放入bucket的顺序\",{\"1\":{\"48\":1}}],[\"使用方法\",{\"0\":{\"43\":1}}],[\"使用\",{\"1\":{\"42\":1,\"267\":2,\"292\":1}}],[\"使用尽可能多的参数\",{\"1\":{\"33\":1}}],[\"可拆分的维度并不一样\",{\"1\":{\"99\":1}}],[\"可以提供更广泛的条件控制\",{\"1\":{\"292\":1}}],[\"可以达到6倍以上加速\",{\"1\":{\"274\":1}}],[\"可以采用2d或2\",{\"1\":{\"255\":1}}],[\"可以极大地重叠计算\",{\"1\":{\"253\":1}}],[\"可以通过以下方程推导出xt−1​\",{\"1\":{\"277\":1}}],[\"可以通过在序列维度上切分\",{\"1\":{\"254\":1}}],[\"可以通过计算对比来估计一下斯特林公式算出结果\",{\"1\":{\"185\":1}}],[\"可以通过fa的输出结果完成更新\",{\"1\":{\"163\":1}}],[\"可以认为是这样的\",{\"1\":{\"175\":1}}],[\"可以考虑采用4d并行\",{\"1\":{\"157\":1}}],[\"可以结合tp有效减少不必要的计算量\",{\"1\":{\"145\":1}}],[\"可以简单分析一下mandelbrotserial\",{\"1\":{\"108\":1}}],[\"可以准确预测并行策略的性能\",{\"1\":{\"97\":1}}],[\"可以得到以下结论\",{\"1\":{\"95\":1}}],[\"可以将\",{\"1\":{\"89\":1}}],[\"可以在transformer中实现简单并发\",{\"1\":{\"89\":1}}],[\"可以切为\",{\"1\":{\"84\":1}}],[\"可以转移到cpu上\",{\"1\":{\"76\":1}}],[\"可以明显看到gpu数量增多时\",{\"1\":{\"38\":1}}],[\"可证明地最大化节约gpu内存\",{\"1\":{\"75\":1}}],[\"可能导致gpu计算核心利用率下降\",{\"1\":{\"245\":1}}],[\"可能会导致梯度更新不稳定\",{\"1\":{\"85\":1}}],[\"可能内存池的维护并不能做到完全利用\",{\"1\":{\"58\":1}}],[\"可能并不会产生权重累计\",{\"1\":{\"55\":1}}],[\"可能在模型训练过程中梯度不大等原因\",{\"1\":{\"55\":1}}],[\"可能大幅度提升计算效率\",{\"1\":{\"32\":1}}],[\"又称过拟合\",{\"1\":{\"174\":1}}],[\"又同时增加了额外的通讯开销\",{\"1\":{\"38\":1}}],[\"又切分模型的参数\",{\"1\":{\"35\":1}}],[\"聚合再下发梯度操作被称为allreduce\",{\"1\":{\"37\":1}}],[\"把\",{\"1\":{\"89\":1}}],[\"把当前卡上不要的\",{\"1\":{\"89\":1}}],[\"把本地梯度push到梯度收集gpu\",{\"1\":{\"37\":1}}],[\"把一份data\",{\"1\":{\"37\":1}}],[\"每步需要6\",{\"1\":{\"277\":1}}],[\"每道题都体现了一些重要的编程思想\",{\"1\":{\"236\":1}}],[\"每pipeline中的microbatch的数量\",{\"1\":{\"95\":1}}],[\"每块\",{\"1\":{\"84\":1}}],[\"每块gpu上都有一块数据拥有了对应位置完整的聚合\",{\"1\":{\"40\":1}}],[\"每块gpu上的数据也对应被切成4份\",{\"1\":{\"39\":1}}],[\"每块gpu均拷贝一份完整的模型\",{\"1\":{\"37\":1}}],[\"每份\",{\"1\":{\"84\":1}}],[\"每个micro\",{\"1\":{\"293\":2}}],[\"每个工作单元\",{\"1\":{\"243\":1}}],[\"每个用例包含一个字符串\",{\"1\":{\"199\":1}}],[\"每个内存存有限位数据\",{\"1\":{\"184\":1}}],[\"每个设备都拥有模型ϵθ​的一个副本\",{\"1\":{\"279\":1}}],[\"每个设备将用于compute的key\",{\"1\":{\"157\":1}}],[\"每个设备计算其各自的self\",{\"1\":{\"157\":1}}],[\"每个主机将key\",{\"1\":{\"157\":1}}],[\"每个主机设备具有一个query\",{\"1\":{\"157\":1}}],[\"每个头的注意力计算形式为\",{\"1\":{\"151\":1}}],[\"每个本地n\",{\"1\":{\"151\":1}}],[\"每个layernorm层的输入需要2sbh大小\",{\"1\":{\"146\":1}}],[\"每个矩阵元素总大小为2sbh\",{\"1\":{\"146\":1}}],[\"每个元素以半精度\",{\"1\":{\"146\":1}}],[\"每个点的迭代次数\",{\"1\":{\"108\":1}}],[\"每个stage放在独立的设备\",{\"1\":{\"82\":1}}],[\"每个\",{\"1\":{\"79\":1}}],[\"每个梯度传输可以与反向计算重叠\",{\"1\":{\"78\":1}}],[\"每个gpu使用全局的μ和σ2来归一化其本地数据块xi​\",{\"1\":{\"267\":1}}],[\"每个gpu使用同步后的全局μ和σ2来归一化其本地数据块xi​\",{\"1\":{\"261\":1}}],[\"每个gpu持有$\",{\"1\":{\"267\":1}}],[\"每个gpu计算一个部分投影结果\",{\"1\":{\"260\":1}}],[\"每个gpu负责h\",{\"1\":{\"260\":1}}],[\"每个gpu\",{\"1\":{\"259\":1,\"260\":3,\"261\":1,\"262\":1,\"267\":2}}],[\"每个gpu都拥有完整的输入x\",{\"1\":{\"259\":1}}],[\"每个gpu都没有保存或通讯获取完整的参数\",{\"1\":{\"70\":1}}],[\"每个gpu独立计算其分配到的注意力头的得分和输出\",{\"1\":{\"245\":1}}],[\"每个gpu保存1\",{\"1\":{\"70\":1}}],[\"每个进程一个\",{\"1\":{\"43\":1}}],[\"每次交换只需要修改映射数组\",{\"1\":{\"232\":1}}],[\"每次操作可以交换两行或两列\",{\"1\":{\"231\":1}}],[\"每次操作必须同时替换所有匹配的字符\",{\"1\":{\"227\":1}}],[\"每次操作将字符串中的某个字符全部替换为另一个字符\",{\"1\":{\"225\":1}}],[\"每次都会有ψ\",{\"1\":{\"68\":1}}],[\"每次发送对应位置的数据进行累加\",{\"1\":{\"40\":1}}],[\"每一个集群上的节点假如包含g个gpu卡\",{\"1\":{\"95\":1}}],[\"每一列代表一个时间段\",{\"1\":{\"84\":1}}],[\"每一种颜色代表一块\",{\"1\":{\"84\":1}}],[\"每一次累加更新都形成一个拓扑环\",{\"1\":{\"40\":1}}],[\"每一块gpu完成forward和backward后\",{\"1\":{\"37\":1}}],[\"分块输入处理\",{\"1\":{\"294\":1}}],[\"分析阶段\",{\"1\":{\"268\":1}}],[\"分片\",{\"1\":{\"262\":2}}],[\"分解到多个设备上\",{\"1\":{\"258\":1}}],[\"分页注意力\",{\"1\":{\"248\":1}}],[\"分布在不同服务器节点上\",{\"1\":{\"247\":1}}],[\"分布式推理\",{\"2\":{\"296\":1}}],[\"分布式均值和方差的计算\",{\"1\":{\"267\":1}}],[\"分布式应用等问题\",{\"1\":{\"160\":1}}],[\"分布式优化器\",{\"1\":{\"43\":1}}],[\"分布式系统\",{\"2\":{\"103\":1,\"165\":1,\"270\":1,\"296\":1}}],[\"分布式系统如何发挥作用\",{\"0\":{\"34\":1}}],[\"分布式系统顾名思义\",{\"1\":{\"32\":1}}],[\"分布式开发\",{\"0\":{\"30\":1,\"142\":1},\"1\":{\"291\":1}}],[\"分配到不同gpu上\",{\"1\":{\"245\":1}}],[\"分配n块计算gpu\",{\"1\":{\"37\":1}}],[\"分别是compute\",{\"1\":{\"285\":1}}],[\"分别代表第一个和第二个块\",{\"1\":{\"279\":1}}],[\"分别针对cpu环境下的内存带宽和单样本延迟瓶颈进行深度优化\",{\"1\":{\"256\":1}}],[\"分别部署在不同的硬件单元上\",{\"1\":{\"253\":1}}],[\"分别存储输入矩阵大小为2sbh\",{\"1\":{\"146\":1}}],[\"分别计算图像的上下两个部分\",{\"1\":{\"107\":1}}],[\"分别表示pp\",{\"1\":{\"95\":1}}],[\"分别表示第\",{\"1\":{\"84\":2}}],[\"分发到不同的设备上\",{\"1\":{\"99\":1}}],[\"分割a\",{\"1\":{\"89\":1}}],[\"分开重算\",{\"1\":{\"62\":1}}],[\"典型数据并行的流程\",{\"0\":{\"37\":1}}],[\"是\",{\"1\":{\"293\":1}}],[\"是扩散模型领域的一个重要的技巧\",{\"1\":{\"292\":1}}],[\"是可能存在无效结果的\",{\"1\":{\"277\":1}}],[\"是在特征维度上计算的\",{\"1\":{\"267\":1}}],[\"是在利用切分层内数据\",{\"1\":{\"35\":1}}],[\"是所有局部结果\",{\"1\":{\"262\":1}}],[\"是所有部分输出\",{\"1\":{\"259\":1}}],[\"是分配给第\",{\"1\":{\"259\":1}}],[\"是基础\",{\"1\":{\"255\":1}}],[\"是微批次数量\",{\"1\":{\"244\":1}}],[\"是流水线阶段数\",{\"1\":{\"244\":1}}],[\"是实现经济高效服务的关键\",{\"1\":{\"241\":1}}],[\"是gradient\",{\"1\":{\"173\":1}}],[\"是指碎片化内存\",{\"1\":{\"58\":1}}],[\"是指通讯过程中\",{\"1\":{\"57\":1}}],[\"是独立计算的\",{\"1\":{\"48\":1}}],[\"是一件非常复杂的事情\",{\"1\":{\"35\":1}}],[\"在xdit开发了\",{\"1\":{\"294\":1}}],[\"在这个diffusion\",{\"1\":{\"293\":1}}],[\"在图像内部的不同\",{\"1\":{\"293\":1}}],[\"在图像和nlp的模型上都得到更好的模型效果\",{\"1\":{\"82\":1}}],[\"在高分辨率图像生成时\",{\"1\":{\"288\":1}}],[\"在pipefusion基础上升级的xdit诞生了\",{\"1\":{\"285\":1}}],[\"在pytorch中可以通过下面的代码简单实现\",{\"1\":{\"89\":1}}],[\"在设备\",{\"1\":{\"279\":1}}],[\"在设备网格的每一行内\",{\"1\":{\"262\":1}}],[\"在获取输入激活块atl\",{\"1\":{\"279\":1}}],[\"在例图使用了n=2\",{\"1\":{\"279\":1}}],[\"在预测ϵθ​\",{\"1\":{\"279\":1}}],[\"在计算某一区块某一层的激活信息时\",{\"1\":{\"278\":1}}],[\"在计算时\",{\"1\":{\"55\":1}}],[\"在多个设备间并行化处理计算\",{\"1\":{\"278\":1}}],[\"在时间步t给定含噪图像xt​\",{\"1\":{\"277\":1}}],[\"在生成单张图像时\",{\"1\":{\"275\":1}}],[\"在生物分子结构预测等场景中\",{\"1\":{\"255\":1}}],[\"在最朴素的调度\",{\"1\":{\"265\":1}}],[\"在某些网络拓扑下可以获得更优的通信效率\",{\"1\":{\"262\":1}}],[\"在列维度上的拼接和行维度上的求和\",{\"1\":{\"262\":1}}],[\"在其本地数据\",{\"1\":{\"261\":1}}],[\"在逻辑上需要被拼接\",{\"1\":{\"260\":1}}],[\"在前向传播中是all\",{\"1\":{\"259\":1}}],[\"在前向传播中是广播\",{\"1\":{\"259\":1}}],[\"在此过程中\",{\"1\":{\"263\":1}}],[\"在此处进行非线性激活是通信优化的核心\",{\"1\":{\"259\":1}}],[\"在此基础上\",{\"1\":{\"240\":1}}],[\"在每一步t\",{\"1\":{\"279\":1}}],[\"在每一个batch计算完毕时执行pipeline\",{\"1\":{\"93\":1}}],[\"在每个layer上做集合通信\",{\"1\":{\"283\":1}}],[\"在每个去噪步骤中\",{\"1\":{\"277\":1}}],[\"在每个cpu节点内部\",{\"1\":{\"256\":1}}],[\"在基于slurm的纯cpu高性能计算\",{\"1\":{\"256\":1}}],[\"在处理长序列时\",{\"1\":{\"254\":1}}],[\"在量化过程中极大地降低了精度损失\",{\"1\":{\"250\":1}}],[\"在整个集群上使用dp\",{\"1\":{\"247\":1}}],[\"在流水线的启动和排空阶段\",{\"1\":{\"244\":1}}],[\"在流水线的不同阶段重叠计算\",{\"1\":{\"244\":1}}],[\"在流水线并行技术中\",{\"1\":{\"93\":1}}],[\"在全局批量大小\",{\"1\":{\"243\":1}}],[\"在全对全集合后\",{\"1\":{\"151\":1}}],[\"在满足严格延迟\",{\"1\":{\"241\":1}}],[\"在过半数的天数中\",{\"1\":{\"219\":1}}],[\"在过去训练中人们往往忽略了cpu的计算潜力\",{\"1\":{\"74\":1}}],[\"在乘法时做类似竖式乘法的高精度运算\",{\"1\":{\"184\":1}}],[\"在n\",{\"1\":{\"184\":1}}],[\"在数据存储\",{\"1\":{\"184\":1}}],[\"在开始讨论自动微分之前\",{\"1\":{\"174\":1}}],[\"在megatron\",{\"1\":{\"259\":1}}],[\"在ml领域\",{\"1\":{\"175\":1}}],[\"在mlp中最常用的操作是mm\",{\"1\":{\"89\":1}}],[\"在machine\",{\"1\":{\"173\":1}}],[\"在内循环期间\",{\"1\":{\"157\":1}}],[\"在attention计算之前\",{\"1\":{\"150\":1}}],[\"在sp一次前向和后向总共有4次all\",{\"1\":{\"148\":1}}],[\"在tp已将模型参数和计算在设备间切分的基础上\",{\"1\":{\"261\":1}}],[\"在tp并行中只在attention和mlp两个地方进行了并行计算\",{\"1\":{\"147\":1}}],[\"在transformer中的应用\",{\"1\":{\"245\":1}}],[\"在tensor模型并行基础上提出了sequence\",{\"1\":{\"148\":1}}],[\"在task1中解释了空间分解的概念\",{\"1\":{\"107\":1}}],[\"在workerthreadstart\",{\"1\":{\"107\":1}}],[\"在需要时\",{\"1\":{\"95\":1}}],[\"在大模型训练过程中显存占用过大往往成为瓶颈\",{\"1\":{\"145\":1}}],[\"在大模型训练中内存往往比算力更加宝贵\",{\"1\":{\"95\":1}}],[\"在大多数情况下\",{\"1\":{\"75\":1}}],[\"在一个大规模的gpu集群上达到超过50\",{\"1\":{\"91\":1}}],[\"在所有模型并行\",{\"1\":{\"89\":1}}],[\"在优化\",{\"1\":{\"89\":1}}],[\"在反向传播中是identity\",{\"1\":{\"259\":1}}],[\"在反向传播中是all\",{\"1\":{\"259\":1}}],[\"在反向传播时要保证在保存不同切分w的gpu中均及时更新a\",{\"1\":{\"89\":1}}],[\"在反向传播计算完成后\",{\"1\":{\"48\":1}}],[\"在该方法下\",{\"1\":{\"89\":1}}],[\"在将梯度传输到cpu内存之前\",{\"1\":{\"78\":1}}],[\"在\",{\"1\":{\"78\":5,\"79\":2}}],[\"在实现最小通信量的同时\",{\"1\":{\"75\":1}}],[\"在cpu上的计算量比gpu少多个数量级\",{\"1\":{\"75\":1}}],[\"在模型计算的过程中\",{\"1\":{\"54\":1}}],[\"在小batch时\",{\"1\":{\"48\":1}}],[\"在初始化期间将变量从\",{\"1\":{\"43\":1}}],[\"在128卡时甚至已经超过训练开销\",{\"1\":{\"38\":1}}],[\"在探讨了dp\",{\"1\":{\"35\":1}}],[\"在训练过程中\",{\"1\":{\"78\":1}}],[\"在训练过程中总会有数据\",{\"1\":{\"35\":1}}],[\"在训练ai时\",{\"1\":{\"33\":1}}],[\"梯度缩放由学习率超参进行调整\",{\"1\":{\"174\":1}}],[\"梯度下降法是一种优化算法\",{\"1\":{\"173\":1}}],[\"梯度下降\",{\"0\":{\"174\":1},\"1\":{\"173\":1}}],[\"梯度等概念将不会重新简述\",{\"1\":{\"172\":1}}],[\"梯度与\",{\"1\":{\"76\":1}}],[\"梯度和参数\",{\"1\":{\"53\":1}}],[\"梯度同步算法\",{\"0\":{\"46\":1}}],[\"梯度收集gpu聚合梯度\",{\"1\":{\"37\":1}}],[\"梯度\",{\"1\":{\"35\":1}}],[\"既切分优化器状态\",{\"1\":{\"35\":1}}],[\"既然我们有了对数据切分的方法\",{\"1\":{\"35\":1}}],[\"既然可能有很大的数据集需要切分\",{\"1\":{\"35\":1}}],[\"谷歌专门为tp开发了tpu进行并发适配\",{\"1\":{\"35\":1}}],[\"算子内部\",{\"1\":{\"245\":1}}],[\"算子间\",{\"1\":{\"244\":1}}],[\"算子并行\",{\"1\":{\"35\":1}}],[\"算法流程总结\",{\"1\":{\"268\":1}}],[\"算法流程并行\",{\"1\":{\"255\":1}}],[\"算法与数据结构\",{\"2\":{\"237\":1}}],[\"算法与数据结构实验题\",{\"1\":{\"183\":1}}],[\"算法\",{\"2\":{\"189\":1,\"238\":1}}],[\"算法的实现转化为一个独立的\",{\"1\":{\"42\":1}}],[\"算法还是0基础新人\",{\"1\":{\"0\":1}}],[\"计算尺度因子\",{\"1\":{\"268\":1}}],[\"计算激活的二阶矩\",{\"1\":{\"268\":1}}],[\"计算局部的∑xi​和∑xi2​\",{\"1\":{\"267\":1}}],[\"计算全局平方的均值\",{\"1\":{\"267\":1}}],[\"计算其局部平方和∑xi2​\",{\"1\":{\"267\":1}}],[\"计算其局部元素总和∑xi​\",{\"1\":{\"267\":1}}],[\"计算部分输出\",{\"1\":{\"259\":1}}],[\"计算流程\",{\"1\":{\"259\":2,\"261\":1}}],[\"计算流水线\",{\"1\":{\"244\":1}}],[\"计算模型\",{\"1\":{\"244\":1}}],[\"计算墙\",{\"1\":{\"240\":1,\"241\":1}}],[\"计算需要补充多少个才能达到目标次数m\",{\"1\":{\"214\":1}}],[\"计算用时\",{\"1\":{\"184\":1}}],[\"计算输入的真实值与对这组输入的预测值之间的损失\",{\"1\":{\"174\":1}}],[\"计算机组成\",{\"2\":{\"170\":1}}],[\"计算机体系结构\",{\"2\":{\"169\":1}}],[\"计算机大二本科小白\",{\"1\":{\"0\":1}}],[\"计算后再传递回去\",{\"1\":{\"161\":1}}],[\"计算z=gelu\",{\"1\":{\"148\":1}}],[\"计算梯度时\",{\"1\":{\"86\":1}}],[\"计算梯度并平均\",{\"1\":{\"79\":1}}],[\"计算\",{\"1\":{\"84\":1}}],[\"计算损失并反向传播\",{\"1\":{\"78\":1}}],[\"计算损失\",{\"1\":{\"78\":1}}],[\"计算放置在一起\",{\"1\":{\"76\":1}}],[\"计算时间呈二次方增长\",{\"1\":{\"283\":1}}],[\"计算时间\",{\"1\":{\"74\":1}}],[\"计算参数时\",{\"1\":{\"70\":1}}],[\"计算粒度更精细\",{\"1\":{\"60\":1}}],[\"计算过程中产生的一些临时数据\",{\"1\":{\"57\":1}}],[\"计算与通信流程\",{\"1\":{\"262\":1}}],[\"计算与通信比率的定量或定性度量\",{\"1\":{\"52\":1}}],[\"计算与聚合之间存在间隔\",{\"1\":{\"47\":1}}],[\"计算成本过大的问题\",{\"1\":{\"38\":1}}],[\"计算gpu从聚合gpu中pull完整梯度\",{\"1\":{\"37\":1}}],[\"计算出本地的梯度\",{\"1\":{\"37\":1}}],[\"计算矩阵进行切分\",{\"1\":{\"35\":1}}],[\"为简化说明\",{\"1\":{\"279\":1}}],[\"为解决上述问题\",{\"1\":{\"278\":1}}],[\"为例\",{\"1\":{\"262\":1}}],[\"为将理论与实践相结合\",{\"1\":{\"240\":1}}],[\"为代表的现代推理系统的核心架构演进\",{\"1\":{\"240\":1}}],[\"为什么不流水线并行batch\",{\"1\":{\"85\":1}}],[\"为什么ai训练需要分布式系统\",{\"0\":{\"33\":1}}],[\"为什么ai需要分布式系统\",{\"0\":{\"31\":1}}],[\"为一个\",{\"1\":{\"76\":1}}],[\"为避免cpu计算成为瓶颈\",{\"1\":{\"76\":1}}],[\"为了保护这些重要权重\",{\"1\":{\"268\":1}}],[\"为了减小气泡\",{\"1\":{\"266\":1}}],[\"为了得到最终输出\",{\"1\":{\"262\":1}}],[\"为了得到整个序列的准确均值μ和方差σ2\",{\"1\":{\"261\":1}}],[\"为了让后续的tp层\",{\"1\":{\"261\":1}}],[\"为了优化\",{\"1\":{\"260\":1}}],[\"为了训练和部署万亿参数级别的模型\",{\"1\":{\"247\":1}}],[\"为了加速并行策略的搜索\",{\"1\":{\"97\":1}}],[\"为了确保optimizer\",{\"1\":{\"93\":1}}],[\"为了确定最佳的下载策略\",{\"1\":{\"76\":1}}],[\"为了解决该问题\",{\"1\":{\"154\":1}}],[\"为了解决这个问题\",{\"1\":{\"75\":1}}],[\"为了解决传统dp计算节点和聚合节点比例不好确定导致的通讯成本\",{\"1\":{\"38\":1}}],[\"为了解决层切分的弊端\",{\"1\":{\"35\":1}}],[\"被切分为\",{\"1\":{\"262\":1}}],[\"被分发到第i$个gpu\",{\"1\":{\"261\":1}}],[\"被视为一个f操作\",{\"1\":{\"259\":1}}],[\"被模型状态占用\",{\"1\":{\"53\":1}}],[\"被称作模型并行\",{\"1\":{\"35\":1}}],[\"被我们称作数据并行\",{\"1\":{\"35\":1}}],[\"那样轻松地实现并行策略的混合\",{\"1\":{\"293\":1}}],[\"那么还需要从目标模型对最后一个位置的预测\",{\"1\":{\"263\":1}}],[\"那么已验证通过的序列是\",{\"1\":{\"263\":1}}],[\"那么代价呢\",{\"1\":{\"157\":1}}],[\"那么通过对图片本身的上下多份分割\",{\"1\":{\"107\":1}}],[\"那并行配置\",{\"1\":{\"99\":1}}],[\"那就有4ψ个byte\",{\"1\":{\"55\":1}}],[\"那自然也可能有很大的模型进行切分\",{\"1\":{\"35\":1}}],[\"那我们也可以考虑进行结合\",{\"1\":{\"35\":1}}],[\"那我们就可以考虑把数据切分成好几份\",{\"1\":{\"35\":1}}],[\"那我们可以分到多台设备上\",{\"1\":{\"33\":1}}],[\"那我们可以分到多张卡上面\",{\"1\":{\"33\":1}}],[\"我觉得\",{\"1\":{\"157\":1}}],[\"我希望能把这个数据很快的训练完\",{\"1\":{\"35\":1}}],[\"我们应该最小化最终输出的误差\",{\"1\":{\"268\":1}}],[\"我们需要在序列和特征两个维度上进行归一化\",{\"1\":{\"267\":1}}],[\"我们需要保证不同机器处理bucket的顺序一致\",{\"1\":{\"48\":1}}],[\"我们知道对于一个n进制数x\",{\"1\":{\"185\":1}}],[\"我们先看一下更为朴素的数值微分和符号微分\",{\"1\":{\"174\":1}}],[\"我们不得不感谢前人在这方面作出的杰出贡献\",{\"1\":{\"174\":1}}],[\"我们将进一步推导用斯特林公式估算阶乘位数n的公式\",{\"1\":{\"185\":1}}],[\"我们将一开始就对任务给出多线程的解决方式\",{\"1\":{\"108\":1}}],[\"我们将这种方式称作张量并行\",{\"1\":{\"35\":1}}],[\"我们称这个过程中的等待时间为pipeline\",{\"1\":{\"93\":1}}],[\"我们期望有可以自动为我们选定并行方法的策略\",{\"1\":{\"35\":1}}],[\"我们可以看到\",{\"1\":{\"185\":1}}],[\"我们可以考虑在某一个gpu中存储和更新参数\",{\"1\":{\"60\":1}}],[\"我们可以考虑将三种并行方式进行综合\",{\"1\":{\"35\":1}}],[\"我们可以考虑将计算过程中的算子\",{\"1\":{\"35\":1}}],[\"我们可以把一台机器存不下的任务放到多个结点里面\",{\"1\":{\"32\":1}}],[\"我们可以把一台机器的task进行切分\",{\"1\":{\"32\":1}}],[\"我们有我们自己的很大的数据集\",{\"1\":{\"35\":1}}],[\"我们要怎么做呢\",{\"1\":{\"35\":1}}],[\"我们针对ai训练达成的一种共识\",{\"1\":{\"33\":1}}],[\"我们总要拿出一部分数据用来预训练模型\",{\"1\":{\"33\":1}}],[\"输出聚合\",{\"1\":{\"261\":1}}],[\"输出时使用映射数组访问matrix\",{\"1\":{\"233\":1}}],[\"输出格式\",{\"1\":{\"209\":1,\"227\":1,\"233\":1}}],[\"输出大小写问题\",{\"1\":{\"202\":1}}],[\"输出顺序\",{\"1\":{\"195\":1}}],[\"输出字符的大小写问题\",{\"1\":{\"195\":1}}],[\"输出字符\",{\"1\":{\"194\":1}}],[\"输出\",{\"1\":{\"35\":1,\"55\":1,\"84\":1,\"183\":1,\"202\":1,\"209\":1}}],[\"输出矩阵\",{\"1\":{\"24\":1}}],[\"输入特征图分割成块\",{\"1\":{\"294\":1}}],[\"输入q\",{\"1\":{\"293\":1}}],[\"输入分片\",{\"1\":{\"262\":1}}],[\"输入激活\",{\"1\":{\"262\":1}}],[\"输入张量\",{\"1\":{\"261\":1}}],[\"输入切分\",{\"1\":{\"261\":1}}],[\"输入x沿序列维度被分散到各个gpu\",{\"1\":{\"267\":1}}],[\"输入x也相应地按列切分\",{\"1\":{\"245\":1}}],[\"输入x广播给所有设备\",{\"1\":{\"245\":1}}],[\"输入x分割\",{\"1\":{\"89\":1}}],[\"输入的行列号从1开始\",{\"1\":{\"233\":1}}],[\"输入的元素个数为sbh个\",{\"1\":{\"146\":1}}],[\"输入整数不超过10^60\",{\"1\":{\"207\":1}}],[\"输入字符串长度未判断\",{\"1\":{\"202\":1}}],[\"输入字符串只包含大小写英文字母\",{\"1\":{\"200\":1}}],[\"输入第一行为一个正整数\",{\"1\":{\"183\":1}}],[\"输入\",{\"1\":{\"35\":1,\"55\":1,\"183\":1,\"259\":1,\"260\":1}}],[\"但dit架构则呈现出较大差异性\",{\"1\":{\"285\":1}}],[\"但difussion模型的特点是\",{\"1\":{\"276\":1}}],[\"但保存原本激活值的方式和distrifusion保持一致\",{\"1\":{\"284\":1}}],[\"但激活尺寸大\",{\"1\":{\"276\":1}}],[\"但激活尺寸小\",{\"1\":{\"276\":1}}],[\"但这忽略了输入x的影响\",{\"1\":{\"268\":1}}],[\"但在sp中\",{\"1\":{\"267\":1}}],[\"但在实际工程中应该加上长度判断\",{\"1\":{\"202\":1}}],[\"但代价是需要缓存更多的激活值\",{\"1\":{\"266\":1}}],[\"但每次通信只在较小的设备组\",{\"1\":{\"262\":1}}],[\"但每一个gpu均做保留完整的模型做计算\",{\"1\":{\"35\":1}}],[\"但其通信成本与并行度\",{\"1\":{\"262\":1}}],[\"但有一个优化技巧\",{\"1\":{\"232\":1}}],[\"但有一个重要的数学性质\",{\"1\":{\"208\":1}}],[\"但结果较不精确\",{\"1\":{\"185\":1}}],[\"但仅用于注意力头的非重叠子集\",{\"1\":{\"150\":1}}],[\"但mask单个元素的大小只用1\",{\"1\":{\"146\":1}}],[\"但会带来额外的计算代价\",{\"1\":{\"145\":1}}],[\"但即使这样\",{\"1\":{\"99\":1}}],[\"但大多数\",{\"1\":{\"99\":1}}],[\"但要增加计算时长\",{\"1\":{\"86\":1}}],[\"但占用空间会多很多\",{\"1\":{\"85\":1}}],[\"但对于小\",{\"1\":{\"75\":1}}],[\"但重点是计算规模和速度\",{\"1\":{\"53\":1}}],[\"但参数量的上升在传统的单机gpu\",{\"1\":{\"53\":1}}],[\"但是pipefusion则需要在下一个diffusion\",{\"1\":{\"293\":1}}],[\"但是在之前都是基于hf\",{\"1\":{\"285\":1}}],[\"但是在更新参数进行新的计算时\",{\"1\":{\"55\":1}}],[\"但是与此同时dits\",{\"1\":{\"285\":1}}],[\"但是图片不是整体了\",{\"1\":{\"275\":1}}],[\"但是这种方法由于缺少各个patch间的信息感知\",{\"1\":{\"275\":1}}],[\"但是同生成式对话使用有限的文本量便能达成较为不错的生成效果不同\",{\"1\":{\"272\":1}}],[\"但是一般来说高精度阶乘的时间复杂度是o\",{\"1\":{\"184\":1}}],[\"但是我们都知道\",{\"1\":{\"184\":1}}],[\"但是我们必须要注意到神经网络实际上要对多维的array做算术运算\",{\"1\":{\"175\":1}}],[\"但是包含dropout用到的mask\",{\"1\":{\"146\":1}}],[\"但是单卡\",{\"1\":{\"82\":1}}],[\"但是事实上可以考虑在不同隐藏层中实现异步更新参数\",{\"1\":{\"68\":1}}],[\"但是zero3需要对参数进行切分和更新\",{\"1\":{\"68\":1}}],[\"但是需要时常更新\",{\"1\":{\"62\":1}}],[\"但是考虑在更新权重的时候\",{\"1\":{\"55\":1}}],[\"但是由于nvidia对于fp16精度计算的高优化度\",{\"1\":{\"55\":1}}],[\"但是也可以利用类似cpu指令流水线执行的方式\",{\"1\":{\"35\":1}}],[\"但是仅让一个卡存模型的几层在计算某些必须要用到之前的数据的模型时可能不尽如人意\",{\"1\":{\"35\":1}}],[\"但可惜一台机器的性能总是有限的\",{\"1\":{\"33\":1}}],[\"但同时也可能引入更多的问题\",{\"1\":{\"32\":1}}],[\"这套开发接口尽可能可能复用\",{\"1\":{\"285\":1}}],[\"这存在两个选择和两个问题\",{\"1\":{\"278\":1}}],[\"这篇文章也是neurips\",{\"1\":{\"277\":1}}],[\"这篇blog是本人结合reference对automatic\",{\"1\":{\"172\":1}}],[\"这给t与t−1步的模型ϵ并行带来了极大的困难\",{\"1\":{\"277\":1}}],[\"这也是前言讲难以像文本推理一样通过简单的切分矩阵实现并行的原因\",{\"1\":{\"275\":1}}],[\"这也限制了可以使用的卡的规模\",{\"1\":{\"92\":1}}],[\"这里的s是一个对角矩阵\",{\"1\":{\"268\":1}}],[\"这里仅作简述\",{\"1\":{\"149\":1}}],[\"这使得pipefusion无法像大型语言模型\",{\"1\":{\"293\":1}}],[\"这使得最为常用的\",{\"1\":{\"272\":1}}],[\"这使得计算流水更加紧密\",{\"1\":{\"266\":1}}],[\"这使得参与的gpu可以并行计算不同的注意力头\",{\"1\":{\"150\":1}}],[\"这会得到目标模型对于每个位置的预测概率分布\",{\"1\":{\"263\":1}}],[\"这会导致梯度同步结果出现错误\",{\"1\":{\"48\":1}}],[\"这被视为一个g操作\",{\"1\":{\"259\":1}}],[\"这被称为空间分解\",{\"1\":{\"107\":1}}],[\"这通过一次\",{\"1\":{\"259\":1,\"261\":1}}],[\"这其中包含多次独立的采样或去噪步骤\",{\"1\":{\"255\":1}}],[\"这意味着我们希望w\",{\"1\":{\"268\":1}}],[\"这意味着\",{\"1\":{\"254\":1}}],[\"这实现了组件级的流水线并行\",{\"1\":{\"253\":1}}],[\"这表明增加微批次数量或减少流水线阶段数可以减小气泡\",{\"1\":{\"244\":1}}],[\"这些权重并非其绝对值大\",{\"1\":{\"268\":1}}],[\"这些步骤天然适合进行任务级并行\",{\"1\":{\"255\":1}}],[\"这些思想在更复杂的算法题中都会经常用到\",{\"1\":{\"236\":1}}],[\"这些都问题都有进一步研究的价值\",{\"1\":{\"32\":1}}],[\"这组题目涵盖了多个基础算法知识点\",{\"1\":{\"236\":1}}],[\"这对估算阶乘的规模来说是完全可以接受的误差\",{\"1\":{\"185\":1}}],[\"这将是一件极为恐怖的事情\",{\"1\":{\"184\":1}}],[\"这道题看上去还挺有意思的很符合大学生的心理状态\",{\"1\":{\"184\":1}}],[\"这一过程也采用了与mlp层类似的行并行策略\",{\"1\":{\"260\":1}}],[\"这一过程被分解为两次矩阵乘法\",{\"1\":{\"259\":1}}],[\"这一天晚上\",{\"1\":{\"183\":1}}],[\"这一部分依赖于softmax实现\",{\"1\":{\"146\":1}}],[\"这一部分主要介绍作者使用zero的一些想法\",{\"1\":{\"59\":1}}],[\"这是优化流水线效率的根本出发点\",{\"1\":{\"265\":1}}],[\"这是一个\",{\"1\":{\"261\":1}}],[\"这是一个典型的\",{\"1\":{\"256\":1}}],[\"这是一道矩阵模拟题\",{\"1\":{\"232\":1}}],[\"这是一道字符串模拟题\",{\"1\":{\"226\":1}}],[\"这是一道字符串比较题\",{\"1\":{\"201\":1}}],[\"这是一道二维统计题\",{\"1\":{\"220\":1}}],[\"这是一道哈希统计题\",{\"1\":{\"214\":1}}],[\"这是一道大数处理题\",{\"1\":{\"208\":1}}],[\"这是一道简单的字符串构造题\",{\"1\":{\"194\":1}}],[\"这是cse234\",{\"1\":{\"176\":1}}],[\"这是我的事后诸葛亮\",{\"1\":{\"107\":1}}],[\"这主要是由于causal\",{\"1\":{\"161\":1}}],[\"这就是第一种错误的可能\",{\"1\":{\"184\":1}}],[\"这就是ai使用分布式系统想要解决的问题\",{\"1\":{\"33\":1}}],[\"这就给我们加大数据量和多机并行提供了极大的便利\",{\"1\":{\"145\":1}}],[\"这个scatter操作的输出随后被输入到稀疏操作fl​中\",{\"1\":{\"279\":1}}],[\"这个sp也是目的也是解决输入序列规模问题\",{\"1\":{\"153\":1}}],[\"这个公式直观地显示了流水线的\",{\"1\":{\"265\":1}}],[\"这个公式以詹姆斯\",{\"1\":{\"185\":1}}],[\"这个男孩给了弯通一个数字\",{\"1\":{\"183\":1}}],[\"这个过程显然需要一个足够强大的深度神经网络来支撑\",{\"1\":{\"174\":1}}],[\"这个操作等价于先对输入进行\",{\"1\":{\"89\":1}}],[\"这个时候就不得不引入分布式系统来改善这种局面了\",{\"1\":{\"33\":1}}],[\"这两种措施共同保证了zero\",{\"1\":{\"75\":1}}],[\"这又是一个仅在计算时才能用到的额外copy\",{\"1\":{\"55\":1}}],[\"这样一个diffusion\",{\"1\":{\"293\":1}}],[\"这样\",{\"1\":{\"245\":1}}],[\"这样就可以将每一部分计算矩阵进行并发处理\",{\"1\":{\"35\":1}}],[\"这样可以训练更大的模型\",{\"1\":{\"35\":1}}],[\"这种计算需求会以超过二次方的速度增加\",{\"1\":{\"277\":1}}],[\"这种分层设计最大化地利用了异构的通信带宽层级\",{\"1\":{\"247\":1}}],[\"这种ring\",{\"1\":{\"157\":1}}],[\"这种方法的好处是保障了各自在独立计算时的语义对等\",{\"1\":{\"89\":1}}],[\"这种方式在服务层实现了请求的并行处理\",{\"1\":{\"256\":1}}],[\"这种方式能不能过这个题我没有试过因为我懒\",{\"1\":{\"184\":1}}],[\"这种方式理论上并不影响训练结果\",{\"1\":{\"157\":1}}],[\"这种方式有两个问题需要注意\",{\"1\":{\"48\":1}}],[\"这种方式被称作流水线并行\",{\"1\":{\"35\":1}}],[\"这种策略被称作自动并行\",{\"1\":{\"35\":1}}],[\"这种并行考量被称为3d并行\",{\"1\":{\"35\":1}}],[\"这种并行方式被称作完全分片数据并行\",{\"1\":{\"35\":1}}],[\"这种按层切分的方式固然可以增加可容纳的模型的大小\",{\"1\":{\"35\":1}}],[\"这种按层切分模型\",{\"1\":{\"35\":1}}],[\"这种每个一份数据切分成多份\",{\"1\":{\"35\":1}}],[\"这种系统的好处是显而易见的\",{\"1\":{\"32\":1}}],[\"这基本上是在不考虑机器性能上限的情况下\",{\"1\":{\"33\":1}}],[\"拓展数据规模\",{\"1\":{\"32\":1}}],[\"什么是分布式系统\",{\"0\":{\"32\":1}}],[\"yq​=quant\",{\"1\":{\"268\":1}}],[\"yij​\",{\"1\":{\"262\":2}}],[\"yij​=xi​aj​\",{\"1\":{\"262\":1}}],[\"yi​\",{\"1\":{\"259\":1,\"262\":1}}],[\"yi​=gi​bi​\",{\"1\":{\"259\":1}}],[\"yp​\",{\"1\":{\"259\":1}}],[\"y2​\",{\"1\":{\"245\":1,\"259\":1}}],[\"y2s​\",{\"1\":{\"148\":3}}],[\"y=wx=\",{\"1\":{\"268\":1}}],[\"y=wx\",{\"1\":{\"268\":1}}],[\"y=var\",{\"1\":{\"267\":1}}],[\"y=xa\",{\"1\":{\"262\":1}}],[\"y=i=1∑p​yi​=all\",{\"1\":{\"259\":1}}],[\"y=gelu\",{\"1\":{\"259\":1}}],[\"y=y1​+y2​\",{\"1\":{\"245\":1}}],[\"y=\",{\"1\":{\"245\":1}}],[\"ya2c​\",{\"1\":{\"148\":1}}],[\"ya1c​\",{\"1\":{\"148\":1}}],[\"ya\",{\"1\":{\"148\":2}}],[\"yzwv​=layernorm\",{\"1\":{\"148\":1}}],[\"y\",{\"1\":{\"128\":5,\"148\":1,\"222\":2,\"234\":5,\"259\":1,\"262\":1}}],[\"y1​\",{\"1\":{\"245\":1,\"259\":1}}],[\"y1s​\",{\"1\":{\"148\":3}}],[\"y1\",{\"1\":{\"108\":2}}],[\"y0\",{\"1\":{\"108\":2}}],[\"yes字符串判断\",{\"0\":{\"198\":1}}],[\"yes\",{\"1\":{\"26\":1,\"199\":1,\"201\":1,\"202\":1,\"203\":2,\"222\":1}}],[\"your\",{\"1\":{\"125\":1,\"128\":1,\"139\":2,\"177\":3,\"179\":2}}],[\"you\",{\"1\":{\"10\":1,\"114\":2,\"117\":1,\"128\":2,\"138\":1,\"177\":3}}],[\"kv是相互补充的\",{\"1\":{\"293\":1}}],[\"kv进行计算\",{\"1\":{\"293\":1}}],[\"kv\",{\"1\":{\"293\":3}}],[\"kv更新的位置在不同rank间也没有重叠\",{\"1\":{\"293\":1}}],[\"k+1\",{\"1\":{\"263\":1}}],[\"k++\",{\"1\":{\"24\":1}}],[\"ki​\",{\"1\":{\"260\":2}}],[\"ke\",{\"1\":{\"183\":1}}],[\"keepdim=false\",{\"1\":{\"177\":1}}],[\"keepdim=true\",{\"1\":{\"177\":1}}],[\"keepdim=node\",{\"1\":{\"177\":1}}],[\"keepdim\",{\"1\":{\"177\":7}}],[\"key\",{\"1\":{\"146\":1,\"260\":1}}],[\"kwargs\",{\"1\":{\"177\":1}}],[\"kr​\",{\"1\":{\"162\":1}}],[\"k矩阵一样\",{\"1\":{\"146\":1}}],[\"k\",{\"1\":{\"24\":4,\"84\":4,\"146\":2,\"150\":1,\"151\":1,\"216\":3,\"222\":3,\"245\":1,\"260\":1,\"263\":3,\"293\":1}}],[\"know\",{\"1\":{\"7\":1,\"168\":1}}],[\"+ϵ​x−e\",{\"1\":{\"267\":1}}],[\"++\",{\"1\":{\"216\":1}}],[\"+nlog10​\",{\"1\":{\"185\":1}}],[\"+1\",{\"1\":{\"185\":2}}],[\"+\",{\"1\":{\"24\":11,\"89\":1,\"107\":1,\"117\":1,\"128\":3,\"129\":2,\"146\":1,\"168\":1,\"177\":4,\"185\":1,\"186\":2,\"222\":1,\"223\":1,\"235\":1,\"265\":1}}],[\"+=\",{\"1\":{\"9\":1,\"12\":1,\"14\":1,\"24\":1,\"216\":1}}],[\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\",{\"1\":{\"128\":1,\"130\":1}}],[\"~~\",{\"1\":{\"107\":1}}],[\"~\",{\"1\":{\"24\":1}}],[\">g的顺序\",{\"1\":{\"195\":1}}],[\">n\",{\"1\":{\"195\":1}}],[\">o\",{\"1\":{\"195\":1}}],[\">>\",{\"1\":{\"184\":1,\"196\":1,\"203\":2,\"210\":2,\"216\":4,\"222\":4,\"228\":5,\"234\":6}}],[\">>>\",{\"1\":{\"117\":6,\"118\":5,\"119\":6,\"120\":1,\"122\":2,\"128\":7,\"129\":1,\"131\":2}}],[\">=\",{\"1\":{\"89\":1,\"222\":1}}],[\">\",{\"1\":{\"12\":1,\"13\":2,\"14\":5,\"89\":2,\"108\":16,\"128\":2,\"139\":3,\"177\":51,\"179\":3,\"186\":1,\"196\":1,\"203\":1,\"210\":1,\"216\":2,\"222\":2,\"259\":2,\"278\":2}}],[\"v和输出o都是沿着序列维度切分\",{\"1\":{\"293\":1}}],[\"vllm的cpu后端功能当时尚不完善\",{\"1\":{\"256\":1}}],[\"v的投影计算按\",{\"1\":{\"245\":1}}],[\"vr+1​​workerr+1的覆盖\",{\"1\":{\"162\":1}}],[\"vr​\",{\"1\":{\"162\":1}}],[\"v2s​\",{\"1\":{\"148\":1}}],[\"v矩阵的大小之前没有统计\",{\"1\":{\"146\":1}}],[\"v\",{\"1\":{\"146\":3,\"150\":1,\"151\":2,\"260\":1}}],[\"vae\",{\"0\":{\"288\":1,\"294\":1},\"1\":{\"288\":2,\"294\":2}}],[\"var\",{\"1\":{\"267\":1}}],[\"variance\",{\"1\":{\"76\":1}}],[\"variable>\",{\"1\":{\"14\":1}}],[\"variables\",{\"1\":{\"13\":1,\"118\":1}}],[\"variable\",{\"1\":{\"8\":1,\"12\":1,\"13\":1,\"14\":1,\"117\":1}}],[\"val\",{\"1\":{\"177\":6}}],[\"value的投影矩阵\",{\"1\":{\"260\":1}}],[\"valueerror\",{\"1\":{\"177\":1}}],[\"value数据\",{\"1\":{\"161\":1}}],[\"values\",{\"1\":{\"139\":3,\"177\":52,\"179\":3}}],[\"value\",{\"1\":{\"119\":2,\"146\":1,\"157\":6,\"168\":3,\"177\":5}}],[\"v1s​\",{\"1\":{\"148\":1}}],[\"v1\",{\"1\":{\"106\":4,\"128\":1,\"130\":1}}],[\"vocabutility\",{\"1\":{\"89\":1}}],[\"vocab\",{\"1\":{\"89\":9}}],[\"vocabparallelembedding\",{\"1\":{\"89\":2}}],[\"void\",{\"1\":{\"24\":1,\"108\":1}}],[\"vi​\",{\"1\":{\"260\":2}}],[\"vi​=xwiq​\",{\"1\":{\"260\":1}}],[\"visualize\",{\"1\":{\"121\":1}}],[\"visible\",{\"1\":{\"43\":1}}],[\"vim\",{\"1\":{\"24\":1,\"26\":1}}],[\"verification\",{\"1\":{\"263\":1}}],[\"version\",{\"1\":{\"24\":1,\"128\":1,\"130\":1}}],[\"vector<vector<int>>\",{\"1\":{\"234\":1}}],[\"vector<int>\",{\"1\":{\"216\":1,\"222\":1,\"234\":2}}],[\"vector<t>\",{\"1\":{\"14\":1}}],[\"vector初始化大小必须足够存储所有可能的字符\",{\"1\":{\"215\":1}}],[\"vector\",{\"1\":{\"14\":2}}],[\"vec\",{\"1\":{\"12\":1,\"14\":4}}],[\"404\",{\"1\":{\"297\":1}}],[\"4个维度描述搜索空间\",{\"1\":{\"98\":1}}],[\"4个cell\",{\"1\":{\"84\":1}}],[\"4\",{\"0\":{\"212\":1},\"1\":{\"10\":1,\"12\":1,\"14\":1,\"23\":1,\"24\":2,\"42\":1,\"45\":1,\"84\":1,\"107\":1,\"168\":1,\"268\":2}}],[\"3d\",{\"1\":{\"177\":1}}],[\"3d并行\",{\"0\":{\"90\":1},\"1\":{\"247\":1}}],[\"3的训练过程\",{\"1\":{\"174\":1}}],[\"34+5has​\",{\"1\":{\"146\":1,\"148\":1}}],[\"314\",{\"1\":{\"118\":1,\"119\":1}}],[\"32和p32\",{\"1\":{\"76\":1}}],[\"32\",{\"1\":{\"76\":1}}],[\"3次更新之后\",{\"1\":{\"40\":1}}],[\"3\",{\"0\":{\"205\":1},\"1\":{\"10\":1,\"12\":1,\"14\":1,\"23\":1,\"24\":3,\"45\":1,\"84\":1,\"107\":1,\"117\":1,\"128\":1,\"186\":2,\"267\":1,\"268\":1,\"293\":1}}],[\"2405\",{\"1\":{\"281\":1,\"285\":1}}],[\"2d\",{\"1\":{\"255\":1,\"262\":1}}],[\"2πn\",{\"1\":{\"185\":1}}],[\"2πn​\",{\"1\":{\"185\":1}}],[\"25000\",{\"1\":{\"184\":1}}],[\"27的时候\",{\"1\":{\"184\":1}}],[\"2的计算资源浪费\",{\"1\":{\"161\":1}}],[\"2sbh\",{\"1\":{\"146\":1}}],[\"28\",{\"1\":{\"128\":1}}],[\"2^n\",{\"1\":{\"99\":1}}],[\"2m\",{\"1\":{\"76\":2}}],[\"2依托allreduce算法实现\",{\"1\":{\"68\":1}}],[\"2310\",{\"1\":{\"155\":1,\"158\":1}}],[\"2309\",{\"1\":{\"149\":1}}],[\"23位尾数位\",{\"1\":{\"55\":1}}],[\"23\",{\"1\":{\"45\":1}}],[\"2335144\",{\"1\":{\"21\":1}}],[\"21​log10​\",{\"1\":{\"185\":1}}],[\"2105\",{\"1\":{\"152\":1}}],[\"2104\",{\"1\":{\"90\":1}}],[\"2101\",{\"1\":{\"73\":2}}],[\"21\",{\"1\":{\"45\":1,\"106\":4}}],[\"2023的一篇\",{\"1\":{\"277\":1}}],[\"2024的cvpr\",{\"1\":{\"273\":1}}],[\"2024\",{\"1\":{\"129\":3,\"131\":1,\"193\":1,\"273\":1}}],[\"2025\",{\"1\":{\"24\":1,\"176\":1}}],[\"2025competition\",{\"2\":{\"16\":1,\"29\":1}}],[\"20\",{\"1\":{\"45\":3,\"119\":1,\"128\":2}}],[\"2006\",{\"1\":{\"44\":1}}],[\"2207\",{\"1\":{\"287\":1}}],[\"2205\",{\"1\":{\"144\":1}}],[\"2246467991473532e\",{\"1\":{\"117\":1}}],[\"22\",{\"1\":{\"26\":1,\"45\":1,\"106\":1,\"128\":1,\"166\":1}}],[\"2\",{\"0\":{\"86\":1,\"198\":1},\"1\":{\"10\":1,\"12\":1,\"14\":2,\"24\":1,\"37\":1,\"45\":1,\"48\":1,\"53\":1,\"79\":1,\"117\":1,\"118\":1,\"128\":2,\"130\":1,\"139\":1,\"146\":1,\"168\":1,\"177\":6,\"179\":1,\"186\":3,\"210\":1,\"221\":3,\"222\":2,\"236\":1,\"245\":1,\"259\":1,\"267\":2,\"268\":1,\"279\":3}}],[\"5924\",{\"1\":{\"294\":1}}],[\"5d的张量并行策略\",{\"1\":{\"255\":1}}],[\"5as2b\",{\"1\":{\"146\":1}}],[\"5位指数位\",{\"1\":{\"55\":1}}],[\"5000000\",{\"1\":{\"10\":1}}],[\"5\",{\"0\":{\"218\":1},\"1\":{\"10\":2,\"12\":4,\"14\":8,\"45\":1,\"107\":1,\"128\":1,\"268\":2,\"293\":1}}],[\"<iostream>\",{\"1\":{\"228\":1,\"234\":1}}],[\"<bits\",{\"1\":{\"196\":1,\"203\":1,\"210\":1,\"216\":1,\"222\":1}}],[\"<return\",{\"1\":{\"120\":1}}],[\"<formal\",{\"1\":{\"120\":1}}],[\"<name>\",{\"1\":{\"120\":1}}],[\"<args>检验正确与否\",{\"1\":{\"107\":1}}],[\"<atomic>\",{\"1\":{\"14\":1}}],[\"<<\",{\"1\":{\"24\":2,\"184\":1,\"196\":4,\"210\":2,\"228\":2,\"234\":2}}],[\"<stdin>\",{\"1\":{\"117\":1}}],[\"<stdio\",{\"1\":{\"24\":1,\"27\":1,\"186\":1}}],[\"<stdlib\",{\"1\":{\"24\":1}}],[\"<string>\",{\"1\":{\"228\":1}}],[\"<string\",{\"1\":{\"24\":1}}],[\"<module>\",{\"1\":{\"117\":1}}],[\"<math\",{\"1\":{\"24\":1,\"27\":1,\"186\":1}}],[\"<mpich\",{\"1\":{\"24\":2}}],[\"<mutex>\",{\"1\":{\"14\":1}}],[\"<time\",{\"1\":{\"24\":1}}],[\"<typename\",{\"1\":{\"14\":1}}],[\"<thread>\",{\"1\":{\"14\":1}}],[\"<vector>\",{\"1\":{\"14\":1,\"234\":1}}],[\"<cctype>\",{\"1\":{\"203\":1}}],[\"<cstdlib>\",{\"1\":{\"14\":1}}],[\"<cstdio>\",{\"1\":{\"14\":1,\"203\":1}}],[\"<condition\",{\"1\":{\"14\":1}}],[\"<\",{\"1\":{\"10\":1,\"12\":1,\"14\":3,\"24\":13,\"89\":1,\"184\":1,\"222\":1,\"228\":2,\"234\":6}}],[\"1d\",{\"1\":{\"262\":1}}],[\"1<=n<=25000\",{\"1\":{\"183\":1}}],[\"1位符号位\",{\"1\":{\"55\":3}}],[\"1909\",{\"1\":{\"88\":1}}],[\"1910\",{\"1\":{\"51\":1}}],[\"19\",{\"1\":{\"45\":1}}],[\"18sbh\",{\"1\":{\"147\":1}}],[\"1807\",{\"1\":{\"97\":1}}],[\"1802\",{\"1\":{\"38\":1}}],[\"1811\",{\"1\":{\"81\":1}}],[\"18\",{\"1\":{\"45\":1,\"128\":1,\"130\":1}}],[\"17\",{\"1\":{\"45\":1}}],[\"1502\",{\"1\":{\"174\":1}}],[\"1592653589793\",{\"1\":{\"118\":1,\"119\":1}}],[\"15\",{\"1\":{\"45\":1}}],[\"15704https\",{\"1\":{\"44\":1}}],[\"14430\",{\"1\":{\"285\":1}}],[\"14430v2\",{\"1\":{\"281\":1}}],[\"141592654\",{\"1\":{\"186\":1}}],[\"141592653589793\",{\"1\":{\"117\":1}}],[\"14509\",{\"1\":{\"149\":1}}],[\"14\",{\"1\":{\"45\":1}}],[\"13120\",{\"1\":{\"152\":1}}],[\"13\",{\"1\":{\"45\":1}}],[\"12598\",{\"1\":{\"287\":1}}],[\"1256\",{\"1\":{\"119\":1}}],[\"121\",{\"1\":{\"119\":1}}],[\"121482116\",{\"1\":{\"23\":1}}],[\"12\",{\"1\":{\"45\":1,\"84\":1}}],[\"11sbh\",{\"1\":{\"146\":1}}],[\"11\",{\"1\":{\"45\":1,\"119\":1}}],[\"1块梯度收集gpu\",{\"1\":{\"37\":1}}],[\"16\",{\"1\":{\"24\":1,\"45\":1,\"117\":1,\"128\":1}}],[\"100\",{\"1\":{\"207\":1}}],[\"1000000\",{\"1\":{\"10\":3,\"12\":1,\"14\":7}}],[\"103\",{\"1\":{\"200\":1}}],[\"10+t24​+5htas​\",{\"1\":{\"147\":1}}],[\"10位尾数位\",{\"1\":{\"55\":1}}],[\"10\",{\"1\":{\"24\":1,\"45\":5,\"118\":2,\"168\":2,\"183\":1}}],[\"1\",{\"0\":{\"17\":1,\"85\":1,\"137\":1,\"138\":1,\"191\":1},\"1\":{\"9\":1,\"10\":1,\"12\":2,\"14\":5,\"24\":6,\"45\":1,\"48\":1,\"53\":1,\"78\":1,\"89\":2,\"117\":3,\"128\":4,\"129\":3,\"130\":1,\"131\":2,\"168\":2,\"175\":1,\"177\":19,\"183\":1,\"184\":2,\"185\":2,\"186\":2,\"193\":1,\"197\":1,\"200\":1,\"204\":1,\"207\":1,\"210\":1,\"211\":1,\"217\":1,\"221\":1,\"222\":5,\"233\":1,\"234\":2,\"245\":1,\"255\":2,\"259\":1,\"262\":2,\"263\":2,\"266\":2,\"267\":1,\"268\":3,\"279\":3,\"294\":1}}],[\"ji\",{\"1\":{\"183\":1}}],[\"journey\",{\"0\":{\"166\":1}}],[\"joinable\",{\"1\":{\"13\":1,\"14\":1}}],[\"join\",{\"1\":{\"8\":1,\"13\":3,\"14\":1}}],[\"j++\",{\"1\":{\"24\":2,\"228\":1,\"234\":2}}],[\"j\",{\"1\":{\"24\":7,\"228\":4,\"233\":1,\"234\":7,\"262\":3,\"268\":1}}],[\"judgement\",{\"1\":{\"12\":1}}],[\"just\",{\"1\":{\"8\":1,\"10\":2,\"12\":1,\"13\":1,\"18\":1,\"27\":1,\"114\":2,\"117\":1,\"122\":1}}],[\"f和f​替换为g和g​​在前向是reduce\",{\"1\":{\"148\":1}}],[\"f​在前向时执行all\",{\"1\":{\"147\":1}}],[\"f在前向时不做操作\",{\"1\":{\"147\":1}}],[\"f\",{\"1\":{\"89\":2}}],[\"fk和\",{\"1\":{\"84\":2}}],[\"flops\",{\"1\":{\"241\":1}}],[\"float\",{\"1\":{\"108\":4,\"177\":4}}],[\"float2half\",{\"1\":{\"76\":1}}],[\"flashattention部分的梯度更新为kqv三个矩阵\",{\"1\":{\"163\":1}}],[\"flash\",{\"1\":{\"117\":1}}],[\"flag\",{\"1\":{\"12\":2,\"13\":1,\"14\":4}}],[\"flexflow负责将\",{\"1\":{\"99\":1}}],[\"flexflow\",{\"0\":{\"97\":1},\"1\":{\"97\":2}}],[\"flush消耗的时间则越小\",{\"1\":{\"93\":1}}],[\"flush操作\",{\"1\":{\"93\":1}}],[\"fwd\",{\"1\":{\"76\":1,\"78\":1}}],[\"fsp​\",{\"1\":{\"267\":1}}],[\"fsdp并行\",{\"0\":{\"50\":1}}],[\"fsanitize=thread\",{\"1\":{\"8\":1}}],[\"failed\",{\"1\":{\"130\":1}}],[\"fast\",{\"1\":{\"38\":1,\"166\":1}}],[\"false\",{\"1\":{\"12\":1,\"14\":2,\"78\":1,\"177\":1}}],[\"fuctions\",{\"1\":{\"120\":1}}],[\"fully\",{\"1\":{\"35\":1}}],[\"functions\",{\"0\":{\"115\":1,\"120\":1,\"129\":1},\"1\":{\"117\":1,\"118\":1,\"119\":2,\"131\":1,\"168\":1}}],[\"function\",{\"1\":{\"13\":1,\"89\":1,\"117\":1,\"119\":1}}],[\"func\",{\"1\":{\"9\":1,\"12\":1,\"14\":2}}],[\"fp16梯度和所有优化器状态存储在cpu上\",{\"1\":{\"78\":1}}],[\"fp16\",{\"1\":{\"55\":2,\"78\":3}}],[\"fp32\",{\"1\":{\"55\":2,\"76\":1,\"78\":6}}],[\"fprintf\",{\"1\":{\"27\":1}}],[\"fpie\",{\"1\":{\"8\":1}}],[\"freq\",{\"1\":{\"89\":1}}],[\"free\",{\"1\":{\"24\":5,\"292\":1}}],[\"fragmented\",{\"0\":{\"64\":1}}],[\"fragmentation\",{\"0\":{\"58\":1}}],[\"frames\",{\"1\":{\"120\":2}}],[\"frame\",{\"0\":{\"9\":1}}],[\"from\",{\"0\":{\"20\":1},\"1\":{\"21\":1,\"24\":1,\"25\":1,\"26\":3,\"27\":1,\"89\":2,\"117\":2,\"119\":1,\"124\":1,\"152\":1,\"166\":1}}],[\"follow\",{\"1\":{\"174\":1}}],[\"following\",{\"0\":{\"21\":1},\"1\":{\"128\":1}}],[\"four\",{\"1\":{\"129\":2,\"131\":1}}],[\"found\",{\"1\":{\"10\":2,\"297\":1}}],[\"format\",{\"1\":{\"168\":3}}],[\"forward\",{\"1\":{\"45\":1,\"78\":3,\"84\":1,\"89\":2,\"175\":1,\"266\":1}}],[\"fortran\",{\"1\":{\"24\":3}}],[\"for\",{\"0\":{\"25\":1,\"26\":1,\"166\":1},\"1\":{\"9\":4,\"10\":2,\"12\":5,\"13\":1,\"14\":7,\"24\":14,\"25\":1,\"26\":1,\"78\":6,\"97\":1,\"117\":1,\"118\":1,\"120\":1,\"149\":1,\"155\":1,\"166\":1,\"168\":1,\"177\":5,\"184\":1,\"203\":1,\"216\":2,\"222\":1,\"228\":2,\"234\":6,\"273\":1}}],[\"fetch\",{\"1\":{\"9\":1,\"12\":1,\"14\":1}}],[\"figure\",{\"1\":{\"168\":3}}],[\"fix\",{\"0\":{\"11\":1,\"12\":1},\"1\":{\"14\":2,\"24\":1}}],[\"five\",{\"1\":{\"10\":1}}],[\"fill\",{\"1\":{\"10\":1,\"12\":1,\"14\":1}}],[\"files\",{\"1\":{\"19\":1,\"124\":1}}],[\"file\",{\"0\":{\"20\":1},\"1\":{\"8\":1,\"19\":2,\"117\":1}}],[\"firstly\",{\"1\":{\"26\":1}}],[\"first\",{\"0\":{\"127\":1},\"1\":{\"9\":1,\"114\":1}}],[\"finalize\",{\"1\":{\"24\":1,\"27\":1}}],[\"finished++\",{\"1\":{\"12\":1,\"14\":1}}],[\"finished\",{\"1\":{\"9\":1,\"10\":8,\"12\":13,\"13\":1,\"14\":17}}],[\"finish\",{\"1\":{\"9\":3,\"10\":2,\"12\":2,\"14\":3,\"18\":1}}],[\"find\",{\"0\":{\"2\":1,\"6\":1,\"7\":1},\"1\":{\"8\":2,\"10\":2,\"13\":2,\"14\":1,\"23\":1,\"24\":1,\"117\":1}}],[\"=ws−1在量化时损失最小\",{\"1\":{\"268\":1}}],[\"=w\",{\"1\":{\"268\":1}}],[\"=n∑x2​\",{\"1\":{\"267\":1}}],[\"=tsbh​\",{\"1\":{\"148\":1}}],[\"=gˉ​\",{\"1\":{\"148\":1}}],[\"=gelu\",{\"1\":{\"148\":1}}],[\"=z1h​b1r​\",{\"1\":{\"148\":1}}],[\"=zb\",{\"1\":{\"148\":1}}],[\"=dropout\",{\"1\":{\"148\":1}}],[\"=softmax\",{\"1\":{\"146\":1,\"260\":1}}],[\"=====================================================================\",{\"1\":{\"128\":2,\"130\":2}}],[\"==\",{\"1\":{\"10\":1,\"12\":1,\"14\":1,\"24\":1,\"139\":1,\"177\":13,\"179\":1,\"203\":1,\"210\":1,\"222\":3,\"228\":1,\"234\":2}}],[\"=\",{\"1\":{\"8\":1,\"10\":3,\"12\":5,\"14\":15,\"24\":34,\"43\":7,\"45\":6,\"78\":7,\"89\":10,\"108\":3,\"118\":2,\"119\":1,\"128\":3,\"148\":2,\"177\":14,\"184\":5,\"186\":3,\"203\":1,\"216\":1,\"222\":5,\"228\":3,\"234\":8,\"245\":1}}],[\"equivalent\",{\"1\":{\"268\":1}}],[\"equal\",{\"1\":{\"168\":1}}],[\"era\",{\"1\":{\"241\":1}}],[\"error\",{\"1\":{\"8\":1,\"10\":1,\"14\":1,\"174\":1}}],[\"e\",{\"1\":{\"186\":2,\"267\":2}}],[\"ei​表示一个单位向量\",{\"1\":{\"175\":1}}],[\"esay\",{\"1\":{\"128\":1}}],[\"edu\",{\"1\":{\"128\":1}}],[\"edit\",{\"1\":{\"119\":1}}],[\"evaluates\",{\"1\":{\"129\":1}}],[\"evaluate\",{\"1\":{\"122\":1}}],[\"evaluated\",{\"1\":{\"119\":1,\"120\":1}}],[\"even\",{\"1\":{\"118\":1,\"209\":1,\"210\":1}}],[\"every\",{\"1\":{\"12\":2,\"119\":1}}],[\"email\",{\"1\":{\"128\":1}}],[\"empty\",{\"1\":{\"89\":1}}],[\"embeddings\",{\"1\":{\"89\":4}}],[\"embedding\",{\"1\":{\"89\":7}}],[\"efficient\",{\"1\":{\"81\":1,\"90\":1}}],[\"efficiency\",{\"0\":{\"75\":1},\"1\":{\"168\":1}}],[\"etc\",{\"1\":{\"26\":1}}],[\"else\",{\"1\":{\"24\":1,\"78\":2,\"89\":1,\"177\":1,\"203\":1,\"210\":1,\"222\":1,\"234\":1}}],[\"elements\",{\"1\":{\"14\":1}}],[\"element\",{\"1\":{\"14\":2,\"139\":2,\"177\":12,\"179\":2,\"259\":1}}],[\"easily\",{\"1\":{\"13\":1}}],[\"easy\",{\"1\":{\"13\":1,\"18\":1,\"38\":1,\"114\":1}}],[\"each\",{\"1\":{\"9\":1,\"14\":2,\"128\":1,\"139\":1,\"177\":5,\"179\":1}}],[\"encoder\",{\"1\":{\"253\":1}}],[\"en​\",{\"1\":{\"185\":3}}],[\"enumerate\",{\"1\":{\"177\":1}}],[\"enabling\",{\"1\":{\"149\":1}}],[\"environment\",{\"0\":{\"121\":1},\"1\":{\"27\":1}}],[\"envirment\",{\"1\":{\"18\":1}}],[\"ensure\",{\"1\":{\"14\":1}}],[\"endl\",{\"1\":{\"24\":1,\"228\":1}}],[\"end\",{\"1\":{\"10\":1,\"12\":1,\"14\":1,\"89\":3,\"222\":1,\"245\":1}}],[\"enter\",{\"1\":{\"9\":1,\"10\":1,\"12\":1,\"14\":1,\"128\":1}}],[\"example\",{\"1\":{\"168\":1}}],[\"examples\",{\"1\":{\"131\":1}}],[\"excute\",{\"1\":{\"168\":1}}],[\"extreme\",{\"1\":{\"149\":1}}],[\"extended\",{\"0\":{\"53\":1}}],[\"execution\",{\"1\":{\"97\":1,\"98\":2}}],[\"executable\",{\"1\":{\"8\":1}}],[\"expandasop3d\",{\"1\":{\"177\":2}}],[\"expandasop\",{\"1\":{\"177\":2}}],[\"expand\",{\"1\":{\"177\":5}}],[\"expr\",{\"1\":{\"168\":6}}],[\"expression>\",{\"1\":{\"120\":1}}],[\"expression\",{\"1\":{\"9\":1,\"114\":1,\"119\":2,\"129\":1,\"168\":1}}],[\"expect\",{\"1\":{\"128\":1}}],[\"experiences\",{\"1\":{\"44\":1}}],[\"explanation\",{\"1\":{\"168\":1}}],[\"explains\",{\"1\":{\"125\":1}}],[\"explaination\",{\"1\":{\"114\":1}}],[\"explnation\",{\"1\":{\"114\":1}}],[\"exp\",{\"1\":{\"45\":2}}],[\"export\",{\"1\":{\"24\":6,\"106\":1}}],[\"exit\",{\"0\":{\"13\":1},\"1\":{\"12\":3,\"13\":2,\"14\":7,\"128\":1}}],[\"exist\",{\"1\":{\"8\":1,\"27\":1,\"168\":2}}],[\"gmacs\",{\"1\":{\"277\":1}}],[\"gsp​\",{\"1\":{\"267\":1}}],[\"gdb\",{\"1\":{\"168\":2}}],[\"guidance\",{\"1\":{\"292\":1}}],[\"guide\",{\"1\":{\"49\":1,\"292\":2}}],[\"guest\",{\"1\":{\"168\":3}}],[\"gi​\",{\"1\":{\"259\":1}}],[\"gi​=gelu\",{\"1\":{\"259\":1}}],[\"given\",{\"1\":{\"139\":1,\"177\":14,\"179\":1}}],[\"giant\",{\"1\":{\"81\":1}}],[\"git\",{\"1\":{\"106\":2}}],[\"github\",{\"1\":{\"38\":1,\"44\":1,\"51\":1,\"88\":1,\"90\":1,\"106\":2,\"273\":1,\"285\":1,\"294\":1}}],[\"gitbooks\",{\"1\":{\"2\":1}}],[\"gptq\",{\"1\":{\"250\":1}}],[\"gpipe使用模型并行\",{\"1\":{\"82\":1}}],[\"gpipe\",{\"0\":{\"81\":1,\"82\":1},\"1\":{\"81\":1}}],[\"gpu总数\",{\"1\":{\"95\":1}}],[\"gpu服务器内部\",{\"1\":{\"91\":1}}],[\"gpu内存占用是一件非常昂贵的事情\",{\"1\":{\"74\":1}}],[\"gpu\",{\"1\":{\"43\":3,\"74\":2,\"76\":3,\"78\":5,\"79\":6,\"82\":1,\"84\":4,\"86\":1,\"89\":1,\"90\":1,\"262\":1}}],[\"greaterthanop\",{\"1\":{\"177\":1}}],[\"graph\",{\"1\":{\"99\":1,\"100\":1,\"101\":1,\"177\":1,\"244\":1}}],[\"grads\",{\"1\":{\"177\":1}}],[\"gradient\",{\"1\":{\"89\":3,\"139\":2,\"177\":28,\"179\":2}}],[\"gradients\",{\"0\":{\"55\":1},\"1\":{\"177\":1}}],[\"grad\",{\"1\":{\"78\":7,\"89\":1,\"139\":1,\"177\":40,\"179\":1}}],[\"group所有设备的kv\",{\"1\":{\"293\":1}}],[\"group内不同rank设备的fresh\",{\"1\":{\"293\":1}}],[\"group\",{\"1\":{\"45\":1}}],[\"goes\",{\"1\":{\"117\":1,\"118\":1}}],[\"go\",{\"0\":{\"54\":1}}],[\"gateway\",{\"1\":{\"49\":1}}],[\"gather拼接起来再激活\",{\"1\":{\"259\":1}}],[\"gather和4次reduce\",{\"1\":{\"148\":1}}],[\"gather通信\",{\"1\":{\"148\":1}}],[\"gather阶段\",{\"1\":{\"40\":1}}],[\"gather\",{\"0\":{\"41\":1},\"1\":{\"39\":1,\"79\":1,\"148\":2,\"245\":1,\"254\":1,\"259\":1,\"261\":2,\"267\":2}}],[\"gloo\",{\"1\":{\"49\":1}}],[\"gloo和mpi\",{\"1\":{\"49\":1}}],[\"global\",{\"1\":{\"13\":1,\"89\":1,\"95\":1,\"120\":1,\"243\":1}}],[\"generalized\",{\"1\":{\"250\":1}}],[\"generate\",{\"1\":{\"24\":3}}],[\"gelu的反向也需要对输入进行缓存\",{\"1\":{\"146\":1}}],[\"gelu\",{\"1\":{\"89\":1,\"148\":2}}],[\"getting\",{\"0\":{\"124\":1}}],[\"gets\",{\"1\":{\"119\":1}}],[\"get\",{\"1\":{\"12\":2,\"19\":1,\"27\":1,\"89\":1,\"128\":1,\"177\":1}}],[\"gz\",{\"1\":{\"19\":2,\"106\":2}}],[\"g\",{\"1\":{\"8\":1,\"192\":1,\"194\":1,\"195\":1,\"196\":1}}],[\"g++\",{\"1\":{\"8\":1}}],[\"gcc\",{\"1\":{\"8\":1}}],[\"m≫p\",{\"1\":{\"265\":1}}],[\"mtb​是反向传播时间\",{\"1\":{\"265\":1}}],[\"mtf​是所有微批次在最后一个阶段完成前向的时间\",{\"1\":{\"265\":1}}],[\"mt​\",{\"1\":{\"263\":2}}],[\"md​\",{\"1\":{\"263\":2}}],[\"mha层的并行化利用了注意力头\",{\"1\":{\"260\":1}}],[\"mha层\",{\"1\":{\"245\":1}}],[\"mha\",{\"1\":{\"245\":1}}],[\"m×n\",{\"1\":{\"223\":1}}],[\"mlogm\",{\"1\":{\"223\":1}}],[\"mlp层包含两个线性变换和一个非线性激活函数\",{\"1\":{\"259\":1}}],[\"mlp层\",{\"1\":{\"245\":1}}],[\"mlp层由两个linear层组成\",{\"1\":{\"148\":1}}],[\"mlp中有两层线性layer\",{\"1\":{\"146\":1}}],[\"mlp的activation大小计算\",{\"1\":{\"146\":1}}],[\"mlsys\",{\"0\":{\"30\":1,\"142\":1},\"2\":{\"103\":1,\"141\":1,\"165\":1,\"181\":1,\"270\":1}}],[\"mz��\\u0003���\\u0004���������������\",{\"1\":{\"134\":1}}],[\"mcmc算法内部维护一个\",{\"1\":{\"101\":1}}],[\"m=b\",{\"1\":{\"95\":1}}],[\"mm\",{\"1\":{\"89\":1}}],[\"mb\",{\"1\":{\"75\":1,\"76\":3}}],[\"mp占据模型内存\",{\"1\":{\"62\":1}}],[\"mp都在通信和计算效率之间权衡\",{\"1\":{\"53\":1}}],[\"mp\",{\"1\":{\"52\":1,\"82\":1}}],[\"mpiexec\",{\"1\":{\"24\":1}}],[\"mpi\",{\"0\":{\"27\":1},\"1\":{\"24\":33,\"27\":10,\"49\":2}}],[\"mpicc\",{\"1\":{\"24\":1}}],[\"mpich\",{\"0\":{\"24\":1},\"1\":{\"23\":1,\"24\":4,\"27\":1}}],[\"mpipathshare=$mpipath\",{\"1\":{\"24\":1}}],[\"mpipathlib=$mpipath\",{\"1\":{\"24\":1}}],[\"mpipathinclude=$mpipath\",{\"1\":{\"24\":1}}],[\"mpipathbin=$mpipath\",{\"1\":{\"24\":1}}],[\"mpipath=\",{\"1\":{\"24\":1}}],[\"mit\",{\"1\":{\"273\":1}}],[\"mini\",{\"1\":{\"85\":1}}],[\"minimize\",{\"1\":{\"43\":1}}],[\"microbatch的选择也非常重要\",{\"1\":{\"95\":1}}],[\"micro\",{\"0\":{\"85\":1},\"1\":{\"93\":1,\"95\":1,\"244\":2,\"265\":1}}],[\"microsoft\",{\"1\":{\"51\":1}}],[\"mseloss\",{\"1\":{\"45\":1}}],[\"m++\",{\"1\":{\"24\":2}}],[\"mulbyconstop\",{\"1\":{\"177\":1}}],[\"mulop\",{\"1\":{\"177\":1}}],[\"mul\",{\"1\":{\"119\":1,\"146\":1,\"177\":1}}],[\"multiply\",{\"1\":{\"177\":2}}],[\"multiple\",{\"1\":{\"118\":1}}],[\"multiplication\",{\"1\":{\"24\":2,\"177\":4}}],[\"multi\",{\"1\":{\"88\":1}}],[\"mutex>\",{\"1\":{\"14\":1}}],[\"mutex\",{\"1\":{\"14\":4}}],[\"must\",{\"1\":{\"10\":1}}],[\"myid\",{\"1\":{\"27\":3}}],[\"my\",{\"0\":{\"14\":1},\"1\":{\"19\":1,\"26\":1,\"120\":1,\"166\":1}}],[\"modify\",{\"1\":{\"177\":3}}],[\"mode\",{\"1\":{\"134\":1}}],[\"modelshttps\",{\"1\":{\"149\":1}}],[\"models\",{\"1\":{\"51\":1,\"88\":1,\"144\":1,\"273\":1}}],[\"model\",{\"0\":{\"55\":1,\"84\":1,\"89\":1},\"1\":{\"35\":1,\"45\":1,\"54\":1,\"73\":1,\"88\":1,\"89\":8,\"90\":1,\"97\":1,\"241\":1,\"243\":1,\"251\":2}}],[\"mooc\",{\"2\":{\"133\":1}}],[\"most\",{\"1\":{\"117\":1,\"129\":1}}],[\"momentum\",{\"1\":{\"76\":1}}],[\"monte\",{\"1\":{\"98\":1}}],[\"mon\",{\"1\":{\"43\":3}}],[\"monitoredtrainingsession\",{\"1\":{\"43\":2}}],[\"monitor\",{\"0\":{\"8\":1},\"1\":{\"168\":5}}],[\"more\",{\"1\":{\"10\":1}}],[\"map的相似性\",{\"1\":{\"274\":1}}],[\"macro\",{\"1\":{\"253\":1}}],[\"maas\",{\"1\":{\"241\":1}}],[\"markov\",{\"1\":{\"98\":1}}],[\"mask就是会用矩阵以对角线为界mask数据\",{\"1\":{\"161\":1}}],[\"mask引起的attention计算问题\",{\"1\":{\"161\":1}}],[\"masked\",{\"1\":{\"89\":4}}],[\"mask\",{\"1\":{\"89\":6}}],[\"matmulop\",{\"1\":{\"138\":1}}],[\"math\",{\"1\":{\"117\":2}}],[\"materialization\",{\"0\":{\"86\":1}}],[\"matrx\",{\"1\":{\"24\":4}}],[\"matrix3\",{\"1\":{\"24\":6}}],[\"matrix2\",{\"1\":{\"24\":10}}],[\"matrix1\",{\"1\":{\"24\":8}}],[\"matrix\",{\"1\":{\"24\":7,\"146\":1,\"234\":3}}],[\"mandelbrotserial\",{\"1\":{\"108\":2}}],[\"mandelbort\",{\"0\":{\"105\":1},\"1\":{\"107\":1}}],[\"managing\",{\"0\":{\"63\":1,\"64\":1}}],[\"many\",{\"1\":{\"19\":1}}],[\"maxiterations\",{\"1\":{\"108\":2}}],[\"max\",{\"1\":{\"27\":1,\"89\":1,\"184\":1}}],[\"malloc\",{\"1\":{\"24\":8}}],[\"make\",{\"0\":{\"24\":1},\"1\":{\"10\":1,\"12\":1,\"21\":1,\"24\":3,\"177\":1}}],[\"may\",{\"1\":{\"10\":1,\"19\":1,\"114\":1,\"117\":1}}],[\"main函数末尾漏掉return\",{\"1\":{\"202\":1}}],[\"main\",{\"1\":{\"9\":2,\"13\":2,\"14\":4,\"24\":1,\"27\":1,\"186\":1,\"196\":1,\"203\":1,\"210\":1,\"216\":1,\"222\":1,\"228\":1,\"234\":1}}],[\"message\",{\"1\":{\"168\":1}}],[\"meanop\",{\"1\":{\"138\":1}}],[\"means\",{\"1\":{\"117\":1,\"168\":1}}],[\"meaning\",{\"1\":{\"10\":1}}],[\"megatron\",{\"0\":{\"88\":1,\"144\":1},\"1\":{\"88\":2,\"90\":2,\"245\":1,\"266\":1}}],[\"memory的计算有以下几块\",{\"1\":{\"146\":1}}],[\"memory\",{\"0\":{\"54\":1,\"56\":1,\"58\":1,\"62\":1,\"64\":1,\"72\":1,\"146\":1},\"1\":{\"51\":1,\"146\":1,\"168\":2,\"240\":1,\"241\":1,\"288\":1}}],[\"media\",{\"1\":{\"24\":1}}],[\"method=init\",{\"1\":{\"89\":1}}],[\"met\",{\"1\":{\"4\":1}}],[\"m\",{\"1\":{\"9\":1,\"10\":2,\"12\":4,\"14\":7,\"24\":8,\"26\":1,\"75\":1,\"76\":1,\"131\":2,\"216\":4,\"222\":6,\"223\":1,\"234\":3,\"235\":1,\"244\":2,\"265\":1}}],[\"wrapper\",{\"1\":{\"285\":1}}],[\"write\",{\"1\":{\"24\":1}}],[\"ws−1\",{\"1\":{\"268\":1}}],[\"wv\",{\"1\":{\"260\":1}}],[\"wk\",{\"1\":{\"260\":1}}],[\"wq\",{\"1\":{\"260\":1}}],[\"w1​\",{\"1\":{\"148\":1}}],[\"w1s​\",{\"1\":{\"148\":2}}],[\"w2​\",{\"1\":{\"148\":1}}],[\"w2​=z2h​b2r​\",{\"1\":{\"148\":1}}],[\"w2s​\",{\"1\":{\"148\":2}}],[\"w\",{\"1\":{\"148\":1,\"168\":3,\"268\":6}}],[\"wo\",{\"1\":{\"260\":2}}],[\"would\",{\"0\":{\"128\":1},\"1\":{\"128\":2}}],[\"world\",{\"1\":{\"24\":8,\"27\":3,\"78\":1}}],[\"worker\",{\"1\":{\"243\":1}}],[\"workerargs\",{\"1\":{\"108\":1}}],[\"workload\",{\"0\":{\"161\":1},\"1\":{\"161\":1}}],[\"workonce\",{\"1\":{\"14\":2}}],[\"worktime\",{\"1\":{\"10\":2,\"12\":11,\"14\":7}}],[\"working\",{\"1\":{\"9\":1,\"10\":1,\"12\":2,\"14\":4}}],[\"work\",{\"0\":{\"12\":1},\"1\":{\"9\":1,\"10\":2,\"12\":3,\"14\":1,\"177\":3}}],[\"wget\",{\"1\":{\"106\":1}}],[\"www\",{\"1\":{\"49\":1}}],[\"wiv​\",{\"1\":{\"260\":1}}],[\"wik​\",{\"1\":{\"260\":1}}],[\"wiq​\",{\"1\":{\"260\":1}}],[\"winter\",{\"1\":{\"176\":1}}],[\"wise\",{\"1\":{\"139\":2,\"177\":12,\"179\":2,\"259\":1}}],[\"width\",{\"1\":{\"108\":2}}],[\"with\",{\"0\":{\"21\":1,\"163\":1},\"1\":{\"26\":1,\"43\":1,\"45\":1,\"129\":1,\"155\":1,\"177\":9}}],[\"will\",{\"1\":{\"10\":1,\"14\":4,\"117\":1,\"119\":2,\"120\":1,\"138\":1,\"166\":1}}],[\"whose\",{\"1\":{\"168\":1,\"177\":2}}],[\"what\",{\"0\":{\"128\":1,\"171\":1},\"1\":{\"128\":2,\"171\":1}}],[\"which\",{\"1\":{\"114\":1,\"168\":1}}],[\"while\",{\"1\":{\"12\":1,\"14\":1,\"43\":1,\"196\":1,\"203\":1,\"210\":1,\"216\":1,\"222\":1}}],[\"where\",{\"0\":{\"54\":1},\"1\":{\"168\":1}}],[\"when\",{\"1\":{\"7\":1,\"13\":1,\"120\":1,\"168\":1}}],[\"warmup预热不再赘述\",{\"1\":{\"284\":1}}],[\"warning\",{\"1\":{\"8\":1,\"10\":1,\"13\":2}}],[\"wall\",{\"1\":{\"241\":3}}],[\"way\",{\"1\":{\"117\":1,\"166\":1}}],[\"want\",{\"1\":{\"21\":1}}],[\"was\",{\"1\":{\"10\":1}}],[\"waiting\",{\"1\":{\"9\":3,\"10\":6,\"12\":14,\"13\":1,\"14\":18}}],[\"wait\",{\"1\":{\"9\":2,\"10\":2,\"12\":4,\"13\":2,\"14\":5}}],[\"welcome\",{\"0\":{\"114\":1}}],[\"week1\",{\"0\":{\"112\":1}}],[\"weight\",{\"1\":{\"89\":2,\"250\":1,\"268\":1}}],[\"we\",{\"1\":{\"8\":4,\"10\":4,\"12\":1,\"13\":4,\"19\":1,\"21\":2,\"23\":1,\"24\":3,\"26\":4,\"27\":2,\"117\":1,\"118\":1,\"119\":2,\"128\":2,\"168\":1,\"177\":1}}],[\"were\",{\"1\":{\"7\":1}}],[\"aj​\",{\"1\":{\"262\":2}}],[\"ai​\",{\"1\":{\"259\":1}}],[\"a和b是权重矩阵\",{\"1\":{\"259\":1}}],[\"a和b是linear的权重weight矩阵\",{\"1\":{\"148\":1}}],[\"aware\",{\"1\":{\"250\":1,\"268\":1}}],[\"awq将量化的\",{\"1\":{\"268\":1}}],[\"awq提出\",{\"1\":{\"268\":1}}],[\"awq的目标是找到一个最佳的s\",{\"1\":{\"268\":1}}],[\"awq引入了一个尺度因子s\",{\"1\":{\"268\":1}}],[\"awq发现\",{\"1\":{\"268\":1}}],[\"awq认为\",{\"1\":{\"268\":1}}],[\"awq通过保护这约1\",{\"1\":{\"250\":1}}],[\"awq\",{\"1\":{\"250\":1,\"268\":1}}],[\"a2​\",{\"1\":{\"245\":1,\"259\":1}}],[\"a1​\",{\"1\":{\"245\":1,\"259\":1}}],[\"a100的显存\",{\"1\":{\"145\":1}}],[\"a=\",{\"1\":{\"245\":1,\"259\":1}}],[\"ap​\",{\"1\":{\"259\":1}}],[\"append\",{\"1\":{\"177\":1}}],[\"appendix\",{\"0\":{\"131\":1}}],[\"api\",{\"1\":{\"42\":1,\"49\":1}}],[\"alpha$是一个超参数\",{\"1\":{\"268\":1}}],[\"alphafold3采用了类似扩散模型的生成过程\",{\"1\":{\"255\":1}}],[\"alphafold3作为典型代表\",{\"1\":{\"255\":1}}],[\"along\",{\"1\":{\"177\":1}}],[\"always\",{\"1\":{\"168\":1}}],[\"also\",{\"1\":{\"117\":1,\"118\":2,\"120\":1,\"168\":1}}],[\"all来在注意力头上收集结果\",{\"1\":{\"150\":1}}],[\"all通信操作\",{\"1\":{\"150\":1}}],[\"allocate\",{\"1\":{\"78\":4}}],[\"allows\",{\"1\":{\"21\":1}}],[\"allreduce将这个过程分为reduce\",{\"1\":{\"39\":1}}],[\"allreduce就是要让每块gpu上的数据都变成箭头右边汇总的样子\",{\"1\":{\"39\":1}}],[\"allreduce\",{\"0\":{\"39\":1},\"1\":{\"42\":2,\"154\":1}}],[\"allreduce的方式\",{\"1\":{\"38\":1}}],[\"all\",{\"0\":{\"41\":1,\"54\":1},\"1\":{\"9\":2,\"10\":3,\"12\":3,\"13\":1,\"14\":9,\"79\":1,\"89\":1,\"131\":1,\"148\":1,\"151\":2,\"168\":1,\"177\":4,\"245\":2,\"254\":1,\"259\":1,\"260\":1,\"261\":2,\"262\":1,\"267\":5}}],[\"acm\",{\"0\":{\"190\":1}}],[\"activations\",{\"1\":{\"261\":1}}],[\"activationmemoryperlayer​=sbh\",{\"1\":{\"148\":1}}],[\"activationmemoryperlayer的值降为\",{\"1\":{\"147\":1}}],[\"activationmemoryperlayer=sbh\",{\"1\":{\"146\":1,\"147\":1}}],[\"activation指fwd和bwd梯度计算中创建的所有tensor\",{\"1\":{\"146\":1}}],[\"activation\",{\"0\":{\"62\":1,\"70\":1,\"146\":1},\"1\":{\"95\":1,\"144\":1,\"145\":1,\"250\":1,\"268\":2}}],[\"acceptance\",{\"1\":{\"251\":1,\"263\":1}}],[\"accelerating\",{\"1\":{\"44\":1}}],[\"access\",{\"1\":{\"21\":2}}],[\"abstract\",{\"0\":{\"52\":1,\"82\":1,\"145\":1}}],[\"abs\",{\"1\":{\"38\":1,\"44\":1,\"51\":1,\"81\":1,\"88\":1,\"285\":1,\"287\":1}}],[\"absolute\",{\"1\":{\"24\":1}}],[\"about\",{\"1\":{\"18\":1,\"122\":1}}],[\"adjoints\",{\"1\":{\"177\":2}}],[\"adjoint\",{\"1\":{\"139\":1,\"177\":9,\"179\":1}}],[\"adam\",{\"1\":{\"55\":1}}],[\"adam本身对每一个参数都需要保留momentum和variance两个参数进行数据更新\",{\"1\":{\"55\":1}}],[\"adagradoptimizer\",{\"1\":{\"43\":1}}],[\"admin\",{\"1\":{\"21\":3,\"26\":1}}],[\"addbyconstop\",{\"1\":{\"177\":1}}],[\"addition\",{\"1\":{\"177\":2}}],[\"addop\",{\"1\":{\"177\":1}}],[\"address\",{\"1\":{\"24\":1}}],[\"add\",{\"1\":{\"12\":2,\"21\":3,\"25\":1,\"26\":1,\"128\":1,\"177\":4}}],[\"atl\",{\"1\":{\"279\":1}}],[\"attrs\",{\"1\":{\"177\":6}}],[\"attrs=\",{\"1\":{\"177\":4}}],[\"attribute\",{\"1\":{\"98\":2}}],[\"attention实现\",{\"1\":{\"293\":1}}],[\"attention缺少高效的attention实现\",{\"1\":{\"159\":1}}],[\"attention方式可以突破序列长度限制\",{\"1\":{\"157\":1}}],[\"attention和fwd\",{\"1\":{\"157\":1}}],[\"attention和fwd计算\",{\"1\":{\"157\":1}}],[\"attention时\",{\"1\":{\"157\":1}}],[\"attention的外循环\",{\"1\":{\"157\":1}}],[\"attention来解决此问题\",{\"1\":{\"154\":1}}],[\"attention例子\",{\"1\":{\"146\":1}}],[\"attention块的activation\",{\"1\":{\"146\":1}}],[\"attention块中包含一个self\",{\"1\":{\"146\":1}}],[\"attention\",{\"0\":{\"155\":1,\"157\":1},\"1\":{\"146\":3,\"155\":1,\"157\":1,\"260\":1}}],[\"atomic<bool>\",{\"1\":{\"14\":1}}],[\"at\",{\"1\":{\"12\":1,\"117\":1,\"128\":1,\"168\":1}}],[\"autoregressive\",{\"1\":{\"241\":1}}],[\"autodiff\",{\"0\":{\"176\":1}}],[\"automatic\",{\"0\":{\"136\":1,\"171\":1},\"1\":{\"171\":1}}],[\"autograd\",{\"1\":{\"89\":1}}],[\"auto\",{\"0\":{\"137\":1},\"1\":{\"12\":1,\"13\":1,\"14\":3,\"216\":2}}],[\"ans\",{\"1\":{\"184\":3,\"186\":3}}],[\"answer\",{\"0\":{\"14\":1},\"1\":{\"10\":1}}],[\"another\",{\"1\":{\"177\":2}}],[\"an\",{\"1\":{\"131\":1,\"174\":1,\"177\":4}}],[\"analysing\",{\"1\":{\"13\":1}}],[\"andmholm\",{\"1\":{\"171\":1}}],[\"and\",{\"0\":{\"8\":1,\"9\":1,\"24\":1,\"55\":1,\"59\":1,\"60\":1,\"61\":1,\"122\":1,\"163\":1},\"1\":{\"9\":3,\"10\":1,\"12\":1,\"13\":2,\"14\":2,\"23\":1,\"25\":1,\"26\":2,\"27\":1,\"38\":1,\"45\":1,\"97\":1,\"114\":2,\"117\":1,\"120\":1,\"122\":2,\"125\":1,\"129\":2,\"148\":1,\"168\":2,\"177\":4}}],[\"again\",{\"1\":{\"10\":1}}],[\"after\",{\"1\":{\"10\":2,\"14\":1,\"23\":1,\"117\":1}}],[\"ary\",{\"1\":{\"114\":1}}],[\"arxiv\",{\"1\":{\"38\":1,\"44\":1,\"51\":1,\"73\":2,\"81\":1,\"88\":1,\"90\":1,\"97\":1,\"144\":1,\"149\":1,\"152\":1,\"155\":1,\"158\":1,\"174\":1,\"281\":1,\"285\":1,\"287\":1}}],[\"args\",{\"1\":{\"108\":16}}],[\"argv\",{\"1\":{\"24\":2,\"27\":2}}],[\"argc\",{\"1\":{\"24\":2,\"27\":2}}],[\"article\",{\"1\":{\"21\":1,\"23\":1}}],[\"array\",{\"1\":{\"9\":1,\"177\":2}}],[\"area\",{\"1\":{\"118\":2,\"119\":3}}],[\"are\",{\"1\":{\"7\":1,\"8\":2,\"10\":1,\"13\":1,\"14\":2,\"138\":1,\"168\":1,\"177\":1}}],[\"as2b\",{\"1\":{\"146\":2}}],[\"assert\",{\"1\":{\"139\":1,\"177\":13,\"179\":1}}],[\"assignment\",{\"0\":{\"127\":1},\"1\":{\"128\":1,\"130\":1}}],[\"assignments\",{\"1\":{\"125\":1}}],[\"assignment1\",{\"0\":{\"104\":1}}],[\"asst1\",{\"1\":{\"106\":1}}],[\"asst1并不需要额外配置运行环境\",{\"1\":{\"106\":1}}],[\"asc\",{\"1\":{\"24\":2},\"2\":{\"15\":1,\"16\":1,\"28\":1,\"29\":1}}],[\"asc2025\",{\"0\":{\"1\":1,\"17\":1},\"1\":{\"26\":1}}],[\"as\",{\"1\":{\"8\":1,\"12\":1,\"43\":3,\"45\":3,\"168\":4,\"177\":8,\"241\":1}}],[\"a\",{\"0\":{\"166\":1},\"1\":{\"7\":1,\"8\":2,\"9\":2,\"12\":3,\"13\":1,\"14\":1,\"19\":1,\"24\":7,\"25\":1,\"26\":3,\"27\":1,\"60\":1,\"62\":1,\"114\":2,\"117\":5,\"119\":3,\"139\":3,\"166\":1,\"168\":4,\"177\":51,\"179\":3,\"216\":1,\"241\":1,\"245\":3,\"259\":1,\"262\":2}}],[\"sj​\",{\"1\":{\"268\":1}}],[\"sj∗​=\",{\"1\":{\"268\":1}}],[\"sx\",{\"1\":{\"268\":1}}],[\"s×b×h\",{\"1\":{\"261\":1}}],[\"swap\",{\"1\":{\"234\":2}}],[\"sdb\",{\"1\":{\"168\":1}}],[\"system\",{\"1\":{\"149\":1,\"152\":1}}],[\"sysctl\",{\"1\":{\"21\":1,\"26\":1}}],[\"sqrtop\",{\"1\":{\"138\":1}}],[\"square\",{\"1\":{\"119\":2}}],[\"skip\",{\"1\":{\"125\":1,\"126\":1}}],[\"same\",{\"1\":{\"117\":1,\"118\":1,\"177\":4}}],[\"sample并行\",{\"1\":{\"98\":1}}],[\"sample\",{\"1\":{\"98\":5,\"99\":1}}],[\"sa两个环节做了如下优化\",{\"1\":{\"89\":1}}],[\"sp执行完毕后\",{\"1\":{\"293\":1}}],[\"sp只拥有1\",{\"1\":{\"293\":1}}],[\"sp的kv\",{\"1\":{\"293\":1}}],[\"sp中的完整通信流程\",{\"1\":{\"267\":1}}],[\"sp将激活内存的占用降低了p倍\",{\"1\":{\"261\":1}}],[\"sp避免了在单个设备上实例化完整的注意力矩阵\",{\"1\":{\"254\":1}}],[\"sp\",{\"1\":{\"254\":1,\"293\":5,\"294\":1}}],[\"speculative\",{\"1\":{\"240\":1}}],[\"specified\",{\"1\":{\"177\":1}}],[\"specific\",{\"1\":{\"8\":1}}],[\"sp​\",{\"1\":{\"162\":1}}],[\"sp没有引入更多的通信代价\",{\"1\":{\"148\":1}}],[\"spatial\",{\"1\":{\"107\":1}}],[\"sparse\",{\"1\":{\"89\":1}}],[\"split\",{\"0\":{\"85\":1},\"1\":{\"292\":1}}],[\"src\",{\"1\":{\"78\":1}}],[\"srand\",{\"1\":{\"24\":1}}],[\"scheduling\",{\"0\":{\"163\":1}}],[\"schedule\",{\"0\":{\"77\":1}}],[\"scores问题\",{\"1\":{\"154\":1}}],[\"scanf\",{\"1\":{\"186\":1}}],[\"scale\",{\"1\":{\"73\":1,\"89\":1,\"90\":1,\"168\":1,\"268\":1}}],[\"scatter然后一个all\",{\"1\":{\"148\":1}}],[\"scatter操作\",{\"1\":{\"148\":1}}],[\"scatter更新各个参数状态\",{\"1\":{\"70\":1}}],[\"scatter阶段结束\",{\"1\":{\"40\":1}}],[\"scatter\",{\"0\":{\"40\":1},\"1\":{\"79\":1,\"148\":2,\"254\":1,\"261\":2,\"262\":1,\"267\":1}}],[\"scatter和all\",{\"1\":{\"39\":1}}],[\"sgd\",{\"1\":{\"45\":1}}],[\"sbin\",{\"1\":{\"26\":1}}],[\"sshd\",{\"1\":{\"26\":2}}],[\"ssh\",{\"0\":{\"26\":1},\"1\":{\"26\":8}}],[\"si\",{\"1\":{\"168\":2}}],[\"significant\",{\"1\":{\"168\":1}}],[\"signal\",{\"1\":{\"12\":1,\"14\":1}}],[\"sin\",{\"1\":{\"117\":4}}],[\"simultaneously\",{\"1\":{\"118\":1}}],[\"simulator\",{\"1\":{\"97\":1,\"98\":1}}],[\"simplest\",{\"0\":{\"167\":1}}],[\"simple\",{\"1\":{\"114\":1,\"168\":2}}],[\"size=1的\",{\"1\":{\"255\":1}}],[\"sizeof\",{\"1\":{\"24\":8}}],[\"size\",{\"0\":{\"71\":1},\"1\":{\"24\":51,\"27\":1,\"75\":4,\"76\":1,\"78\":1,\"82\":1,\"89\":4,\"92\":1,\"95\":2,\"108\":2,\"177\":4,\"243\":2}}],[\"shape\",{\"1\":{\"177\":30}}],[\"sharded\",{\"1\":{\"35\":1}}],[\"share\",{\"1\":{\"24\":1}}],[\"shb\",{\"1\":{\"146\":1}}],[\"should\",{\"1\":{\"14\":1,\"27\":1,\"43\":1}}],[\"subtraction\",{\"1\":{\"177\":2}}],[\"subtract\",{\"1\":{\"177\":1}}],[\"subop\",{\"1\":{\"177\":1}}],[\"subcmd\",{\"1\":{\"168\":1}}],[\"suspend\",{\"1\":{\"168\":1}}],[\"suspended\",{\"1\":{\"168\":1}}],[\"suite\",{\"1\":{\"128\":1}}],[\"supported\",{\"1\":{\"118\":1}}],[\"super\",{\"1\":{\"76\":1,\"89\":1}}],[\"suprisely\",{\"1\":{\"10\":1}}],[\"sucessfully\",{\"0\":{\"13\":1},\"1\":{\"13\":1}}],[\"such\",{\"1\":{\"8\":1,\"168\":1}}],[\"sure\",{\"1\":{\"10\":1,\"12\":1,\"21\":1,\"177\":1}}],[\"sumop\",{\"1\":{\"177\":2}}],[\"summary\",{\"1\":{\"130\":1}}],[\"sum\",{\"1\":{\"10\":1,\"177\":12}}],[\"s\",{\"0\":{\"171\":1},\"1\":{\"9\":1,\"18\":1,\"24\":1,\"27\":1,\"119\":1,\"174\":1,\"186\":3,\"228\":5,\"261\":4,\"267\":2}}],[\"service\",{\"1\":{\"241\":1}}],[\"sequence维度\",{\"1\":{\"262\":1}}],[\"sequences的attention\",{\"1\":{\"154\":1}}],[\"sequence\",{\"0\":{\"148\":1,\"152\":1},\"1\":{\"149\":1,\"152\":2,\"254\":1,\"287\":1,\"293\":1}}],[\"section\",{\"1\":{\"128\":1}}],[\"second\",{\"1\":{\"9\":1}}],[\"several\",{\"1\":{\"114\":1}}],[\"semantics\",{\"1\":{\"93\":1}}],[\"self\",{\"1\":{\"89\":24,\"139\":3,\"154\":1,\"177\":45,\"179\":3}}],[\"select\",{\"1\":{\"89\":1}}],[\"session\",{\"1\":{\"117\":1,\"131\":1}}],[\"sess\",{\"1\":{\"43\":3}}],[\"send\",{\"1\":{\"24\":3}}],[\"see\",{\"1\":{\"13\":1,\"24\":1}}],[\"seem\",{\"1\":{\"10\":1,\"24\":1}}],[\"setup\",{\"0\":{\"126\":1},\"1\":{\"45\":1,\"125\":1}}],[\"setting\",{\"1\":{\"24\":1}}],[\"settings\",{\"1\":{\"18\":1}}],[\"set\",{\"0\":{\"10\":1,\"24\":1},\"1\":{\"6\":1,\"9\":3,\"10\":2,\"24\":1,\"26\":3,\"168\":3,\"177\":1}}],[\"stfw\",{\"1\":{\"166\":1}}],[\"stuck\",{\"1\":{\"128\":1}}],[\"string\",{\"0\":{\"191\":1},\"1\":{\"203\":1,\"210\":1,\"216\":1,\"228\":1}}],[\"structure\",{\"1\":{\"114\":1,\"120\":1}}],[\"strategy\",{\"0\":{\"76\":1},\"1\":{\"100\":1,\"101\":4}}],[\"str\",{\"1\":{\"43\":1,\"177\":1}}],[\"standard\",{\"1\":{\"293\":2}}],[\"stanford\",{\"1\":{\"106\":1}}],[\"stages\",{\"1\":{\"247\":1,\"265\":1}}],[\"stage\",{\"1\":{\"244\":1}}],[\"state需要的显存大小\",{\"1\":{\"145\":1}}],[\"states\",{\"0\":{\"55\":2},\"1\":{\"53\":1,\"54\":2,\"78\":2}}],[\"status\",{\"1\":{\"24\":3,\"168\":2}}],[\"starter\",{\"1\":{\"124\":1}}],[\"started\",{\"0\":{\"124\":1}}],[\"startrow\",{\"1\":{\"108\":5}}],[\"starting\",{\"1\":{\"12\":1}}],[\"start\",{\"0\":{\"21\":1},\"1\":{\"9\":2,\"10\":1,\"12\":2,\"14\":3,\"26\":2,\"89\":4}}],[\"storage\",{\"1\":{\"117\":1}}],[\"store\",{\"1\":{\"13\":1,\"14\":1}}],[\"stop\",{\"1\":{\"13\":2,\"43\":1,\"168\":1}}],[\"stdc++\",{\"1\":{\"196\":1,\"203\":1,\"210\":1,\"216\":1,\"222\":1}}],[\"stderr\",{\"1\":{\"27\":1}}],[\"std\",{\"1\":{\"10\":1,\"12\":1,\"14\":7,\"24\":1,\"184\":2,\"196\":1,\"203\":1,\"210\":1,\"216\":1,\"222\":1,\"228\":1,\"234\":1}}],[\"step使用\",{\"1\":{\"293\":1}}],[\"step后\",{\"1\":{\"293\":1}}],[\"step全部的kv\",{\"1\":{\"293\":1}}],[\"step中\",{\"1\":{\"293\":1}}],[\"step内\",{\"1\":{\"293\":1}}],[\"step计算出fresh\",{\"1\":{\"293\":1}}],[\"step\",{\"1\":{\"45\":1,\"78\":2,\"168\":2,\"293\":1}}],[\"step3\",{\"0\":{\"10\":1,\"21\":1}}],[\"step2\",{\"0\":{\"9\":1,\"13\":1,\"20\":1}}],[\"step1\",{\"0\":{\"8\":1,\"12\":1,\"19\":1,\"24\":1},\"1\":{\"10\":1}}],[\"steps\",{\"1\":{\"7\":1}}],[\"sort\",{\"1\":{\"222\":1}}],[\"softmax的输出也会用于反向的计算\",{\"1\":{\"146\":1}}],[\"softmaxop\",{\"1\":{\"138\":1}}],[\"sosd\",{\"2\":{\"102\":1,\"164\":1,\"269\":1,\"295\":1}}],[\"soap\",{\"1\":{\"97\":1,\"98\":2,\"99\":1}}],[\"solve\",{\"1\":{\"119\":1,\"166\":1}}],[\"solo\",{\"1\":{\"49\":1}}],[\"solution\",{\"1\":{\"19\":1}}],[\"some\",{\"0\":{\"10\":1},\"1\":{\"8\":2,\"10\":3,\"18\":1,\"125\":1}}],[\"so\",{\"1\":{\"8\":2,\"10\":2,\"13\":1,\"18\":1,\"114\":1,\"117\":1,\"125\":1,\"128\":1,\"166\":1}}],[\"ubuntu\",{\"1\":{\"166\":1}}],[\"uber\",{\"1\":{\"38\":1}}],[\"ulysses部分\",{\"1\":{\"291\":1}}],[\"ulysses的核心设计\",{\"0\":{\"151\":1}}],[\"ulysses还使用另一个all\",{\"1\":{\"150\":1}}],[\"ulysses将各个样本在序列维度上分割给参与的gpu\",{\"1\":{\"150\":1}}],[\"ulysses\",{\"0\":{\"149\":1},\"1\":{\"149\":1}}],[\"u\",{\"1\":{\"128\":1,\"253\":1}}],[\"ues\",{\"1\":{\"23\":1}}],[\"u014185088\",{\"1\":{\"23\":1}}],[\"usp\",{\"0\":{\"291\":1}}],[\"usr\",{\"1\":{\"26\":1}}],[\"us\",{\"1\":{\"21\":1,\"119\":1}}],[\"using\",{\"1\":{\"13\":1,\"14\":5,\"24\":1,\"81\":1,\"88\":1,\"90\":1,\"129\":1,\"196\":1,\"203\":1,\"210\":1,\"216\":1,\"222\":1,\"228\":1,\"234\":1}}],[\"useful\",{\"0\":{\"131\":1}}],[\"used\",{\"1\":{\"117\":1}}],[\"user\",{\"1\":{\"24\":1,\"49\":1}}],[\"use\",{\"1\":{\"6\":1,\"8\":1,\"13\":3,\"21\":1,\"24\":3,\"26\":3,\"27\":1,\"130\":1}}],[\"up\",{\"1\":{\"10\":1,\"129\":1}}],[\"update\",{\"1\":{\"9\":1,\"10\":1,\"12\":3,\"14\":1,\"45\":1,\"76\":2,\"78\":1}}],[\"unconditional\",{\"1\":{\"292\":1}}],[\"unsqueeze\",{\"1\":{\"177\":1}}],[\"unsigned\",{\"1\":{\"24\":2}}],[\"unziped\",{\"1\":{\"19\":1}}],[\"unzip\",{\"0\":{\"19\":1},\"1\":{\"23\":1}}],[\"unique\",{\"0\":{\"76\":1},\"1\":{\"14\":1}}],[\"unbalanced\",{\"0\":{\"12\":1}}],[\"unlocking\",{\"1\":{\"128\":1}}],[\"unlock\",{\"1\":{\"10\":2,\"12\":4,\"14\":4}}],[\"understand\",{\"0\":{\"9\":1}}],[\"unnecessary\",{\"0\":{\"4\":1}}],[\"r>pd​pt​​\",{\"1\":{\"263\":1}}],[\"r≤min\",{\"1\":{\"263\":1}}],[\"rsa感觉就是将输入序列进行切分\",{\"1\":{\"154\":1}}],[\"rdata����\\u0006��\",{\"1\":{\"134\":1}}],[\"r\",{\"0\":{\"61\":1,\"69\":1},\"1\":{\"263\":1}}],[\"rinfo\",{\"1\":{\"168\":1}}],[\"ring\",{\"0\":{\"39\":1,\"155\":1,\"157\":1},\"1\":{\"39\":1,\"42\":2,\"148\":1,\"155\":1,\"157\":1}}],[\"right数组的大小应该是m而不是当前的m值\",{\"1\":{\"221\":1}}],[\"right\",{\"1\":{\"24\":1,\"222\":6}}],[\"ratiobubble​≈mp−1​\",{\"1\":{\"265\":1}}],[\"ratiobubble​≈2m+p−12\",{\"1\":{\"265\":1}}],[\"ratiobubble​=ttotal​tbubble​​=m\",{\"1\":{\"265\":1}}],[\"ratio\",{\"1\":{\"265\":1}}],[\"rate\",{\"1\":{\"251\":1}}],[\"ray\",{\"1\":{\"256\":1}}],[\"raise\",{\"1\":{\"177\":6}}],[\"radius\",{\"1\":{\"118\":5,\"119\":3}}],[\"range\",{\"1\":{\"78\":5,\"89\":1,\"177\":1}}],[\"rank=1拥有的fresh\",{\"1\":{\"293\":1}}],[\"rank=0拥有的fresh\",{\"1\":{\"293\":1}}],[\"rank\",{\"1\":{\"24\":4,\"27\":1,\"43\":3,\"78\":5,\"89\":2}}],[\"randn\",{\"1\":{\"45\":2}}],[\"rand\",{\"1\":{\"24\":2}}],[\"race\",{\"1\":{\"5\":1,\"8\":2,\"13\":1}}],[\"router负责接收所有请求并进行负载均衡\",{\"1\":{\"256\":1}}],[\"router\",{\"1\":{\"256\":1}}],[\"rounds\",{\"1\":{\"10\":1,\"14\":3}}],[\"round\",{\"1\":{\"9\":1,\"10\":1,\"12\":1,\"14\":2}}],[\"rows++\",{\"1\":{\"108\":1}}],[\"rows\",{\"1\":{\"108\":6}}],[\"row\",{\"1\":{\"89\":1,\"233\":1,\"234\":5,\"245\":1,\"259\":1}}],[\"root\",{\"1\":{\"23\":1,\"24\":4,\"27\":1}}],[\"runtimeerror\",{\"1\":{\"177\":2}}],[\"running\",{\"0\":{\"130\":1},\"1\":{\"130\":1}}],[\"run\",{\"0\":{\"8\":1,\"27\":1},\"1\":{\"8\":2,\"10\":2,\"14\":2,\"21\":3,\"24\":1,\"26\":1,\"43\":1,\"45\":2,\"131\":1,\"134\":1,\"168\":3}}],[\"rejection\",{\"1\":{\"263\":1}}],[\"replication\",{\"1\":{\"243\":1}}],[\"repo到本地来开始lab了\",{\"1\":{\"106\":1}}],[\"requires\",{\"1\":{\"177\":1}}],[\"regard\",{\"1\":{\"177\":2}}],[\"regards\",{\"1\":{\"177\":1}}],[\"regarded\",{\"1\":{\"168\":1}}],[\"register\",{\"1\":{\"168\":1}}],[\"region\",{\"1\":{\"89\":1}}],[\"reluop\",{\"1\":{\"138\":1}}],[\"releases\",{\"1\":{\"106\":1}}],[\"remaining\",{\"1\":{\"128\":1}}],[\"rename\",{\"1\":{\"117\":1}}],[\"re\",{\"0\":{\"86\":1},\"1\":{\"119\":1}}],[\"reducing\",{\"0\":{\"62\":1},\"1\":{\"144\":1}}],[\"reduce和all\",{\"1\":{\"267\":1}}],[\"reduce通信\",{\"1\":{\"245\":1,\"259\":1}}],[\"reduce操作将各头的输出聚合并投影\",{\"1\":{\"245\":1}}],[\"reduce操作\",{\"1\":{\"148\":1}}],[\"reduce\",{\"0\":{\"40\":1},\"1\":{\"40\":1,\"78\":1,\"79\":1,\"89\":3,\"147\":2,\"148\":1,\"245\":1,\"259\":3,\"260\":1,\"261\":1,\"262\":2,\"267\":3}}],[\"resolution\",{\"1\":{\"273\":1}}],[\"reshape\",{\"1\":{\"177\":2}}],[\"residual\",{\"0\":{\"56\":1},\"1\":{\"53\":1,\"54\":1}}],[\"resultmg0=multiplication\",{\"1\":{\"24\":1}}],[\"resultmg0\",{\"1\":{\"24\":3}}],[\"resultmg\",{\"1\":{\"24\":7}}],[\"result\",{\"1\":{\"24\":4,\"177\":1}}],[\"recent\",{\"1\":{\"117\":1}}],[\"recomputation两种方法\",{\"1\":{\"145\":1}}],[\"recomputation\",{\"1\":{\"144\":1}}],[\"recompute也是一种可以考虑的方法\",{\"1\":{\"95\":1}}],[\"recompile\",{\"1\":{\"10\":1}}],[\"recv\",{\"1\":{\"24\":3}}],[\"revmg\",{\"1\":{\"24\":5}}],[\"reason\",{\"1\":{\"13\":1}}],[\"read\",{\"0\":{\"9\":1},\"1\":{\"6\":1,\"8\":1}}],[\"returns\",{\"1\":{\"177\":5}}],[\"return\",{\"1\":{\"10\":1,\"12\":3,\"14\":5,\"24\":3,\"78\":2,\"89\":3,\"119\":3,\"120\":1,\"129\":1,\"139\":3,\"177\":60,\"179\":3,\"186\":1,\"196\":1,\"210\":1,\"216\":1,\"222\":2,\"228\":1,\"234\":1}}],[\"reference\",{\"1\":{\"2\":1,\"21\":1,\"23\":1,\"171\":1,\"177\":3}}],[\"dk​​qi​kit​​\",{\"1\":{\"260\":1}}],[\"dk​​qkt​\",{\"1\":{\"146\":1}}],[\"days\",{\"1\":{\"221\":1,\"222\":2}}],[\"data���|����\",{\"1\":{\"134\":1}}],[\"dataset\",{\"1\":{\"78\":1}}],[\"data\",{\"1\":{\"5\":1,\"8\":3,\"10\":8,\"12\":6,\"13\":1,\"14\":14,\"24\":1,\"35\":2,\"44\":1,\"97\":1,\"114\":1,\"117\":2,\"240\":1,\"268\":1,\"287\":1}}],[\"drafting\",{\"1\":{\"263\":1}}],[\"draft\",{\"1\":{\"251\":1}}],[\"dragon\",{\"0\":{\"191\":1}}],[\"dropout以及各种逐元素操作\",{\"1\":{\"261\":1}}],[\"dropout\",{\"1\":{\"148\":2}}],[\"dropout的mask层矩阵的大小与softmax的输出一样\",{\"1\":{\"146\":1}}],[\"dropout层需要sbh\",{\"1\":{\"146\":1}}],[\"dropout层需要mask的大小为sbh\",{\"1\":{\"146\":1}}],[\"dropout层\",{\"1\":{\"146\":1}}],[\"dfa相较hf每一个flashattention+ffn约能够减少一个fa的计算量\",{\"1\":{\"163\":1}}],[\"d​qkt​\",{\"1\":{\"151\":1}}],[\"dtype\",{\"1\":{\"89\":1}}],[\"dtype=args\",{\"1\":{\"89\":1}}],[\"dxdf​=h→0lim​hf\",{\"1\":{\"175\":1}}],[\"dx\",{\"1\":{\"78\":3}}],[\"ddp支持用户使用统一的api\",{\"1\":{\"49\":1}}],[\"ddp支持三种通讯库\",{\"1\":{\"49\":1}}],[\"ddp\",{\"0\":{\"44\":1}}],[\"dp无法适用\",{\"1\":{\"243\":1}}],[\"dp并行维度\",{\"1\":{\"95\":1}}],[\"dp的区别是\",{\"1\":{\"70\":1}}],[\"dp\",{\"0\":{\"36\":1,\"60\":1,\"65\":1},\"1\":{\"79\":1,\"240\":1}}],[\"dit\",{\"0\":{\"287\":1},\"1\":{\"285\":4,\"293\":1}}],[\"dit模型在文生图和文生视频等表现出了杰出的性能\",{\"1\":{\"285\":1}}],[\"dit模型的推理延迟显著\",{\"1\":{\"283\":1}}],[\"difussion核心在迭代去噪点生成内容\",{\"1\":{\"276\":1}}],[\"diffusers\",{\"1\":{\"285\":1,\"294\":1}}],[\"diffusers库进行改造\",{\"1\":{\"285\":1}}],[\"diffusion模型在生成时长和内存占用上的表现都差强人意\",{\"1\":{\"272\":1}}],[\"diffusion\",{\"0\":{\"271\":1},\"1\":{\"273\":1}}],[\"diffusion包含多个计算特性各异的阶段\",{\"1\":{\"253\":1}}],[\"diffusion为例\",{\"1\":{\"240\":1}}],[\"diff\",{\"0\":{\"137\":1}}],[\"differentiation\",{\"0\":{\"136\":1,\"171\":1,\"175\":1},\"1\":{\"171\":1,\"172\":2}}],[\"difference\",{\"1\":{\"120\":1,\"122\":1,\"175\":1}}],[\"differs\",{\"1\":{\"119\":1}}],[\"division\",{\"1\":{\"139\":2,\"179\":2}}],[\"divide\",{\"1\":{\"139\":1,\"179\":1}}],[\"divbyconstop\",{\"1\":{\"138\":1}}],[\"divop\",{\"0\":{\"139\":1,\"179\":1},\"1\":{\"138\":1,\"139\":1,\"179\":1}}],[\"dive\",{\"0\":{\"65\":1,\"69\":1}}],[\"diagrams\",{\"0\":{\"121\":1}}],[\"dims\",{\"1\":{\"177\":3}}],[\"dim=list\",{\"1\":{\"177\":1}}],[\"dim=dims\",{\"1\":{\"177\":1}}],[\"dim=\",{\"1\":{\"177\":1}}],[\"dim=0\",{\"1\":{\"177\":1}}],[\"dim=node\",{\"1\":{\"177\":1}}],[\"dimensions\",{\"1\":{\"177\":2}}],[\"dim\",{\"1\":{\"89\":2,\"177\":6}}],[\"dir=\",{\"1\":{\"43\":1}}],[\"distvae解决了这个问题\",{\"1\":{\"294\":1}}],[\"distrifuser\",{\"1\":{\"273\":1}}],[\"distrifusion基于u\",{\"1\":{\"283\":1}}],[\"distrifusion需要为每个gpu维护所有层的kv数据\",{\"1\":{\"283\":1}}],[\"distrifusion的加速效果有限\",{\"1\":{\"280\":1}}],[\"distrifusion是一种利用多gpu并行加速扩散模型的新方法\",{\"1\":{\"280\":1}}],[\"distrifusion方法\",{\"1\":{\"276\":1}}],[\"distrifusion\",{\"0\":{\"273\":1},\"1\":{\"273\":1}}],[\"distributeddataparallel\",{\"1\":{\"45\":1}}],[\"distributedoptimizer\",{\"1\":{\"43\":1}}],[\"distributed\",{\"1\":{\"38\":1,\"44\":1,\"45\":1,\"273\":1}}],[\"distflashattn\",{\"0\":{\"158\":1}}],[\"display\",{\"1\":{\"128\":1}}],[\"disable\",{\"1\":{\"24\":2}}],[\"didn\",{\"1\":{\"7\":1,\"10\":1,\"27\":1}}],[\"did\",{\"0\":{\"7\":1,\"11\":1,\"54\":1}}],[\"does\",{\"1\":{\"177\":3}}],[\"dos\",{\"1\":{\"134\":1}}],[\"don\",{\"1\":{\"128\":1}}],[\"downloads\",{\"1\":{\"106\":1}}],[\"download\",{\"1\":{\"106\":1,\"124\":1}}],[\"doctest\",{\"1\":{\"131\":4}}],[\"docstrings\",{\"1\":{\"131\":1}}],[\"docs\",{\"1\":{\"49\":3}}],[\"doc\",{\"1\":{\"49\":4}}],[\"docker\",{\"0\":{\"20\":1},\"1\":{\"20\":1,\"21\":3,\"24\":1,\"25\":3,\"26\":3}}],[\"do\",{\"0\":{\"128\":1},\"1\":{\"12\":1,\"14\":1,\"27\":1}}],[\"d\",{\"1\":{\"10\":6,\"12\":6,\"14\":8,\"24\":1,\"26\":1,\"27\":2,\"95\":2,\"168\":2,\"186\":2,\"216\":1,\"228\":3}}],[\"degree的fresh\",{\"1\":{\"293\":1}}],[\"degree=2的混合并行方法为例\",{\"1\":{\"293\":1}}],[\"degree=4\",{\"1\":{\"293\":1}}],[\"denote\",{\"1\":{\"177\":1}}],[\"debugging\",{\"1\":{\"168\":1}}],[\"debugger\",{\"1\":{\"168\":3}}],[\"decode\",{\"1\":{\"285\":1}}],[\"decoder\",{\"1\":{\"253\":1}}],[\"decoding\",{\"1\":{\"240\":1,\"241\":1}}],[\"decomposition\",{\"1\":{\"107\":1}}],[\"decrease\",{\"1\":{\"9\":1,\"12\":1,\"14\":1}}],[\"descent\",{\"1\":{\"173\":1}}],[\"design\",{\"0\":{\"83\":1}}],[\"dest\",{\"1\":{\"78\":1}}],[\"destroy\",{\"1\":{\"5\":1,\"13\":1}}],[\"del\",{\"1\":{\"78\":1}}],[\"delete\",{\"1\":{\"13\":1,\"14\":1,\"168\":2}}],[\"democratizing\",{\"1\":{\"73\":1}}],[\"default\",{\"1\":{\"168\":1}}],[\"defining\",{\"0\":{\"120\":1},\"1\":{\"120\":1}}],[\"defined\",{\"1\":{\"117\":1}}],[\"define\",{\"0\":{\"119\":1},\"1\":{\"9\":1,\"119\":1,\"186\":2}}],[\"def\",{\"1\":{\"78\":5,\"89\":4,\"119\":3,\"120\":1,\"129\":1,\"139\":3,\"177\":45,\"179\":3}}],[\"defragmentation\",{\"0\":{\"72\":1}}],[\"device=0无法拿到p1\",{\"1\":{\"293\":1}}],[\"devices\",{\"1\":{\"98\":2}}],[\"device\",{\"1\":{\"43\":1,\"99\":1,\"100\":1,\"101\":1}}],[\"developer\",{\"1\":{\"21\":1}}],[\"deepspeed采用\",{\"1\":{\"247\":1}}],[\"deepspeed在知乎也有官号\",{\"1\":{\"149\":1}}],[\"deepspeed\",{\"0\":{\"149\":1,\"151\":1},\"1\":{\"149\":1,\"150\":2,\"291\":1}}],[\"deepspeedhttps\",{\"1\":{\"51\":1}}],[\"deeplearning\",{\"1\":{\"49\":1}}],[\"deep\",{\"0\":{\"65\":1,\"69\":1},\"1\":{\"38\":1,\"97\":1}}],[\"details\",{\"1\":{\"23\":1}}],[\"dead\",{\"1\":{\"4\":1}}],[\"oom\",{\"1\":{\"288\":1}}],[\"oops\",{\"1\":{\"24\":1,\"117\":1}}],[\"o墙\",{\"1\":{\"240\":1}}],[\"odd\",{\"1\":{\"209\":1,\"210\":1}}],[\"os\",{\"0\":{\"166\":1}}],[\"overlap\",{\"0\":{\"163\":1}}],[\"overhead\",{\"0\":{\"162\":1}}],[\"overview\",{\"0\":{\"59\":1,\"60\":1,\"61\":1,\"286\":1}}],[\"ok\",{\"1\":{\"128\":6,\"130\":3}}],[\"our\",{\"1\":{\"118\":1,\"119\":1}}],[\"ourselves\",{\"0\":{\"118\":1}}],[\"out\",{\"1\":{\"10\":1,\"13\":1,\"45\":2,\"128\":1,\"168\":4,\"177\":3,\"288\":1}}],[\"outputcontext=softmax\",{\"1\":{\"151\":1}}],[\"outputs\",{\"1\":{\"131\":1}}],[\"output\",{\"0\":{\"8\":1},\"1\":{\"8\":2,\"89\":5,\"108\":2,\"128\":1,\"139\":1,\"168\":1,\"177\":42,\"179\":1}}],[\"own\",{\"1\":{\"117\":1,\"118\":1}}],[\"owns\",{\"1\":{\"78\":1}}],[\"owner\",{\"1\":{\"78\":6}}],[\"op=self\",{\"1\":{\"139\":1,\"177\":14,\"179\":1}}],[\"op并行\",{\"1\":{\"98\":1}}],[\"operation\",{\"1\":{\"177\":2}}],[\"operations\",{\"1\":{\"177\":2}}],[\"operators\",{\"0\":{\"138\":1},\"1\":{\"129\":1,\"138\":1}}],[\"operator\",{\"1\":{\"98\":4,\"99\":6,\"100\":1,\"101\":3,\"114\":1,\"244\":1,\"245\":1}}],[\"operands\",{\"1\":{\"114\":1}}],[\"openaccess\",{\"1\":{\"273\":1}}],[\"opens\",{\"1\":{\"131\":1}}],[\"open\",{\"1\":{\"49\":1}}],[\"op\",{\"1\":{\"43\":2,\"139\":2,\"177\":33,\"179\":2,\"234\":3}}],[\"opt\",{\"1\":{\"43\":4,\"45\":2}}],[\"optimal\",{\"0\":{\"76\":1}}],[\"optimizations\",{\"1\":{\"51\":1,\"149\":1}}],[\"optimizer\",{\"0\":{\"55\":1},\"1\":{\"45\":1,\"98\":1}}],[\"optimize\",{\"1\":{\"24\":1}}],[\"optim\",{\"1\":{\"45\":3,\"78\":2}}],[\"options\",{\"0\":{\"131\":1},\"1\":{\"43\":1}}],[\"omp\",{\"1\":{\"24\":1}}],[\"occuered\",{\"1\":{\"18\":1}}],[\"occur\",{\"1\":{\"13\":1}}],[\"other\",{\"1\":{\"13\":2,\"168\":1}}],[\"only\",{\"1\":{\"129\":1,\"172\":1}}],[\"online\",{\"1\":{\"121\":1}}],[\"on\",{\"1\":{\"27\":1,\"44\":1,\"78\":4,\"90\":1,\"117\":1,\"177\":1}}],[\"once\",{\"1\":{\"10\":1}}],[\"oneslike\",{\"1\":{\"177\":1}}],[\"oneslikeop\",{\"1\":{\"177\":1}}],[\"ones\",{\"1\":{\"177\":2}}],[\"one\",{\"1\":{\"9\":2,\"10\":3,\"12\":3,\"14\":1,\"89\":1,\"177\":3}}],[\"offload在cpu上直接更新fp32参数和剩余的优化器状态\",{\"1\":{\"78\":1}}],[\"offload在小\",{\"1\":{\"75\":1}}],[\"offload可以立即将这些梯度逐个或切分传输到\",{\"1\":{\"78\":1}}],[\"offload将数据进行分区\",{\"1\":{\"78\":1}}],[\"offload将深度学习训练建模为数据流图\",{\"1\":{\"76\":1}}],[\"offload\",{\"0\":{\"73\":1,\"76\":1,\"77\":1},\"1\":{\"73\":1,\"74\":1,\"75\":1,\"79\":3}}],[\"of\",{\"1\":{\"9\":2,\"10\":2,\"12\":1,\"27\":3,\"81\":1,\"114\":4,\"117\":2,\"119\":2,\"120\":1,\"122\":1,\"125\":2,\"128\":1,\"131\":1,\"138\":1,\"139\":2,\"149\":1,\"166\":1,\"168\":6,\"177\":24,\"179\":2,\"288\":1}}],[\"original\",{\"1\":{\"177\":1}}],[\"org\",{\"1\":{\"38\":1,\"44\":1,\"49\":1,\"51\":1,\"73\":2,\"81\":1,\"88\":1,\"90\":1,\"97\":1,\"124\":1,\"144\":1,\"149\":1,\"152\":1,\"155\":1,\"158\":1,\"174\":1,\"281\":1,\"285\":1,\"287\":1}}],[\"or\",{\"1\":{\"8\":1,\"12\":1,\"14\":1,\"117\":3,\"172\":1,\"174\":1}}],[\"o\",{\"1\":{\"4\":1,\"8\":2,\"192\":1,\"194\":1,\"195\":2,\"196\":1,\"197\":2,\"204\":2,\"211\":2,\"217\":2,\"223\":2,\"229\":2,\"235\":2,\"240\":1}}],[\"llama\",{\"1\":{\"285\":1}}],[\"llm\",{\"1\":{\"285\":4,\"293\":1}}],[\"llm模型尺寸较大\",{\"1\":{\"276\":1}}],[\"lm提出了交错式1f1b\",{\"1\":{\"266\":1}}],[\"lm提出的新mp方案\",{\"1\":{\"84\":1}}],[\"lm中\",{\"1\":{\"259\":1}}],[\"lm一样解决计算过程中的内存占用问题\",{\"1\":{\"153\":1}}],[\"lm\",{\"0\":{\"88\":1},\"1\":{\"88\":2,\"90\":2,\"245\":1}}],[\"l\",{\"1\":{\"78\":14,\"117\":1,\"192\":1,\"194\":1,\"195\":1,\"196\":1,\"279\":1}}],[\"lr=0\",{\"1\":{\"45\":1}}],[\"level\",{\"0\":{\"161\":1},\"1\":{\"161\":1}}],[\"len\",{\"1\":{\"139\":1,\"177\":18,\"179\":1}}],[\"learned\",{\"1\":{\"114\":1}}],[\"learning中\",{\"1\":{\"173\":1}}],[\"learning基础并完成cse234的pa1\",{\"1\":{\"172\":1}}],[\"learning\",{\"1\":{\"38\":1}}],[\"lecture\",{\"0\":{\"113\":1},\"1\":{\"114\":1,\"117\":1,\"120\":1}}],[\"let\",{\"1\":{\"13\":1,\"24\":1}}],[\"ls\",{\"1\":{\"23\":1}}],[\"lstdc++\",{\"1\":{\"8\":1}}],[\"lk\",{\"1\":{\"10\":5,\"12\":10,\"14\":10}}],[\"look\",{\"1\":{\"166\":1}}],[\"long\",{\"1\":{\"149\":1,\"152\":1,\"184\":2}}],[\"loss\",{\"1\":{\"43\":2,\"78\":1,\"174\":1}}],[\"location\",{\"1\":{\"99\":1}}],[\"local\",{\"1\":{\"43\":1,\"120\":1,\"128\":2,\"130\":1,\"243\":1}}],[\"lock<std\",{\"1\":{\"14\":1}}],[\"lock\",{\"1\":{\"4\":2,\"10\":2,\"12\":4,\"14\":5}}],[\"loadbalanced\",{\"0\":{\"163\":1}}],[\"load\",{\"0\":{\"20\":1},\"1\":{\"12\":2,\"14\":2,\"20\":1,\"243\":1}}],[\"log10\",{\"1\":{\"186\":2}}],[\"log10​\",{\"1\":{\"185\":1}}],[\"logarithm\",{\"1\":{\"177\":2}}],[\"logop\",{\"1\":{\"177\":1}}],[\"logs\",{\"1\":{\"43\":1}}],[\"login\",{\"1\":{\"26\":1}}],[\"log\",{\"0\":{\"10\":1},\"1\":{\"10\":2,\"177\":5,\"185\":1}}],[\"latency\",{\"1\":{\"241\":1}}],[\"latest\",{\"1\":{\"21\":3,\"49\":1}}],[\"lazy\",{\"1\":{\"129\":1}}],[\"label\",{\"1\":{\"168\":1}}],[\"lab00\",{\"1\":{\"124\":2,\"131\":2}}],[\"lab0\",{\"0\":{\"124\":1}}],[\"lab\",{\"0\":{\"104\":1,\"123\":1},\"1\":{\"124\":1,\"125\":1,\"128\":1,\"130\":1,\"273\":1},\"2\":{\"109\":1,\"110\":1,\"140\":1,\"141\":1}}],[\"large\",{\"1\":{\"90\":1,\"144\":1}}],[\"language\",{\"1\":{\"88\":1,\"90\":1}}],[\"layernorm计算如下\",{\"1\":{\"267\":1}}],[\"layernorm需要计算整个序列维度的均值和方差\",{\"1\":{\"267\":1}}],[\"layernorm的原始公式\",{\"1\":{\"267\":1}}],[\"layernorm的结果y=\",{\"1\":{\"148\":1}}],[\"layernorm的activation大小计算\",{\"1\":{\"146\":1}}],[\"layernormop\",{\"1\":{\"138\":1}}],[\"layer\",{\"1\":{\"84\":1}}],[\"layers\",{\"1\":{\"78\":12}}],[\"last\",{\"1\":{\"9\":1,\"117\":1}}],[\"lambda\",{\"1\":{\"9\":1}}],[\"li\",{\"1\":{\"273\":1}}],[\"like\",{\"1\":{\"117\":1,\"177\":10}}],[\"linux\",{\"1\":{\"106\":3}}],[\"linear\",{\"1\":{\"45\":1}}],[\"line为每个进程的行数\",{\"1\":{\"24\":1}}],[\"line\",{\"0\":{\"131\":1},\"1\":{\"24\":17,\"117\":1,\"216\":3}}],[\"list\",{\"1\":{\"43\":1,\"138\":1,\"139\":2,\"177\":35,\"179\":2}}],[\"library\",{\"0\":{\"137\":1,\"176\":1},\"1\":{\"172\":1}}],[\"lib\",{\"1\":{\"24\":1}}],[\"life\",{\"1\":{\"5\":1}}],[\"live\",{\"1\":{\"4\":1}}],[\"b为批量大小\",{\"1\":{\"261\":1}}],[\"b为有效batch\",{\"1\":{\"76\":1}}],[\"b=​b1​b2​⋮bp​​​\",{\"1\":{\"259\":1}}],[\"boundary的\",{\"1\":{\"285\":2}}],[\"boundary和memory\",{\"1\":{\"285\":1}}],[\"bound\",{\"1\":{\"240\":3}}],[\"boolstr\",{\"1\":{\"203\":4}}],[\"bool\",{\"1\":{\"177\":1}}],[\"bot\",{\"1\":{\"14\":1}}],[\"b考虑\",{\"1\":{\"95\":1}}],[\"bk\",{\"1\":{\"84\":2}}],[\"bwd后\",{\"1\":{\"76\":1}}],[\"bwd\",{\"1\":{\"76\":1,\"78\":2,\"79\":1}}],[\"bf16\",{\"1\":{\"55\":1}}],[\"bi​\",{\"1\":{\"259\":1}}],[\"billion\",{\"1\":{\"73\":1,\"88\":1}}],[\"biancheng\",{\"1\":{\"24\":1}}],[\"bin\",{\"1\":{\"21\":3,\"24\":1,\"26\":1,\"106\":1}}],[\"b\",{\"1\":{\"24\":2,\"60\":1,\"62\":1,\"95\":2,\"139\":3,\"177\":19,\"179\":3,\"259\":2,\"261\":2,\"267\":2}}],[\"blog的建议一样\",{\"1\":{\"172\":1}}],[\"blog\",{\"1\":{\"23\":1,\"166\":1,\"171\":1}}],[\"block的副本发送到ring中的下一个设备\",{\"1\":{\"157\":1}}],[\"block发送到下一个主机\",{\"1\":{\"157\":1}}],[\"block遍历主机ring\",{\"1\":{\"157\":1}}],[\"block通信与compute重叠\",{\"1\":{\"157\":1}}],[\"blockwise\",{\"1\":{\"155\":1}}],[\"block\",{\"0\":{\"4\":1},\"1\":{\"4\":1,\"157\":3,\"248\":1}}],[\"balancer\",{\"1\":{\"243\":1}}],[\"balancedly\",{\"1\":{\"10\":1}}],[\"balancing的调度算法\",{\"1\":{\"161\":1}}],[\"batching\",{\"1\":{\"240\":1,\"244\":1,\"248\":1}}],[\"batch的数量\",{\"1\":{\"93\":1}}],[\"batch的操作\",{\"1\":{\"93\":1}}],[\"batch过小\",{\"1\":{\"92\":1}}],[\"batches\",{\"0\":{\"85\":1},\"1\":{\"244\":1,\"265\":1}}],[\"batch\",{\"1\":{\"75\":4,\"78\":3,\"85\":2,\"93\":1,\"95\":2,\"243\":2,\"292\":1}}],[\"batch较小时\",{\"1\":{\"47\":1}}],[\"ban\",{\"1\":{\"24\":1}}],[\"base\",{\"1\":{\"172\":2},\"2\":{\"180\":1,\"181\":1}}],[\"baseenv\",{\"1\":{\"19\":2,\"20\":1,\"21\":3}}],[\"basics\",{\"1\":{\"120\":1,\"125\":1,\"128\":1}}],[\"bashrc\",{\"1\":{\"24\":1}}],[\"bash\",{\"1\":{\"21\":3,\"26\":1}}],[\"bassenv\",{\"1\":{\"19\":2}}],[\"backward\",{\"1\":{\"45\":2,\"78\":4,\"84\":1,\"89\":1,\"266\":1}}],[\"back\",{\"1\":{\"10\":1,\"177\":1,\"210\":1}}],[\"buffer都更新成最新\",{\"1\":{\"293\":1}}],[\"buffer内\",{\"1\":{\"293\":1}}],[\"buffer\",{\"1\":{\"293\":1}}],[\"buffer中黄色部分是sp0\",{\"1\":{\"293\":1}}],[\"built\",{\"0\":{\"117\":1},\"1\":{\"117\":1}}],[\"build\",{\"1\":{\"26\":2,\"168\":2}}],[\"bubble\",{\"1\":{\"93\":1,\"244\":1,\"264\":1,\"265\":2}}],[\"bubble大小也有影响\",{\"1\":{\"91\":1}}],[\"bubble的大小\",{\"1\":{\"91\":1}}],[\"buﬀers\",{\"0\":{\"57\":1,\"63\":1,\"71\":1}}],[\"bucketing\",{\"1\":{\"48\":1}}],[\"bug\",{\"1\":{\"14\":1}}],[\"bugs\",{\"0\":{\"2\":1,\"3\":1,\"6\":1,\"7\":1,\"11\":1},\"1\":{\"7\":1,\"8\":1}}],[\"but\",{\"1\":{\"10\":1,\"13\":1,\"21\":1,\"114\":1,\"119\":1}}],[\"berkeley\",{\"1\":{\"128\":1},\"2\":{\"133\":1}}],[\"between\",{\"1\":{\"120\":1}}],[\"because\",{\"1\":{\"117\":1,\"166\":1}}],[\"beyond\",{\"1\":{\"97\":1}}],[\"be\",{\"1\":{\"14\":2,\"19\":1,\"120\":1,\"128\":1,\"134\":1,\"168\":1,\"177\":3}}],[\"before\",{\"1\":{\"14\":1,\"166\":1,\"177\":1}}],[\"begining\",{\"1\":{\"168\":1}}],[\"beginning\",{\"1\":{\"12\":1,\"117\":1}}],[\"begin\",{\"1\":{\"10\":1,\"12\":1,\"14\":1,\"128\":1,\"166\":1,\"222\":1,\"245\":1}}],[\"below\",{\"1\":{\"7\":1,\"10\":1,\"21\":1,\"24\":1,\"168\":1}}],[\"broadcasting\",{\"1\":{\"177\":1}}],[\"broadcastop\",{\"1\":{\"177\":1}}],[\"broadcasted\",{\"1\":{\"177\":4}}],[\"broadcastglobalvariableshook\",{\"1\":{\"43\":1}}],[\"broadcast\",{\"1\":{\"42\":1,\"78\":1,\"177\":8}}],[\"bridge\",{\"1\":{\"21\":2,\"26\":2}}],[\"brief\",{\"0\":{\"9\":1}}],[\"break\",{\"1\":{\"12\":1,\"14\":1}}],[\"breakpoint\",{\"1\":{\"6\":1,\"168\":1}}],[\"bytes和8sbh\",{\"1\":{\"146\":1}}],[\"bytes即可\",{\"1\":{\"146\":1}}],[\"bytes\",{\"1\":{\"146\":13,\"147\":2,\"168\":1}}],[\"by\",{\"0\":{\"118\":1},\"1\":{\"8\":1,\"9\":1,\"13\":1,\"14\":1,\"19\":1,\"89\":1,\"119\":2,\"168\":4,\"177\":3}}],[\"p+m−1\",{\"1\":{\"265\":1}}],[\"pcorrected​\",{\"1\":{\"263\":2}}],[\"pc​\",{\"1\":{\"262\":2}}],[\"pt​=pt​\",{\"1\":{\"263\":1}}],[\"pt​\",{\"1\":{\"263\":3}}],[\"pthread\",{\"1\":{\"8\":1}}],[\"pd​pt​​\",{\"1\":{\"263\":1}}],[\"pd​=pd​\",{\"1\":{\"263\":1}}],[\"pd​\",{\"1\":{\"263\":1}}],[\"pdf\",{\"1\":{\"73\":2,\"90\":1,\"97\":1,\"144\":1,\"149\":1,\"152\":1,\"155\":1,\"158\":1,\"174\":1,\"273\":1}}],[\"p=pr​×pc​\",{\"1\":{\"262\":1}}],[\"p个头的计算\",{\"1\":{\"260\":1}}],[\"pmatrix\",{\"1\":{\"245\":2}}],[\"p−1\",{\"1\":{\"244\":1,\"265\":5}}],[\"placeholder\",{\"1\":{\"177\":3}}],[\"placeholderop\",{\"1\":{\"177\":1}}],[\"p分区都被投影到查询\",{\"1\":{\"151\":1}}],[\"py\",{\"1\":{\"131\":2}}],[\"pytorch使用hook机制\",{\"1\":{\"48\":1}}],[\"pytorch使用gradient\",{\"1\":{\"48\":1}}],[\"pytorch在设计api时做到了仅调用第11行代码中的distributeddataparallel部分\",{\"1\":{\"45\":1}}],[\"pytorch\",{\"0\":{\"44\":1},\"1\":{\"44\":3,\"49\":1,\"89\":1}}],[\"python3\",{\"1\":{\"131\":2}}],[\"pythontutor\",{\"1\":{\"121\":1}}],[\"python前端api\",{\"0\":{\"45\":1}}],[\"python\",{\"0\":{\"128\":1,\"131\":1},\"1\":{\"42\":1,\"117\":2,\"121\":1,\"125\":1,\"128\":4,\"130\":1}}],[\"p\",{\"1\":{\"95\":1,\"149\":1,\"151\":1,\"168\":2,\"244\":1,\"259\":3,\"261\":1,\"262\":2,\"265\":1,\"267\":1}}],[\"p的策略\",{\"1\":{\"91\":1}}],[\"performed\",{\"1\":{\"177\":1}}],[\"perspectivehttps\",{\"1\":{\"152\":1}}],[\"per\",{\"1\":{\"89\":2}}],[\"permitrootlogin\",{\"1\":{\"26\":2}}],[\"pp支持有限\",{\"1\":{\"256\":1}}],[\"pp和tp\",{\"1\":{\"247\":1}}],[\"pp无法降低单一样本的端到端延迟\",{\"1\":{\"244\":1}}],[\"pp不能解决序列维度的扩展问题\",{\"1\":{\"150\":1}}],[\"pp\",{\"1\":{\"35\":1,\"82\":1,\"240\":1,\"255\":1}}],[\"p8088\",{\"1\":{\"26\":1}}],[\"posts\",{\"0\":{\"299\":1}}],[\"post\",{\"1\":{\"241\":1,\"250\":1}}],[\"pos\",{\"1\":{\"186\":2}}],[\"position\",{\"1\":{\"9\":1}}],[\"powerop\",{\"1\":{\"138\":1}}],[\"port\",{\"1\":{\"26\":2}}],[\"paper\",{\"1\":{\"273\":1}}],[\"papers\",{\"1\":{\"273\":1}}],[\"pagedattention\",{\"1\":{\"240\":1,\"248\":1}}],[\"pa0\",{\"1\":{\"166\":1}}],[\"pa\",{\"0\":{\"166\":1}}],[\"pa1一些autodiff的算子实现\",{\"1\":{\"176\":1}}],[\"pa1\",{\"0\":{\"136\":1,\"167\":1},\"1\":{\"166\":1}}],[\"paste\",{\"1\":{\"114\":1}}],[\"pass可以来获取不同的性能\",{\"1\":{\"93\":1}}],[\"pass和backward\",{\"1\":{\"93\":1}}],[\"pass\",{\"1\":{\"45\":2,\"78\":1,\"128\":1}}],[\"passwd\",{\"1\":{\"26\":1}}],[\"password\",{\"1\":{\"26\":4}}],[\"passed\",{\"1\":{\"12\":1,\"13\":1,\"14\":1,\"24\":1,\"25\":1,\"26\":1,\"27\":1,\"120\":1,\"125\":1,\"130\":2,\"131\":1}}],[\"padding\",{\"1\":{\"89\":1}}],[\"partial\",{\"1\":{\"139\":1,\"177\":12,\"179\":1}}],[\"partition\",{\"1\":{\"79\":1,\"89\":2}}],[\"partitioned\",{\"0\":{\"70\":1}}],[\"part\",{\"0\":{\"138\":1},\"1\":{\"79\":5}}],[\"params\",{\"1\":{\"89\":1}}],[\"param\",{\"1\":{\"76\":1,\"78\":7}}],[\"parameter\",{\"1\":{\"51\":1,\"88\":1,\"89\":1,\"98\":2,\"117\":3,\"118\":1,\"119\":1}}],[\"parameters>\",{\"1\":{\"120\":1}}],[\"parameters\",{\"0\":{\"55\":1},\"1\":{\"45\":2,\"177\":2}}],[\"parallelsim的一种\",{\"1\":{\"273\":1}}],[\"parallelization\",{\"1\":{\"100\":1,\"101\":4}}],[\"parallelism\",{\"0\":{\"84\":1,\"85\":1,\"86\":1,\"152\":1},\"1\":{\"81\":1,\"88\":1,\"97\":1,\"152\":1,\"240\":3,\"245\":2,\"255\":1,\"259\":2,\"272\":1}}],[\"parallel且语义和原始mlp对等\",{\"1\":{\"89\":1}}],[\"parallel\",{\"0\":{\"3\":1,\"89\":1,\"147\":1,\"148\":1,\"288\":1,\"292\":1,\"293\":1,\"294\":1},\"1\":{\"24\":1,\"35\":2,\"44\":1,\"45\":1,\"78\":1,\"89\":10,\"145\":1,\"148\":2,\"273\":1,\"287\":4,\"288\":1}}],[\"par\",{\"1\":{\"45\":2}}],[\"patch\",{\"1\":{\"288\":1,\"293\":1,\"294\":1}}],[\"patallel\",{\"1\":{\"35\":3}}],[\"path=$path\",{\"1\":{\"24\":1,\"106\":1}}],[\"path\",{\"1\":{\"24\":1}}],[\"package\",{\"1\":{\"19\":1}}],[\"packing\",{\"1\":{\"19\":1}}],[\"pr​\",{\"1\":{\"262\":2}}],[\"pr​×pc​\",{\"1\":{\"262\":1}}],[\"prefill\",{\"1\":{\"285\":2}}],[\"prefix=\",{\"1\":{\"24\":1}}],[\"prefetch张量来完成通信workerp⟵kr+1​\",{\"1\":{\"162\":1}}],[\"pragma\",{\"1\":{\"24\":1}}],[\"print\",{\"0\":{\"122\":1},\"1\":{\"24\":4,\"122\":2,\"168\":4,\"177\":1}}],[\"printf\",{\"1\":{\"10\":1,\"12\":1,\"14\":3,\"24\":5,\"186\":1,\"203\":2,\"216\":1,\"222\":2}}],[\"prompt\",{\"1\":{\"292\":1}}],[\"project\",{\"1\":{\"285\":1}}],[\"pro1的内容主要是为了让学生了解std\",{\"1\":{\"107\":1}}],[\"proposals\",{\"1\":{\"101\":1}}],[\"properly\",{\"1\":{\"45\":1}}],[\"processgroup来调用不同的集合通讯库\",{\"1\":{\"49\":1}}],[\"process\",{\"1\":{\"27\":1,\"45\":1}}],[\"processor\",{\"1\":{\"27\":5}}],[\"processed\",{\"1\":{\"14\":1}}],[\"prohibitive\",{\"0\":{\"162\":1}}],[\"prohibit\",{\"1\":{\"26\":1}}],[\"prog1\",{\"0\":{\"105\":1}}],[\"progect\",{\"1\":{\"7\":1}}],[\"programm\",{\"1\":{\"24\":2,\"27\":1,\"168\":5}}],[\"program\",{\"0\":{\"2\":1,\"27\":1},\"1\":{\"134\":1,\"168\":2}}],[\"pro\",{\"0\":{\"7\":1}}],[\"problems\",{\"1\":{\"166\":1}}],[\"problem\",{\"0\":{\"3\":1}}],[\"pi\",{\"1\":{\"117\":8,\"118\":2,\"119\":1,\"186\":2}}],[\"picture\",{\"1\":{\"114\":1}}],[\"pipefusion利用过时的kv进行attention计算\",{\"1\":{\"293\":1}}],[\"pipefusion每个设备只存储与其特定阶段相关的参数的1\",{\"1\":{\"284\":1}}],[\"pipefusion与distrifusion不同的是\",{\"1\":{\"284\":1}}],[\"pipefusion通过将图像分割成多个patch并跨多个gpu分布网络层\",{\"1\":{\"282\":1}}],[\"pipefusion可以看作是在distrifusion上的改进与推广\",{\"1\":{\"281\":1}}],[\"pipefusion\",{\"0\":{\"281\":1,\"290\":1},\"1\":{\"287\":1,\"293\":1}}],[\"pipeline尺寸\",{\"1\":{\"93\":1}}],[\"pipeline\",{\"0\":{\"85\":1,\"86\":1},\"1\":{\"81\":1,\"91\":2,\"240\":1,\"244\":1,\"253\":1,\"264\":1}}],[\"pipe\",{\"1\":{\"35\":1}}],[\"ping\",{\"1\":{\"25\":1}}],[\"pid=30965\",{\"1\":{\"8\":1,\"13\":1}}],[\"pie\",{\"1\":{\"8\":1}}],[\"cfg\",{\"0\":{\"292\":1},\"1\":{\"287\":1,\"292\":2}}],[\"cnt++\",{\"1\":{\"222\":1}}],[\"cnt\",{\"1\":{\"216\":3,\"222\":2}}],[\"cin\",{\"1\":{\"184\":1,\"196\":1,\"203\":2,\"210\":2,\"216\":3,\"222\":2,\"228\":2,\"234\":3}}],[\"circ\",{\"1\":{\"118\":2}}],[\"circle\",{\"1\":{\"5\":1,\"9\":3,\"10\":1,\"12\":2}}],[\"current\",{\"1\":{\"177\":1}}],[\"cut\",{\"1\":{\"117\":1}}],[\"ctrl\",{\"1\":{\"117\":1}}],[\"ctx\",{\"1\":{\"89\":2}}],[\"c++\",{\"1\":{\"117\":1,\"120\":1}}],[\"cse234\",{\"0\":{\"135\":1},\"2\":{\"140\":1}}],[\"cs\",{\"1\":{\"166\":1},\"2\":{\"133\":1}}],[\"cs61a\",{\"0\":{\"111\":1},\"1\":{\"114\":1,\"124\":1},\"2\":{\"132\":1}}],[\"cs149\",{\"0\":{\"104\":1},\"1\":{\"106\":2},\"2\":{\"109\":1}}],[\"csdn\",{\"1\":{\"23\":1}}],[\"cell\",{\"1\":{\"84\":2}}],[\"cpu计算量可能成为瓶颈\",{\"1\":{\"75\":1}}],[\"cpu计算量并不是瓶颈\",{\"1\":{\"75\":1}}],[\"cpu\",{\"1\":{\"74\":2,\"76\":4,\"78\":11,\"79\":4}}],[\"cp\",{\"1\":{\"24\":1}}],[\"cpp中的源码\",{\"1\":{\"108\":1}}],[\"cpp\",{\"1\":{\"24\":3}}],[\"cd\",{\"1\":{\"23\":1,\"24\":1}}],[\"calibration\",{\"1\":{\"268\":1}}],[\"called\",{\"1\":{\"119\":1,\"120\":1,\"168\":1,\"177\":1}}],[\"call\",{\"1\":{\"21\":1,\"26\":1,\"117\":1,\"139\":1,\"177\":15,\"179\":1}}],[\"caught\",{\"1\":{\"168\":1}}],[\"cases\",{\"1\":{\"128\":1,\"130\":2}}],[\"case\",{\"1\":{\"128\":1,\"177\":3}}],[\"carlo\",{\"1\":{\"98\":1}}],[\"category\",{\"1\":{\"89\":3}}],[\"cap\",{\"1\":{\"21\":3,\"26\":1}}],[\"cannot\",{\"1\":{\"134\":1,\"177\":1}}],[\"can\",{\"1\":{\"8\":3,\"10\":1,\"12\":1,\"13\":3,\"14\":1,\"19\":1,\"21\":1,\"23\":1,\"24\":2,\"26\":2,\"27\":2,\"118\":1,\"119\":1,\"166\":1,\"168\":1,\"177\":3}}],[\"classifier\",{\"1\":{\"292\":1}}],[\"class\",{\"1\":{\"89\":2,\"139\":1,\"177\":16,\"179\":1}}],[\"clone\",{\"1\":{\"89\":1,\"106\":1}}],[\"clock\",{\"1\":{\"24\":3}}],[\"cloud\",{\"1\":{\"21\":1}}],[\"clusters\",{\"1\":{\"90\":1}}],[\"cluster\",{\"0\":{\"17\":1,\"27\":1},\"1\":{\"24\":1,\"25\":3}}],[\"clear\",{\"1\":{\"9\":1,\"10\":1,\"12\":1,\"14\":1}}],[\"cvpr\",{\"1\":{\"273\":1}}],[\"cvpr2024\",{\"1\":{\"273\":1}}],[\"cvf\",{\"1\":{\"19\":1}}],[\"cv\",{\"1\":{\"10\":2,\"12\":4,\"13\":1,\"14\":8}}],[\"creative\",{\"1\":{\"129\":1}}],[\"creat\",{\"1\":{\"9\":1,\"25\":1}}],[\"created\",{\"1\":{\"177\":1}}],[\"create\",{\"1\":{\"9\":1,\"12\":1,\"14\":2,\"23\":1,\"25\":1,\"177\":1}}],[\"cc\",{\"1\":{\"8\":2}}],[\"correction\",{\"1\":{\"263\":1}}],[\"corresponding\",{\"1\":{\"14\":1}}],[\"column\",{\"1\":{\"245\":1,\"259\":1}}],[\"columns\",{\"1\":{\"89\":2}}],[\"col\",{\"1\":{\"233\":1,\"234\":5}}],[\"collectives\",{\"1\":{\"151\":1}}],[\"co\",{\"1\":{\"99\":1,\"171\":1}}],[\"course\",{\"1\":{\"168\":1},\"2\":{\"141\":1}}],[\"cout\",{\"1\":{\"24\":1,\"184\":1,\"196\":3,\"210\":2,\"228\":1,\"234\":1}}],[\"count\",{\"1\":{\"9\":3,\"10\":2,\"12\":4,\"14\":4,\"216\":3}}],[\"copy\",{\"1\":{\"24\":2,\"78\":2}}],[\"come\",{\"1\":{\"129\":1}}],[\"communication\",{\"0\":{\"162\":1,\"163\":1}}],[\"commit\",{\"1\":{\"26\":2}}],[\"comm\",{\"1\":{\"24\":10,\"27\":4}}],[\"command\",{\"0\":{\"21\":1,\"131\":1}}],[\"com\",{\"1\":{\"21\":1,\"38\":1,\"44\":1,\"49\":1,\"51\":1,\"88\":1,\"90\":1,\"106\":2,\"121\":1,\"149\":1,\"273\":2,\"285\":1,\"294\":1}}],[\"completion\",{\"1\":{\"263\":1}}],[\"complete\",{\"1\":{\"125\":1}}],[\"comparison\",{\"1\":{\"177\":2}}],[\"compare\",{\"1\":{\"177\":1}}],[\"computational\",{\"1\":{\"177\":1,\"244\":1}}],[\"computation\",{\"0\":{\"163\":1}}],[\"computed\",{\"1\":{\"177\":4}}],[\"computer\",{\"0\":{\"167\":1},\"1\":{\"117\":1,\"125\":1}}],[\"compute\",{\"1\":{\"78\":1,\"99\":1,\"139\":1,\"177\":21,\"179\":1,\"240\":1,\"241\":1}}],[\"components\",{\"1\":{\"125\":1}}],[\"composed\",{\"1\":{\"114\":1}}],[\"compile\",{\"0\":{\"23\":1},\"1\":{\"8\":1,\"24\":1}}],[\"competition\",{\"0\":{\"5\":1}}],[\"code\",{\"0\":{\"8\":1},\"1\":{\"8\":1,\"14\":2,\"24\":4,\"139\":2,\"179\":2}}],[\"codes\",{\"0\":{\"9\":1},\"1\":{\"6\":1,\"8\":2,\"10\":1,\"168\":1}}],[\"concatenate\",{\"1\":{\"260\":1}}],[\"concurrency\",{\"0\":{\"1\":1}}],[\"conjugate\",{\"1\":{\"147\":1}}],[\"const\",{\"1\":{\"108\":1,\"177\":7}}],[\"constant\",{\"0\":{\"71\":1},\"1\":{\"5\":1,\"177\":9}}],[\"consumption\",{\"0\":{\"56\":1}}],[\"config=config\",{\"1\":{\"43\":1}}],[\"configproto\",{\"1\":{\"43\":1}}],[\"config\",{\"1\":{\"26\":1,\"43\":2}}],[\"configure\",{\"0\":{\"24\":1,\"25\":1,\"26\":1},\"1\":{\"24\":1,\"27\":1}}],[\"confirm\",{\"1\":{\"8\":3,\"166\":1}}],[\"connection\",{\"1\":{\"27\":1}}],[\"connect\",{\"1\":{\"21\":1,\"25\":2,\"26\":1}}],[\"continuous\",{\"1\":{\"240\":1,\"248\":1}}],[\"continue\",{\"1\":{\"168\":2}}],[\"continers\",{\"1\":{\"25\":1}}],[\"continer\",{\"1\":{\"23\":1,\"24\":2,\"26\":2}}],[\"contexthttps\",{\"1\":{\"155\":1}}],[\"content\",{\"1\":{\"2\":2,\"273\":1}}],[\"containers\",{\"0\":{\"25\":1,\"26\":1}}],[\"container\",{\"0\":{\"21\":1},\"1\":{\"21\":2,\"27\":2}}],[\"condvar\",{\"1\":{\"8\":2,\"14\":4}}],[\"conditional\",{\"0\":{\"5\":1}}],[\"condition\",{\"1\":{\"4\":1,\"8\":1,\"12\":1,\"14\":1}}],[\"ch\",{\"1\":{\"216\":2}}],[\"cheek\",{\"1\":{\"125\":1}}],[\"checkpointing\",{\"0\":{\"70\":1}}],[\"checkpoint\",{\"1\":{\"43\":1}}],[\"check\",{\"1\":{\"9\":1,\"10\":1,\"12\":1,\"13\":1,\"14\":3,\"24\":1,\"25\":1,\"26\":1,\"27\":1,\"130\":2}}],[\"channel\",{\"1\":{\"268\":1}}],[\"changes\",{\"1\":{\"168\":1}}],[\"change\",{\"1\":{\"26\":2,\"27\":1,\"119\":2}}],[\"chain\",{\"1\":{\"98\":1}}],[\"char\",{\"1\":{\"24\":1,\"27\":2,\"203\":1,\"228\":1}}],[\"chapter10\",{\"1\":{\"2\":2}}],[\"choice\",{\"1\":{\"10\":1,\"12\":1}}],[\"chinese\",{\"1\":{\"2\":1}}],[\"c\",{\"1\":{\"2\":1,\"60\":1,\"117\":1,\"120\":1,\"168\":2,\"203\":3,\"228\":3,\"277\":1}}],[\"i−1​\",{\"1\":{\"263\":2}}],[\"i$\",{\"1\":{\"261\":1}}],[\"identity\",{\"1\":{\"259\":1}}],[\"idx\",{\"1\":{\"89\":3}}],[\"id\",{\"1\":{\"89\":1}}],[\"imax​∣xij​∣\",{\"1\":{\"268\":1}}],[\"image\",{\"0\":{\"20\":1},\"1\":{\"26\":2,\"253\":1}}],[\"imbalance\",{\"0\":{\"161\":1}}],[\"improve\",{\"1\":{\"168\":1}}],[\"implementation\",{\"1\":{\"177\":3}}],[\"implement\",{\"1\":{\"138\":1}}],[\"implementing\",{\"0\":{\"129\":1}}],[\"important\",{\"1\":{\"117\":1}}],[\"imported\",{\"1\":{\"117\":1,\"118\":1}}],[\"import\",{\"1\":{\"43\":2,\"45\":4,\"117\":3}}],[\"ignore\",{\"1\":{\"24\":3}}],[\"iptables=0\",{\"1\":{\"21\":1,\"26\":1}}],[\"its\",{\"1\":{\"117\":1,\"119\":1,\"177\":2}}],[\"it\",{\"0\":{\"24\":1},\"1\":{\"10\":1,\"13\":1,\"14\":3,\"18\":1,\"21\":3,\"23\":1,\"24\":1,\"26\":1,\"27\":1,\"117\":3,\"119\":1,\"120\":2,\"128\":1,\"168\":1,\"174\":1,\"177\":6}}],[\"i++\",{\"1\":{\"10\":1,\"12\":1,\"14\":3,\"24\":6,\"184\":1,\"222\":1,\"228\":1,\"234\":4}}],[\"ispc\",{\"1\":{\"106\":5}}],[\"issue\",{\"1\":{\"8\":2,\"13\":2,\"119\":1}}],[\"issues\",{\"1\":{\"5\":1,\"18\":1,\"294\":1}}],[\"is\",{\"1\":{\"8\":2,\"9\":4,\"10\":2,\"13\":1,\"19\":1,\"24\":2,\"26\":1,\"78\":4,\"114\":2,\"117\":3,\"118\":1,\"120\":1,\"168\":6,\"171\":1,\"172\":1,\"177\":6}}],[\"if\",{\"1\":{\"7\":1,\"8\":1,\"10\":1,\"12\":2,\"13\":1,\"14\":5,\"21\":1,\"24\":1,\"78\":4,\"89\":2,\"108\":1,\"114\":1,\"119\":2,\"128\":1,\"131\":1,\"177\":9,\"186\":1,\"203\":1,\"210\":1,\"216\":1,\"222\":3,\"228\":1,\"234\":1}}],[\"i\",{\"0\":{\"7\":1,\"11\":1},\"1\":{\"4\":1,\"7\":1,\"10\":5,\"12\":5,\"14\":12,\"18\":1,\"20\":1,\"24\":22,\"27\":1,\"78\":18,\"114\":1,\"120\":1,\"131\":2,\"166\":4,\"177\":2,\"184\":3,\"222\":2,\"228\":2,\"233\":1,\"234\":14,\"240\":1,\"259\":2,\"260\":3,\"261\":1,\"262\":3,\"263\":2,\"267\":2,\"277\":1,\"279\":4}}],[\"io\",{\"1\":{\"2\":1,\"49\":1}}],[\"inorder\",{\"1\":{\"168\":1}}],[\"inference\",{\"1\":{\"240\":1,\"273\":1}}],[\"info\",{\"1\":{\"168\":2}}],[\"information\",{\"0\":{\"10\":1},\"1\":{\"10\":2,\"168\":2}}],[\"infrainstructure\",{\"1\":{\"168\":1}}],[\"infrastructure\",{\"0\":{\"168\":1},\"1\":{\"168\":1}}],[\"infinite\",{\"1\":{\"155\":1}}],[\"index\",{\"1\":{\"89\":8}}],[\"insights\",{\"0\":{\"59\":1,\"60\":1,\"61\":1}}],[\"install\",{\"1\":{\"24\":1,\"166\":1}}],[\"installation\",{\"0\":{\"18\":1}}],[\"instructions\",{\"1\":{\"168\":2}}],[\"instruction\",{\"1\":{\"8\":1,\"21\":1,\"24\":1,\"168\":1}}],[\"inputs\",{\"1\":{\"177\":11}}],[\"inputs=\",{\"1\":{\"139\":1,\"177\":14,\"179\":1}}],[\"input共享\",{\"1\":{\"146\":1}}],[\"input\",{\"1\":{\"89\":12,\"139\":4,\"177\":89,\"179\":4}}],[\"inp\",{\"1\":{\"45\":2}}],[\"initialize\",{\"1\":{\"45\":1,\"78\":2}}],[\"init\",{\"1\":{\"24\":1,\"27\":1,\"43\":1,\"45\":1,\"89\":3}}],[\"include<iostream>\",{\"1\":{\"24\":1}}],[\"include<omp\",{\"1\":{\"24\":1}}],[\"include\",{\"1\":{\"14\":7,\"24\":10,\"27\":4,\"186\":2,\"196\":1,\"203\":3,\"210\":1,\"216\":1,\"222\":1,\"228\":2,\"234\":2}}],[\"increase\",{\"1\":{\"12\":1,\"14\":2}}],[\"int3\",{\"1\":{\"268\":1}}],[\"inter\",{\"1\":{\"244\":1,\"247\":1}}],[\"interactive\",{\"1\":{\"131\":1}}],[\"interpreter\",{\"1\":{\"128\":1}}],[\"interesting\",{\"1\":{\"114\":1}}],[\"internet\",{\"1\":{\"21\":2,\"24\":1}}],[\"intra\",{\"1\":{\"245\":1,\"247\":1}}],[\"intranet\",{\"1\":{\"21\":1}}],[\"introduction\",{\"0\":{\"53\":1,\"125\":1,\"173\":1}}],[\"introduces\",{\"1\":{\"125\":1}}],[\"introduce\",{\"1\":{\"12\":1,\"117\":1,\"120\":1}}],[\"into\",{\"0\":{\"65\":1,\"69\":1,\"85\":1},\"1\":{\"12\":1,\"25\":1}}],[\"int\",{\"1\":{\"10\":1,\"12\":2,\"14\":7,\"24\":59,\"27\":4,\"108\":6,\"177\":2,\"184\":2,\"186\":5,\"196\":2,\"203\":2,\"210\":2,\"216\":5,\"222\":8,\"228\":5,\"234\":9}}],[\"in\",{\"0\":{\"2\":1,\"7\":1,\"27\":1,\"117\":1},\"1\":{\"8\":2,\"10\":1,\"12\":1,\"13\":1,\"14\":5,\"19\":1,\"21\":1,\"24\":1,\"26\":1,\"38\":1,\"78\":7,\"117\":4,\"119\":1,\"128\":2,\"131\":1,\"134\":1,\"144\":1,\"166\":1,\"168\":1,\"177\":7}}],[\"n²\",{\"1\":{\"235\":2}}],[\"n+1\",{\"1\":{\"221\":1}}],[\"n++\",{\"1\":{\"24\":2}}],[\"n为字符串长度\",{\"1\":{\"217\":1}}],[\"n为整数\",{\"1\":{\"193\":1}}],[\"n个\",{\"1\":{\"192\":1}}],[\"n=∑ni​是总元素数量\",{\"1\":{\"267\":1}}],[\"n=\",{\"1\":{\"185\":2}}],[\"n=ptd\",{\"1\":{\"95\":1}}],[\"n阶乘的计算量十分大\",{\"1\":{\"185\":1}}],[\"n阶乘的位数\",{\"1\":{\"183\":1}}],[\"n2\",{\"1\":{\"184\":1,\"254\":1}}],[\"natural\",{\"1\":{\"177\":2}}],[\"naive\",{\"0\":{\"84\":1}}],[\"name=name\",{\"1\":{\"177\":1}}],[\"name=f\",{\"1\":{\"139\":1,\"177\":13,\"179\":1}}],[\"named\",{\"1\":{\"118\":1,\"119\":1}}],[\"nameerror\",{\"1\":{\"117\":1}}],[\"namelen\",{\"1\":{\"27\":2}}],[\"namespace\",{\"1\":{\"24\":1,\"196\":1,\"203\":1,\"210\":1,\"216\":1,\"222\":1,\"228\":1,\"234\":1}}],[\"name\",{\"0\":{\"116\":1,\"118\":1},\"1\":{\"21\":3,\"26\":1,\"27\":6,\"117\":2,\"118\":1,\"119\":1,\"139\":2,\"177\":20,\"179\":2}}],[\"n参数\",{\"1\":{\"70\":1}}],[\"n份数据\",{\"1\":{\"66\":1,\"67\":1,\"68\":1}}],[\"nvidia本身支持这种优化并不会有通讯增加\",{\"1\":{\"68\":1}}],[\"nvidia\",{\"1\":{\"49\":1,\"88\":1,\"90\":1}}],[\"nn\",{\"1\":{\"45\":5}}],[\"nccl\",{\"1\":{\"42\":1,\"49\":3}}],[\"numeric\",{\"0\":{\"175\":1}}],[\"numbers\",{\"1\":{\"129\":1}}],[\"numthreads\",{\"1\":{\"108\":2}}],[\"numrows\",{\"1\":{\"108\":2}}],[\"numprocs\",{\"1\":{\"27\":3}}],[\"num为进程数\",{\"1\":{\"24\":1}}],[\"num\",{\"1\":{\"24\":7,\"78\":4,\"89\":4,\"186\":3,\"210\":3,\"222\":3}}],[\"null\",{\"1\":{\"24\":1}}],[\"nf\",{\"1\":{\"21\":1,\"26\":1}}],[\"nonetype\",{\"1\":{\"122\":1}}],[\"none\",{\"0\":{\"122\":1},\"1\":{\"122\":3}}],[\"no\",{\"1\":{\"117\":1,\"130\":1,\"131\":1,\"177\":2,\"202\":1,\"203\":1,\"222\":1}}],[\"novel\",{\"1\":{\"97\":1}}],[\"norm\",{\"1\":{\"89\":2}}],[\"normal\",{\"1\":{\"89\":1}}],[\"normally\",{\"1\":{\"14\":1}}],[\"node\",{\"1\":{\"76\":1,\"139\":17,\"177\":246,\"179\":17,\"247\":2}}],[\"nodes\",{\"1\":{\"26\":1,\"139\":1,\"177\":7,\"179\":1}}],[\"node2\",{\"1\":{\"21\":2,\"25\":3,\"26\":1}}],[\"node1\",{\"1\":{\"21\":4,\"24\":1,\"25\":3,\"26\":6}}],[\"note\",{\"1\":{\"177\":3}}],[\"notimplementederror\",{\"1\":{\"177\":3}}],[\"notify\",{\"1\":{\"9\":3,\"10\":2,\"12\":3,\"13\":3,\"14\":5}}],[\"not\",{\"1\":{\"4\":1,\"8\":1,\"10\":1,\"21\":2,\"43\":1,\"117\":4,\"119\":1,\"172\":1,\"177\":5,\"297\":1}}],[\"n\",{\"1\":{\"10\":1,\"12\":1,\"14\":3,\"24\":13,\"27\":1,\"95\":1,\"99\":1,\"114\":1,\"151\":1,\"168\":7,\"183\":4,\"184\":3,\"185\":4,\"186\":5,\"192\":1,\"193\":1,\"194\":1,\"195\":1,\"196\":4,\"197\":1,\"203\":5,\"207\":1,\"210\":5,\"211\":1,\"216\":3,\"221\":1,\"222\":4,\"228\":3,\"229\":1,\"234\":13,\"279\":2,\"284\":1,\"294\":1}}],[\"nemu\",{\"1\":{\"168\":5}}],[\"nearest\",{\"1\":{\"268\":1}}],[\"near\",{\"1\":{\"155\":1}}],[\"neural\",{\"1\":{\"81\":1,\"97\":1}}],[\"net模型\",{\"1\":{\"283\":1}}],[\"net和vae\",{\"1\":{\"253\":1}}],[\"networks\",{\"1\":{\"81\":1,\"97\":1}}],[\"network\",{\"0\":{\"25\":1},\"1\":{\"25\":3}}],[\"net\",{\"1\":{\"21\":4,\"23\":1,\"25\":2,\"26\":2,\"45\":5,\"277\":1}}],[\"new\",{\"0\":{\"166\":1},\"1\":{\"12\":1,\"14\":1,\"19\":1,\"25\":1,\"26\":4,\"177\":2}}],[\"next\",{\"1\":{\"9\":1,\"10\":1,\"12\":1,\"14\":1}}],[\"need\",{\"1\":{\"8\":1,\"10\":1,\"13\":1,\"24\":1,\"26\":2,\"128\":1,\"138\":1,\"168\":1,\"177\":1}}],[\"nj\",{\"1\":{\"2\":1}}],[\"nlp等ai方向\",{\"1\":{\"0\":1}}],[\"hybrid\",{\"0\":{\"293\":1}}],[\"high\",{\"1\":{\"273\":1}}],[\"h为隐藏层维度\",{\"1\":{\"261\":1}}],[\"hpc\",{\"1\":{\"256\":1}}],[\"huggingface\",{\"1\":{\"171\":1,\"294\":1}}],[\"huggingface中采用的recomputation的检查点放置不合理\",{\"1\":{\"163\":1}}],[\"han\",{\"1\":{\"273\":1}}],[\"hard\",{\"1\":{\"168\":2}}],[\"half\",{\"1\":{\"166\":1}}],[\"have\",{\"1\":{\"117\":1,\"128\":1,\"166\":2,\"177\":3}}],[\"hash\",{\"1\":{\"216\":4}}],[\"has\",{\"1\":{\"14\":1,\"117\":1}}],[\"hvd\",{\"1\":{\"43\":5}}],[\"headp​\",{\"1\":{\"260\":1}}],[\"head1​\",{\"1\":{\"260\":1}}],[\"headi​=attention\",{\"1\":{\"260\":1}}],[\"heads\",{\"1\":{\"260\":1}}],[\"hex\",{\"1\":{\"168\":1}}],[\"help\",{\"1\":{\"168\":4}}],[\"hello\",{\"1\":{\"27\":2}}],[\"height\",{\"1\":{\"108\":6}}],[\"here\",{\"1\":{\"8\":1,\"119\":1,\"139\":2,\"179\":2}}],[\"honor\",{\"1\":{\"174\":1}}],[\"home\",{\"1\":{\"106\":1}}],[\"hot\",{\"1\":{\"89\":1}}],[\"hooks=hooks\",{\"1\":{\"43\":1}}],[\"hooks\",{\"1\":{\"43\":1}}],[\"horovod工作\",{\"0\":{\"42\":1}}],[\"horovod\",{\"0\":{\"38\":1},\"1\":{\"38\":2,\"42\":2,\"43\":3}}],[\"host\",{\"1\":{\"26\":1,\"27\":1}}],[\"however\",{\"1\":{\"21\":1,\"168\":1}}],[\"how\",{\"0\":{\"2\":1,\"6\":1,\"7\":1,\"11\":1},\"1\":{\"125\":1}}],[\"h\",{\"1\":{\"24\":1,\"27\":1,\"175\":1,\"261\":2,\"267\":2}}],[\"h>\",{\"1\":{\"24\":8,\"27\":2,\"186\":2,\"196\":1,\"203\":1,\"210\":1,\"216\":1,\"222\":1}}],[\"html\",{\"1\":{\"2\":1,\"49\":1,\"121\":1,\"281\":1}}],[\"https\",{\"1\":{\"2\":1,\"21\":1,\"23\":1,\"38\":1,\"44\":1,\"49\":3,\"51\":1,\"73\":1,\"81\":1,\"88\":1,\"90\":1,\"97\":1,\"106\":2,\"121\":1,\"124\":1,\"144\":1,\"149\":1,\"158\":1,\"171\":1,\"174\":1,\"273\":2,\"281\":1,\"285\":2,\"287\":1}}],[\"t以及额外的条件c\",{\"1\":{\"277\":1}}],[\"t1​t=1∑t​∣xij\",{\"1\":{\"268\":1}}],[\"t10​+t24​+5htas​\",{\"1\":{\"148\":1}}],[\"tbubble​=\",{\"1\":{\"265\":1}}],[\"tb​\",{\"1\":{\"265\":1}}],[\"ttotal​=\",{\"1\":{\"265\":1}}],[\"t×n\",{\"1\":{\"217\":1}}],[\"t×l\",{\"1\":{\"204\":1}}],[\"tuseful​=m×\",{\"1\":{\"265\":1}}],[\"tuseful​=m×p×\",{\"1\":{\"265\":1}}],[\"tuple\",{\"1\":{\"177\":1}}],[\"tutor\",{\"1\":{\"121\":1}}],[\"twenty\",{\"1\":{\"129\":4,\"131\":2}}],[\"two\",{\"1\":{\"26\":1,\"139\":1,\"177\":3,\"179\":1}}],[\"t和p\",{\"1\":{\"95\":1}}],[\"tmp\",{\"1\":{\"43\":1}}],[\"tf​+tb​\",{\"1\":{\"265\":5}}],[\"tf​+mtb​\",{\"1\":{\"265\":1}}],[\"tf​是流水线\",{\"1\":{\"265\":1}}],[\"tf​\",{\"1\":{\"265\":2}}],[\"tf\",{\"1\":{\"43\":4}}],[\"tp的适用性较低\",{\"1\":{\"285\":1}}],[\"tp中都被切分\",{\"1\":{\"262\":1}}],[\"tp相比\",{\"1\":{\"262\":1}}],[\"tp通过将设备组织成二维网格来进一步优化\",{\"1\":{\"262\":1}}],[\"tp虽然有效\",{\"1\":{\"262\":1}}],[\"tp将权重矩阵w和激活x同时在行和列维度上切分到一个二维的设备网格中\",{\"1\":{\"255\":1}}],[\"tp仅切分一个维度不同\",{\"1\":{\"255\":1}}],[\"tp+pp\",{\"1\":{\"247\":1}}],[\"tp巧妙地应用于transformer的mlp层和多头注意力\",{\"1\":{\"245\":1}}],[\"tp对节点内的高带宽通信\",{\"1\":{\"245\":1}}],[\"tp模型做了比较\",{\"1\":{\"153\":1}}],[\"tp在一次前向和后向总共有4次的all\",{\"1\":{\"148\":1}}],[\"tp\",{\"0\":{\"87\":1},\"1\":{\"95\":1,\"150\":1,\"240\":1,\"255\":1,\"258\":1}}],[\"tpu\",{\"1\":{\"82\":1}}],[\"tpu运算中无以为继\",{\"1\":{\"53\":1}}],[\"tpu没办法高效完成我们的运算\",{\"1\":{\"33\":1}}],[\"tp基本并行方式后\",{\"1\":{\"35\":1}}],[\"tp也是应用较广的基本并发方式\",{\"1\":{\"35\":1}}],[\"target\",{\"1\":{\"177\":11,\"251\":1}}],[\"tar\",{\"0\":{\"20\":1},\"1\":{\"19\":7,\"20\":1,\"106\":3}}],[\"talk\",{\"1\":{\"18\":1,\"122\":1}}],[\"task4\",{\"0\":{\"27\":1}}],[\"task3\",{\"0\":{\"26\":1}}],[\"task2\",{\"0\":{\"25\":1}}],[\"task1\",{\"0\":{\"23\":1}}],[\"task\",{\"0\":{\"22\":1},\"1\":{\"9\":6,\"10\":3,\"12\":5,\"14\":6,\"107\":1,\"166\":1,\"255\":1}}],[\"tasks\",{\"1\":{\"9\":1,\"24\":1}}],[\"t<thread\",{\"1\":{\"14\":1}}],[\"t<int>\",{\"1\":{\"14\":1}}],[\"t<bool>\",{\"1\":{\"12\":1,\"14\":1}}],[\"t>\",{\"1\":{\"14\":1}}],[\"tips\",{\"1\":{\"27\":1}}],[\"tid\",{\"1\":{\"12\":8,\"14\":8}}],[\"time2\",{\"1\":{\"24\":3}}],[\"time1\",{\"1\":{\"24\":3}}],[\"time\",{\"1\":{\"10\":2,\"12\":1,\"24\":1,\"119\":1,\"265\":1}}],[\"tree\",{\"1\":{\"114\":3}}],[\"transformation\",{\"1\":{\"268\":1}}],[\"transformer进行改造\",{\"1\":{\"285\":1}}],[\"transformer\",{\"1\":{\"93\":1,\"144\":1,\"149\":1}}],[\"transformers\",{\"0\":{\"89\":1},\"1\":{\"155\":1}}],[\"transposeop\",{\"1\":{\"138\":1}}],[\"traceback\",{\"1\":{\"117\":1}}],[\"training\",{\"1\":{\"44\":1,\"51\":1,\"73\":1,\"81\":1,\"88\":1,\"90\":1,\"149\":1,\"152\":1,\"241\":1,\"250\":1}}],[\"train\",{\"1\":{\"43\":5}}],[\"trillion\",{\"1\":{\"51\":1}}],[\"true\",{\"1\":{\"10\":1,\"12\":2,\"13\":1,\"14\":3,\"78\":1}}],[\"try\",{\"0\":{\"8\":1,\"190\":1},\"1\":{\"8\":5,\"13\":1,\"24\":2,\"25\":1,\"26\":1,\"128\":1}}],[\"tsan\",{\"1\":{\"8\":2}}],[\"type\",{\"1\":{\"8\":1,\"89\":1,\"117\":1,\"128\":2}}],[\"tying\",{\"1\":{\"7\":1}}],[\"throughput\",{\"1\":{\"241\":1}}],[\"through\",{\"1\":{\"166\":1}}],[\"threadid\",{\"1\":{\"108\":1}}],[\"thread的并行机制和\",{\"1\":{\"107\":1}}],[\"threads\",{\"0\":{\"105\":1},\"1\":{\"9\":3,\"10\":4,\"12\":4,\"13\":3,\"14\":9,\"24\":1}}],[\"threadsanitizer\",{\"1\":{\"8\":2,\"13\":1}}],[\"thread\",{\"1\":{\"9\":5,\"13\":4,\"14\":6}}],[\"thought\",{\"1\":{\"166\":1}}],[\"that\",{\"1\":{\"119\":1,\"129\":1,\"138\":1,\"166\":1,\"177\":2}}],[\"than\",{\"1\":{\"10\":1}}],[\"think\",{\"1\":{\"114\":1,\"129\":1}}],[\"this\",{\"1\":{\"13\":1,\"14\":1,\"21\":1,\"114\":1,\"117\":1,\"125\":1,\"128\":1,\"134\":1,\"166\":1,\"177\":4}}],[\"thecvf\",{\"1\":{\"273\":1}}],[\"them\",{\"1\":{\"8\":1,\"10\":1,\"12\":1,\"13\":1,\"19\":1,\"125\":1,\"174\":1}}],[\"then\",{\"1\":{\"8\":4,\"9\":1,\"12\":2,\"13\":1,\"19\":1,\"23\":1,\"24\":4,\"25\":1,\"26\":3,\"27\":1,\"168\":1}}],[\"there\",{\"1\":{\"7\":1,\"8\":2,\"168\":1,\"177\":1}}],[\"the\",{\"0\":{\"2\":2,\"7\":1,\"8\":2,\"9\":2,\"11\":1,\"19\":1,\"20\":2,\"21\":2,\"23\":1,\"25\":1,\"26\":1,\"27\":2,\"54\":1,\"166\":1,\"167\":1},\"1\":{\"8\":8,\"9\":12,\"10\":10,\"12\":10,\"13\":8,\"14\":17,\"18\":1,\"19\":2,\"21\":5,\"23\":2,\"24\":10,\"25\":2,\"26\":7,\"27\":8,\"114\":4,\"117\":6,\"118\":1,\"119\":3,\"120\":2,\"122\":1,\"125\":1,\"128\":3,\"129\":2,\"131\":2,\"138\":1,\"139\":1,\"166\":4,\"168\":16,\"177\":48,\"179\":1}}],[\"text\",{\"1\":{\"253\":1,\"267\":1,\"292\":1}}],[\"text����\",{\"1\":{\"134\":1}}],[\"tensorflow框架下的通讯开销越大\",{\"1\":{\"38\":1}}],[\"tensorflow\",{\"1\":{\"38\":1,\"42\":2,\"43\":2}}],[\"tensor\",{\"0\":{\"147\":1},\"1\":{\"35\":1,\"89\":6,\"139\":2,\"146\":1,\"177\":52,\"179\":2,\"240\":1}}],[\"tencent\",{\"1\":{\"21\":1}}],[\"temporary\",{\"0\":{\"57\":1,\"63\":1}}],[\"temp\",{\"1\":{\"24\":4}}],[\"template\",{\"1\":{\"14\":1}}],[\"terminal\",{\"1\":{\"8\":1,\"117\":1,\"128\":1}}],[\"tests\",{\"0\":{\"130\":1},\"1\":{\"128\":1,\"130\":1}}],[\"test\",{\"0\":{\"1\":1,\"17\":1},\"1\":{\"7\":1,\"24\":4,\"27\":2,\"130\":2},\"2\":{\"16\":1,\"29\":1}}],[\"t\",{\"1\":{\"7\":1,\"10\":2,\"13\":4,\"14\":9,\"24\":1,\"27\":1,\"95\":1,\"108\":2,\"128\":1,\"200\":1,\"268\":1,\"277\":2,\"279\":1}}],[\"tolower\",{\"1\":{\"203\":1}}],[\"token\",{\"0\":{\"161\":1}}],[\"todo\",{\"1\":{\"139\":2,\"179\":2}}],[\"topology\",{\"1\":{\"99\":1,\"100\":1,\"101\":1}}],[\"toward\",{\"1\":{\"51\":1}}],[\"torch\",{\"1\":{\"45\":7,\"89\":2,\"139\":2,\"177\":35,\"179\":2}}],[\"tools\",{\"1\":{\"6\":1,\"8\":1}}],[\"to\",{\"0\":{\"2\":1,\"6\":1,\"8\":1},\"1\":{\"7\":1,\"8\":6,\"9\":6,\"10\":6,\"12\":6,\"13\":8,\"14\":6,\"18\":1,\"19\":1,\"21\":4,\"23\":1,\"24\":5,\"25\":1,\"26\":10,\"27\":1,\"117\":3,\"125\":2,\"128\":5,\"129\":2,\"130\":1,\"138\":1,\"139\":2,\"150\":2,\"151\":1,\"166\":1,\"168\":6,\"174\":1,\"177\":33,\"179\":2,\"253\":1,\"268\":1}}],[\"0x2000\",{\"1\":{\"168\":1}}],[\"03294\",{\"1\":{\"158\":1}}],[\"04\",{\"1\":{\"106\":1,\"166\":1}}],[\"04473https\",{\"1\":{\"90\":1}}],[\"05767\",{\"1\":{\"174\":1}}],[\"05799https\",{\"1\":{\"38\":1}}],[\"05198\",{\"1\":{\"144\":1}}],[\"05358\",{\"1\":{\"97\":1}}],[\"08053https\",{\"1\":{\"88\":1}}],[\"06965\",{\"1\":{\"81\":1}}],[\"06840\",{\"1\":{\"73\":1}}],[\"06840https\",{\"1\":{\"73\":1}}],[\"02054\",{\"1\":{\"51\":1}}],[\"01889\",{\"1\":{\"155\":1}}],[\"01\",{\"0\":{\"190\":1},\"1\":{\"43\":1,\"45\":1}}],[\"0b1\",{\"1\":{\"23\":1,\"24\":1}}],[\"0\",{\"0\":{\"1\":1},\"1\":{\"8\":1,\"10\":3,\"12\":5,\"14\":11,\"24\":19,\"43\":3,\"78\":1,\"89\":4,\"106\":4,\"117\":1,\"128\":1,\"130\":1,\"175\":1,\"177\":19,\"196\":2,\"202\":1,\"203\":1,\"209\":1,\"210\":3,\"216\":3,\"221\":1,\"222\":7,\"228\":3,\"234\":7,\"263\":2,\"277\":1}}],[\"路漫漫其修远兮\",{\"1\":{\"0\":1}}]],\"serializationVersion\":2}}")).map(([e,t])=>[e,zt(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:s,options:n,id:o}})=>{const u=bt[s];e==="suggest"?self.postMessage([e,o,tt(t,u,n)]):e==="search"?self.postMessage([e,o,Z(t,u,n)]):self.postMessage({suggestions:[e,o,tt(t,u,n)],results:[e,o,Z(t,u,n)]})};
//# sourceMappingURL=index.js.map

"use strict";(self.webpackChunklearn_data=self.webpackChunklearn_data||[]).push([[206],{8789:(s,a)=>{a.A=(s,a)=>{const t=s.__vccOpts||s;for(const[s,n]of a)t[s]=n;return t}},7497:(s,a,t)=>{t.r(a),t.d(a,{comp:()=>l,data:()=>i});var n=t(5252);const e=t.p+"assets/img/TP.404b53db.png",r=t.p+"assets/img/DP.c0f8d8bc.png",o={},l=(0,t(8789).A)(o,[["render",function(s,a){return(0,n.uX)(),(0,n.CE)("div",null,a[0]||(a[0]=[(0,n.Fv)('<h2 id="abstract" tabindex="-1"><a class="header-anchor" href="#abstract"><span><strong>Abstract</strong></span></a></h2><p>随着models参数规模进入万亿级别，其在infer阶段的部署面临 <code>memery bound</code>、<code>compute bound</code>与<code>I/O bound</code>等多重挑战。本文旨在系统性地梳理和剖析大规模模型infer所采用的高性能并行计算范式。首先界定<code>DP</code> <code>PP</code> <code>TP</code>概念及其在推理场景下的应用边界与理论局限。其次，深入探讨了以<code>Continuous Batching</code>和<code>PagedAttention</code>为代表的现代推理系统的核心架构演进。在此基础上，本文进一步剖析了<code>Quantization</code>、<code>Speculative Decoding</code>等前沿算法优化。为将理论与实践相结合，本文选取了三个典型案例进行深度研究：面向特定架构的组件级并行（以Stable Diffusion为例）、应对超长上下文挑战的序列并行，以及在复杂单一样本场景下的混合并行策略（以AlphaFold3为例）。最后，通过一个在HPC环境下部署CPU推理集群的工程案例，展示了在现实约束下进行系统架构设计的智慧。本文期望为从事大规模AI infra的researcher和engineer提供一个清晰、全面的知识图谱与决策参考。</p><hr><h4 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction"><span><strong>Introduction</strong></span></a></h4><ol><li><strong>infer成为新的瓶颈</strong><ul><li>后训练时代（Post-Training Era）的来临：模型即服务（Model-as-a-Service）的兴起</li><li>推理阶段的三大核心挑战： <ul><li><strong>Memory Wall</strong>：模型权重与动态KV缓存的巨大内存占用。</li><li><strong>Compute Wall</strong>：自回归解码带来的序列化计算与单次前向传播的巨大浮点运算量（FLOPs）。</li><li><strong>Throughput Wall</strong>：在满足严格Latency约束下，最大化并发处理能力的经济性问题。</li></ul></li></ul></li><li><strong>constrution and contribution</strong><ul><li>系统性构建推理并行的class。</li><li>深入分析关键系统优化与算法优化的内在机理。</li><li>通过前沿案例研究，展示多维并行策略的综合应用。</li><li>提供一个从理论到工程实践的完整审视视角。</li></ul></li></ol><h4 id="第一章-推理并行的基础范式与理论边界" tabindex="-1"><a class="header-anchor" href="#第一章-推理并行的基础范式与理论边界"><span><strong>第一章：推理并行的基础范式与理论边界</strong></span></a></h4><p>1.1. <strong>张量并行 (Tensor Parallelism, TP): 降低计算延迟的核心手段</strong></p><blockquote><p>Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism https://arxiv.org/abs/1909.08053. How to Train Really Large Models on Many GPUs? https://lilianweng.github.io/posts/2021-09-25-train-large/</p></blockquote><ul><li><strong>核心思想</strong>：算子内部（Intra-Operator）的并行化，将单个计算密集型算子（如GEMM）切分至多个计算单元。</li><li><strong>数学原理</strong>：基于矩阵乘法结合律的行、列切分策略。 <ul><li>矩阵拆分：在神经网络层内部，将权重矩阵按行或列进行切分，分布到不同GPU上。（主要是Self-Attention层和MLP层）</li><li>局部计算：每个GPU仅存储所分配的参数量，执行局部的矩阵乘法运算</li><li>通信同步： <ul><li>列并行：通过 All-Gather 将不同GPU的输出部分进行拼接（如<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>X</mi><msub><mi>W</mi><mn>1</mn></msub><mo separator="true">,</mo><mi>X</mi><msub><mi>W</mi><mn>2</mn></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[X W_{1},X W_{2}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">]</span></span></span></span>）</li><li>行并行：通过 All-Reduce 对部分结果求和（如<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mn>1</mn></msub><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msub><mi>X</mi><mn>2</mn></msub><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">X_{1}W_{1}+X_{2}W_{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>​）</li></ul></li></ul></li></ul><p><img src="'+e+'" alt=""></p><ul><li><strong>应用与理论局限</strong>：对节点内高带宽通信（如NVLink）有强依赖性；通信开销随并行度增加而上升，存在扩展性边界。</li></ul><p>1.2. <strong>模型并行 (Model Parallelism, MP): 低显存占用但并不高效</strong> * <strong>核心思想</strong>：computation 和 model 参数在多台计算机上进行分区。与每个 worker 托管整个模型的完整副本的数据并行性不同，MP 仅在一个 worker 上分配一小部分模型参数，因此内存使用和计算量都减少了。 * <strong>主要缺陷</strong>：通过具有顺序依赖关系的多个此类 worker 运行每个数据批次的都会导致微观上的串行。 * <strong>实现示意</strong>： <img src="'+r+'" alt=""></p><p>1.3. <strong>流水线并行 (Pipeline Parallelism, PP): 提升硬件利用率与吞吐量的关键</strong> * <strong>核心思想</strong>：算子间（Inter-Operator）的并行化，将模型的计算图（Computational Graph）按层（Stage）切分。 * <strong>计算模型</strong>：微批处理（Micro-batching）与流水线调度，分析“流水线气泡”（Pipeline Bubble）的成因与影响。 * <strong>应用与理论局限</strong>：无法降低单一样本的端到端延迟；负载均衡（Load Balancing）是关键挑战；气泡开销限制了其在小批量或低延迟场景下的效率。</p><p>1.4. <strong>数据并行 (Data Parallelism, DP): 服务能力水平扩展的直接范式</strong> * <strong>核心思想</strong>：通过模型副本（Model Replication）实现请求级并行。 * <strong>架构模式</strong>：负载均衡器（Load Balancer） + 多个独立推理副本。 * <strong>应用与理论局限</strong>：无法解决单模型内存或计算瓶颈；成本随副本数线性增长；不适用于<code>batch_size=1</code>的场景。</p><h4 id="第二章-现代推理系统的核心技术与架构演进" tabindex="-1"><a class="header-anchor" href="#第二章-现代推理系统的核心技术与架构演进"><span><strong>第二章：现代推理系统的核心技术与架构演进</strong></span></a></h4><p>2.1. <strong>架构范式：混合并行 (Hybrid Parallelism)</strong> * <strong>设计哲学</strong>：节点内TP + 节点间PP，最大化利用异构的通信带宽层级。 * <strong>典型架构</strong>：以Megatron-LM的部署架构为例，分析其通信模式。</p><p>2.2. <strong>I/O与内存管理优化：从根本上提升系统效率</strong> * <strong>调度革命：连续批处理 (Continuous Batching)</strong>：将静态、离散的批处理，演进为动态、连续的请求流处理，从调度层面消除GPU空闲。 * <strong>内存革命：分页注意力 (PagedAttention)</strong>：借鉴操作系统虚拟内存思想，将逻辑上连续的KV缓存存储在物理上非连续的内存块（Block）中，从根本上解决内存碎片问题。</p><h4 id="第三章-前沿算法优化-突破性能瓶颈" tabindex="-1"><a class="header-anchor" href="#第三章-前沿算法优化-突破性能瓶颈"><span><strong>第三章：前沿算法优化：突破性能瓶颈</strong></span></a></h4><p>3.1. <strong>模型压缩与量化 (Model Compression &amp; Quantization)</strong> * <strong>原理</strong>：通过降低权重和/或激活值的数值精度，减少内存占用与计算量。 * <strong>主流方法评述</strong>：从GPTQ到AWQ，分析其如何在保持模型精度的前提下进行有效量化。</p><p>3.2. <strong>解码加速：推测解码 (Speculative Decoding)</strong> * <strong>原理</strong>：利用轻量级草稿模型（Draft Model）与原始目标模型（Target Model）的验证机制，打破自回归解码“一次生成一个Token”的序列化限制。 * <strong>性能模型分析</strong>：分析其加速效果与草稿模型接受率（Acceptance Rate）之间的关系。</p><h4 id="第四章-特定领域并行策略剖析-案例研究-case-studies" tabindex="-1"><a class="header-anchor" href="#第四章-特定领域并行策略剖析-案例研究-case-studies"><span><strong>第四章：特定领域并行策略剖析：案例研究 (Case Studies)</strong></span></a></h4><p>4.1. <strong>案例一：组件级与任务级并行——以Stable Diffusion为例</strong> * <strong>问题域</strong>：文生图模型的多阶段、异构计算特性。 * <strong>解决方案</strong>：构建“宏观流水线”（Macro-pipeline），将Text Encoder, U-Net, VAE Decoder等独立组件部署于不同硬件单元，实现组件级的流水线并行，最大化图像生成吞吐量。</p><p>4.2. <strong>案例二：序列并行 (Sequence Parallelism)——应对超长上下文挑战</strong> * <strong>问题域</strong>：注意力机制中<code>O(N²)</code>的内存复杂度成为处理长序列的根本瓶颈。 * <strong>解决方案</strong>：沿序列维度对张量及计算进行切分，通过与张量并行协同的通信策略（如Reduce-Scatter, All-Gather），避免在单设备上实例化完整的注意力矩阵。</p><p>4.3. <strong>案例三：复杂单一样本的混合并行——AlphaFold3的系统设计启示</strong> * <strong>问题域</strong>：<code>batch_size=1</code>的场景下，DDP失效，如何对单个“超级样本”进行并行计算？ * <strong>多维度并行策略剖析</strong>： * <strong>模型内部并行 (Intra-Model)</strong>：TP（Head/Dimension并行）与PP是基础。 * <strong>算法流程并行 (Inter-Task)</strong>：利用DiT模块多次独立采样的特性，进行任务级并行（Task Parallelism）。 * <strong>硬件内核级优化 (Kernel-Level)</strong>：采用序列感知的算子调度策略，动态选择SDPA、FlashAttention或更前沿的SageAttention等最高效的注意力计算内核。</p><h4 id="第五章-工程实践的智慧-当理论框架遇到现实约束" tabindex="-1"><a class="header-anchor" href="#第五章-工程实践的智慧-当理论框架遇到现实约束"><span><strong>第五章：工程实践的智慧：当理论框架遇到现实约束</strong></span></a></h4><ul><li><strong>案例背景</strong>：在基于SLURM的纯CPU HPC集群上，规模化部署基于vLLM的DeepSeek模型。</li><li><strong>面临的约束</strong>：vLLM的CPU后端功能受限（无TP/PP），且其分布式后端Ray与HPC生态不兼容。</li><li><strong>架构设计：服务层并行化与解耦编排</strong><ul><li><strong>核心思路</strong>：构建一个由SGLang Router作为统一入口，SLURM作为资源调度与进程管理器的解耦式架构。</li><li><strong>架构解析</strong>：该架构在服务层实现了请求的负载均衡与并行处理，有效绕开了底层框架的限制。这是一个典型的“以系统架构设计弥补框架能力不足”的工程范例。</li></ul></li><li><strong>单节点优化策略</strong>：结合AWQ量化和投机解码，分别针对CPU环境下的内存带宽和单样本延迟瓶颈进行优化。</li></ul><h4 id="结论与展望" tabindex="-1"><a class="header-anchor" href="#结论与展望"><span><strong>结论与展望</strong></span></a></h4><ol><li><strong>构建决策框架</strong>：总结一张决策图或表格，为从业者在面对不同业务目标（延迟、吞吐量、成本）和技术挑战（模型大小、序列长度）时，如何选择与组合不同的并行及优化技术提供指导。</li><li><strong>未来展望</strong>： <ul><li><strong>自动化并行 (Automated Parallelism)</strong>：自动并行策略搜索与编译优化。</li><li><strong>软硬件协同设计 (Hardware-Software Co-design)</strong>：面向未来AI硬件的算法与系统架构。</li><li><strong>新计算范式</strong>：探索非Transformer架构（如Mamba, RWKV）的并行潜力。</li></ul></li></ol>',28)]))}]]),i=JSON.parse('{"path":"/posts/%E5%A4%A7%E8%A7%84%E6%A8%A1AI%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E8%8C%83%E5%BC%8F.html","title":"大规模AI模型的infer并行计算范式","lang":"zh-CN","frontmatter":{"title":"大规模AI模型的infer并行计算范式","category":["SOSD"],"tag":["分布式系统","MLSys","并行运算"],"sticky":true,"star":true,"order":-2,"description":"Abstract 随着models参数规模进入万亿级别，其在infer阶段的部署面临 memery bound、compute bound与I/O bound等多重挑战。本文旨在系统性地梳理和剖析大规模模型infer所采用的高性能并行计算范式。首先界定DP PP TP概念及其在推理场景下的应用边界与理论局限。其次，深入探讨了以Continuous Ba...","head":[["meta",{"property":"og:url","content":"https://newzone.top/posts/%E5%A4%A7%E8%A7%84%E6%A8%A1AI%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E7%90%86%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E8%8C%83%E5%BC%8F.html"}],["meta",{"property":"og:site_name","content":"CS_Blog"}],["meta",{"property":"og:title","content":"大规模AI模型的infer并行计算范式"}],["meta",{"property":"og:description","content":"Abstract 随着models参数规模进入万亿级别，其在infer阶段的部署面临 memery bound、compute bound与I/O bound等多重挑战。本文旨在系统性地梳理和剖析大规模模型infer所采用的高性能并行计算范式。首先界定DP PP TP概念及其在推理场景下的应用边界与理论局限。其次，深入探讨了以Continuous Ba..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-06-23T04:22:38.000Z"}],["meta",{"property":"article:tag","content":"分布式系统"}],["meta",{"property":"article:tag","content":"MLSys"}],["meta",{"property":"article:tag","content":"并行运算"}],["meta",{"property":"article:modified_time","content":"2025-06-23T04:22:38.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"大规模AI模型的infer并行计算范式\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-06-23T04:22:38.000Z\\",\\"author\\":[]}"]]},"headers":[{"level":2,"title":"Abstract","slug":"abstract","link":"#abstract","children":[]}],"git":{"createdTime":1750652558000,"updatedTime":1750652558000,"contributors":[{"name":"youth00000000","email":"youthandqueen@qq.com","commits":1}]},"readingTime":{"minutes":8.21,"words":2463},"filePathRelative":"_posts/大规模AI模型的推理并行计算范式.md","localizedDate":"2025年6月23日","excerpt":"<h2><strong>Abstract</strong></h2>\\n<p>随着models参数规模进入万亿级别，其在infer阶段的部署面临 <code>memery bound</code>、<code>compute bound</code>与<code>I/O bound</code>等多重挑战。本文旨在系统性地梳理和剖析大规模模型infer所采用的高性能并行计算范式。首先界定<code>DP</code> <code>PP</code> <code>TP</code>概念及其在推理场景下的应用边界与理论局限。其次，深入探讨了以<code>Continuous Batching</code>和<code>PagedAttention</code>为代表的现代推理系统的核心架构演进。在此基础上，本文进一步剖析了<code>Quantization</code>、<code>Speculative Decoding</code>等前沿算法优化。为将理论与实践相结合，本文选取了三个典型案例进行深度研究：面向特定架构的组件级并行（以Stable Diffusion为例）、应对超长上下文挑战的序列并行，以及在复杂单一样本场景下的混合并行策略（以AlphaFold3为例）。最后，通过一个在HPC环境下部署CPU推理集群的工程案例，展示了在现实约束下进行系统架构设计的智慧。本文期望为从事大规模AI infra的researcher和engineer提供一个清晰、全面的知识图谱与决策参考。</p>","autoDesc":true}')}}]);